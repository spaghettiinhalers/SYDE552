{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence, save_path=None, show=True):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.plot(xs, ys, linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ded58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [[] for _ in range(10)]\n",
    "\n",
    "for i in range(10000):\n",
    "    try:\n",
    "        data = np.loadtxt(f'../sequences/testimg-{i}-targetdata.txt', delimiter=' ')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found at path: {i}\")\n",
    "        continue\n",
    "    \n",
    "    inputOneshot = data[0, 0:10]\n",
    "    outputStrokes = data[:, 10:]\n",
    "    outputStrokes[:, 0] = outputStrokes[:, 0]/28\n",
    "    outputStrokes[:, 1] = outputStrokes[:, 1]/28\n",
    "    \n",
    "    datas[getNumFromOneHot(inputOneshot)].append(outputStrokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e460841",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[i] = 1\n",
    "    \n",
    "    smallest_10 = sorted(datas[i], key=len)[:100]\n",
    "    for k in smallest_10:\n",
    "        input_data.append(temp_onehot)\n",
    "        output_data.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ec919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAE4klEQVR4nO3dz2scZQDG8SdpTJo22IaWxqKUapSmFgSDhSAeAkoIXryqBxEKheYsFo9ehBYPIhKloPRS+w+IBMkhQgUFLZhDYzX1Rw0JFqXZlLoxJju9NGttNrTdzuw7z/t+P6cZyOEJ+TIMO2S2LcuyTICh9tADgGYRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2wRL2x1hB5QRhfmlzQ9txh6Rq62d3Xo+YN7tK0znj95PL9JTj6cuqQTEz+EnlGIkSf7dOq1Z0LPyA23DbeIOVxJmp6rhJ6QK668N90e7pHnHtUTe3oCLsrPO5/PaGl5NfSM3BGvNoZ7fHRAx4b7Ay7K13uTP0UZb/K3DbGHG7Ok4yVcb8nGS7j+koyXcOOQXLyEG4+kPm0Yn5rVyYmL9XPC9ZbMlZdw45NEvIQbp+jjJdx4RR0v4cYt2ngJN35Rxku4aYguXsJNR1TxEm5aoon3/OWrhJuYaOId3NerN0cPSCLcVET1eHhs+HENPbZLg/t6Q09BC0Rz5V1HuOmILl6kg3hhi3hhi3hhi3hhi3hhi3hhi3hhi3hhi3hhi3hhi3hhi3hhi3gjt1Cp6tryv6FnFIJ4I7ZQqerlU1/r+sqaJOmpR3YEXpQv4o3Ueri//fW3JGn/rm16+6VDgVfli3gj1Cjcs0eHtHdHd+Bl+SLeyKQSrkS8UUkpXIl4o5FauBLxRiHFcCXitXfl2nKS4UrEa+/0V78mGa5EvPbmF6v14w9eHUwmXIl4o9LTFdULkO6IeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeI2trNb0y8235aSIeE2trNY0duY7ff/7oiRpd0+n9u7cGnZUixGvofVwJ2euSJK2PtCu9195Wl0dWwIvay3iNdMo3E9eP6xn+3cHXtZ6xGuEcP+PeE0Q7kbEa4BwGyPekiPczRFvidVqmcbOnCfcTRBviZ2b/VOTM39IItxGiLfEbn1l/xsjBwj3NsRrIrVX9t8N4oUt4oUt4oUt4oUt4oUt4i2xLPSAkiPeklqoVPXRl5fq59v5qGwD4i2hhUp1wze5v3CwL/Cq8iHekmkU7tmjQ+ruTOu/JO4G8ZbIZuGm9E3u94J4S4Jw7x3xlgDhNod4AyPc5hFvQIR7f4g3kCzLdOT0t4R7H4g3kLmrVV1YWJIkPbyzm3CbQLyBrNX+e/h7eH8v4TaBeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeGGLeAP5bHq+ftze3hZwiS/iDWB8albvfvFj/Xz00EMB1/gi3hYbn5rVyYmL9fPjowMaId6mEG8LNQr32HB/wEXeiLdFCDd/xNsChFsM4i0Y4RaHeAtEuMUi3oJ8+s1lwi0Y8Rbk43M/148JtxjEW5Dr/6xJkvoe7CLcghBvwdrEo9+iEC9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sES9sdYQeEKu3XhxQdWVN3Z1bQk+JVluWZdmdfwwoH24bYIt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YYt4YesGIVrUDHHc9BgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_stroke_sequence(output_data[782])\n",
    "\n",
    "# 0: [0, 3, 5, 10, 12, 16, 18, 20, 21, 22]\n",
    "# 1: [100, 101, 102, 103, 104, 105, 107, 109, 110, 112]\n",
    "# 2: [213, 218, 221, 230, 232, 234, 241, 249, 253, 254]\n",
    "# 3; [314, 325, 327, 345, 347, 350, 354, 358, 365, 366]\n",
    "# 4: [403, 405, 414, 415, 430, 434, 438, 439, 450, 464]\n",
    "# 5: [500, 524, 527, 531, 532, 545, 549, 558, 565, 569]\n",
    "# 6: [625, 627, 628, 643, 659, 661, 671, 676, 679 ,682]\n",
    "# 7: [712, 714, 723, 727, 729, 730, 736, 782, 795, 799]\n",
    "# 8: \n",
    "# 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04766e4",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6cd2c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# Finding the max length of a sequence\n",
    "max_length = 0\n",
    "j = 0\n",
    "for i in range(len(output_data)):\n",
    "    if len(output_data[i]) > max_length:\n",
    "        max_length = len(output_data[i])\n",
    "    j += 1\n",
    "\n",
    "print(max_length)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "edeec1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "642bf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences so that they are all the same size (good for batching)\n",
    "padded_output_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_output_data[i, :len(output_data[i]), :] = output_data[i]\n",
    "    padded_output_data[i, len(output_data[i]):, :] = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "37cfe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_input_data[i, 0, :] = [0, 0, 0, 0]\n",
    "    padded_input_data[i, 1:, :] = padded_output_data[i, :max_length-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8523173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, onehot, inputs, outputstroke):\n",
    "        self.digit = onehot                     # shape: [N]\n",
    "        self.inputstroke = inputs               # list of [seq_len, 4] arrays\n",
    "        self.outputstroke = outputstroke        # list of [seq_len, 4] arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digit)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digit[idx]\n",
    "        inputs = self.inputstroke[idx]\n",
    "        outputs = self.outputstroke[idx]\n",
    "        return torch.tensor(digit, dtype=torch.float32), torch.tensor(inputs, dtype=torch.float32), torch.tensor(outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1171d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokeDataset = StrokeDataset(input_data, padded_input_data, padded_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d67af144",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(strokeDataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8f994",
   "metadata": {},
   "source": [
    "Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2082a2d",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "\n",
    "RNN:\n",
    "input_size = output_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e58d8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7354307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 18.7401\n",
      "Epoch 2 | Loss: 6.9217\n",
      "Epoch 3 | Loss: 5.0782\n",
      "Epoch 4 | Loss: 4.6483\n",
      "Epoch 5 | Loss: 4.6595\n",
      "Epoch 6 | Loss: 4.6680\n",
      "Epoch 7 | Loss: 4.4757\n",
      "Epoch 8 | Loss: 4.3323\n",
      "Epoch 9 | Loss: 4.1930\n",
      "Epoch 10 | Loss: 4.1514\n",
      "Epoch 11 | Loss: 4.3001\n",
      "Epoch 12 | Loss: 4.3544\n",
      "Epoch 13 | Loss: 4.0385\n",
      "Epoch 14 | Loss: 4.0746\n",
      "Epoch 15 | Loss: 4.0415\n",
      "Epoch 16 | Loss: 4.0953\n",
      "Epoch 17 | Loss: 4.0305\n",
      "Epoch 18 | Loss: 3.9374\n",
      "Epoch 19 | Loss: 3.9106\n",
      "Epoch 20 | Loss: 3.9738\n",
      "Epoch 21 | Loss: 4.0589\n",
      "Epoch 22 | Loss: 3.9802\n",
      "Epoch 23 | Loss: 3.8196\n",
      "Epoch 24 | Loss: 3.9461\n",
      "Epoch 25 | Loss: 3.8913\n",
      "Epoch 26 | Loss: 4.1068\n",
      "Epoch 27 | Loss: 4.0066\n",
      "Epoch 28 | Loss: 3.8094\n",
      "Epoch 29 | Loss: 3.8681\n",
      "Epoch 30 | Loss: 4.0371\n",
      "Epoch 31 | Loss: 3.9227\n",
      "Epoch 32 | Loss: 3.8798\n",
      "Epoch 33 | Loss: 3.7890\n",
      "Epoch 34 | Loss: 3.9186\n",
      "Epoch 35 | Loss: 3.9464\n",
      "Epoch 36 | Loss: 3.9255\n",
      "Epoch 37 | Loss: 3.8674\n",
      "Epoch 38 | Loss: 3.8647\n",
      "Epoch 39 | Loss: 3.8153\n",
      "Epoch 40 | Loss: 3.8018\n",
      "Epoch 41 | Loss: 3.7401\n",
      "Epoch 42 | Loss: 3.8263\n",
      "Epoch 43 | Loss: 3.7378\n",
      "Epoch 44 | Loss: 3.8773\n",
      "Epoch 45 | Loss: 3.7979\n",
      "Epoch 46 | Loss: 3.7789\n",
      "Epoch 47 | Loss: 3.8074\n",
      "Epoch 48 | Loss: 3.7886\n",
      "Epoch 49 | Loss: 3.9492\n",
      "Epoch 50 | Loss: 3.7755\n",
      "Epoch 51 | Loss: 3.7102\n",
      "Epoch 52 | Loss: 3.7166\n",
      "Epoch 53 | Loss: 3.7576\n",
      "Epoch 54 | Loss: 3.7296\n",
      "Epoch 55 | Loss: 3.7638\n",
      "Epoch 56 | Loss: 3.7375\n",
      "Epoch 57 | Loss: 3.7639\n",
      "Epoch 58 | Loss: 3.7394\n",
      "Epoch 59 | Loss: 3.7039\n",
      "Epoch 60 | Loss: 3.6467\n",
      "Epoch 61 | Loss: 3.7145\n",
      "Epoch 62 | Loss: 3.6688\n",
      "Epoch 63 | Loss: 3.6854\n",
      "Epoch 64 | Loss: 3.7856\n",
      "Epoch 65 | Loss: 3.7587\n",
      "Epoch 66 | Loss: 3.7995\n",
      "Epoch 67 | Loss: 3.6969\n",
      "Epoch 68 | Loss: 3.7066\n",
      "Epoch 69 | Loss: 3.6817\n",
      "Epoch 70 | Loss: 3.6454\n",
      "Epoch 71 | Loss: 3.7072\n",
      "Epoch 72 | Loss: 3.6288\n",
      "Epoch 73 | Loss: 3.7148\n",
      "Epoch 74 | Loss: 3.6947\n",
      "Epoch 75 | Loss: 3.6626\n",
      "Epoch 76 | Loss: 3.6682\n",
      "Epoch 77 | Loss: 3.6290\n",
      "Epoch 78 | Loss: 3.6006\n",
      "Epoch 79 | Loss: 3.6275\n",
      "Epoch 80 | Loss: 3.6187\n",
      "Epoch 81 | Loss: 3.6074\n",
      "Epoch 82 | Loss: 3.6456\n",
      "Epoch 83 | Loss: 3.6732\n",
      "Epoch 84 | Loss: 3.6305\n",
      "Epoch 85 | Loss: 3.6586\n",
      "Epoch 86 | Loss: 3.6258\n",
      "Epoch 87 | Loss: 3.6008\n",
      "Epoch 88 | Loss: 3.6360\n",
      "Epoch 89 | Loss: 3.6010\n",
      "Epoch 90 | Loss: 3.5445\n",
      "Epoch 91 | Loss: 3.6490\n",
      "Epoch 92 | Loss: 3.6243\n",
      "Epoch 93 | Loss: 3.7300\n",
      "Epoch 94 | Loss: 3.7942\n",
      "Epoch 95 | Loss: 3.6082\n",
      "Epoch 96 | Loss: 3.6128\n",
      "Epoch 97 | Loss: 3.6389\n",
      "Epoch 98 | Loss: 3.5378\n",
      "Epoch 99 | Loss: 3.5231\n",
      "Epoch 100 | Loss: 3.5130\n",
      "Epoch 101 | Loss: 3.5380\n",
      "Epoch 102 | Loss: 3.5027\n",
      "Epoch 103 | Loss: 3.5401\n",
      "Epoch 104 | Loss: 3.5981\n",
      "Epoch 105 | Loss: 3.4866\n",
      "Epoch 106 | Loss: 3.5747\n",
      "Epoch 107 | Loss: 3.5829\n",
      "Epoch 108 | Loss: 3.6059\n",
      "Epoch 109 | Loss: 3.4743\n",
      "Epoch 110 | Loss: 3.5389\n",
      "Epoch 111 | Loss: 3.4490\n",
      "Epoch 112 | Loss: 3.5664\n",
      "Epoch 113 | Loss: 3.4487\n",
      "Epoch 114 | Loss: 3.4453\n",
      "Epoch 115 | Loss: 3.4300\n",
      "Epoch 116 | Loss: 3.4828\n",
      "Epoch 117 | Loss: 3.4201\n",
      "Epoch 118 | Loss: 3.4109\n",
      "Epoch 119 | Loss: 3.5615\n",
      "Epoch 120 | Loss: 3.4599\n",
      "Epoch 121 | Loss: 3.4536\n",
      "Epoch 122 | Loss: 3.4048\n",
      "Epoch 123 | Loss: 3.4187\n",
      "Epoch 124 | Loss: 3.3784\n",
      "Epoch 125 | Loss: 3.3921\n",
      "Epoch 126 | Loss: 3.3725\n",
      "Epoch 127 | Loss: 3.4170\n",
      "Epoch 128 | Loss: 3.4148\n",
      "Epoch 129 | Loss: 3.3810\n",
      "Epoch 130 | Loss: 3.3167\n",
      "Epoch 131 | Loss: 3.2524\n",
      "Epoch 132 | Loss: 3.2842\n",
      "Epoch 133 | Loss: 3.3807\n",
      "Epoch 134 | Loss: 3.2570\n",
      "Epoch 135 | Loss: 3.2708\n",
      "Epoch 136 | Loss: 3.3377\n",
      "Epoch 137 | Loss: 3.2518\n",
      "Epoch 138 | Loss: 3.2593\n",
      "Epoch 139 | Loss: 3.2668\n",
      "Epoch 140 | Loss: 3.1987\n",
      "Epoch 141 | Loss: 3.1843\n",
      "Epoch 142 | Loss: 3.1588\n",
      "Epoch 143 | Loss: 3.1646\n",
      "Epoch 144 | Loss: 3.1970\n",
      "Epoch 145 | Loss: 3.1810\n",
      "Epoch 146 | Loss: 3.0902\n",
      "Epoch 147 | Loss: 3.0517\n",
      "Epoch 148 | Loss: 3.1320\n",
      "Epoch 149 | Loss: 3.0308\n",
      "Epoch 150 | Loss: 3.0131\n",
      "Epoch 151 | Loss: 2.9495\n",
      "Epoch 152 | Loss: 2.9298\n",
      "Epoch 153 | Loss: 2.9914\n",
      "Epoch 154 | Loss: 2.9471\n",
      "Epoch 155 | Loss: 2.9439\n",
      "Epoch 156 | Loss: 2.8793\n",
      "Epoch 157 | Loss: 2.8444\n",
      "Epoch 158 | Loss: 2.8057\n",
      "Epoch 159 | Loss: 2.7831\n",
      "Epoch 160 | Loss: 2.7678\n",
      "Epoch 161 | Loss: 2.6943\n",
      "Epoch 162 | Loss: 2.7041\n",
      "Epoch 163 | Loss: 2.6869\n",
      "Epoch 164 | Loss: 2.6595\n",
      "Epoch 165 | Loss: 2.6643\n",
      "Epoch 166 | Loss: 2.6050\n",
      "Epoch 167 | Loss: 2.6458\n",
      "Epoch 168 | Loss: 2.6960\n",
      "Epoch 169 | Loss: 2.4727\n",
      "Epoch 170 | Loss: 2.5517\n",
      "Epoch 171 | Loss: 2.3896\n",
      "Epoch 172 | Loss: 2.4793\n",
      "Epoch 173 | Loss: 2.3997\n",
      "Epoch 174 | Loss: 2.4702\n",
      "Epoch 175 | Loss: 2.3539\n",
      "Epoch 176 | Loss: 2.3134\n",
      "Epoch 177 | Loss: 2.2478\n",
      "Epoch 178 | Loss: 2.4231\n",
      "Epoch 179 | Loss: 2.2255\n",
      "Epoch 180 | Loss: 2.1673\n",
      "Epoch 181 | Loss: 2.2156\n",
      "Epoch 182 | Loss: 2.2954\n",
      "Epoch 183 | Loss: 2.1662\n",
      "Epoch 184 | Loss: 2.1004\n",
      "Epoch 185 | Loss: 2.0611\n",
      "Epoch 186 | Loss: 1.9855\n",
      "Epoch 187 | Loss: 2.0290\n",
      "Epoch 188 | Loss: 1.8985\n",
      "Epoch 189 | Loss: 1.9924\n",
      "Epoch 190 | Loss: 2.0007\n",
      "Epoch 191 | Loss: 2.1658\n",
      "Epoch 192 | Loss: 1.9197\n",
      "Epoch 193 | Loss: 1.7541\n",
      "Epoch 194 | Loss: 1.6921\n",
      "Epoch 195 | Loss: 1.9157\n",
      "Epoch 196 | Loss: 1.8799\n",
      "Epoch 197 | Loss: 1.7615\n",
      "Epoch 198 | Loss: 1.5446\n",
      "Epoch 199 | Loss: 1.6370\n",
      "Epoch 200 | Loss: 1.6782\n",
      "Epoch 201 | Loss: 1.7839\n",
      "Epoch 202 | Loss: 1.5977\n",
      "Epoch 203 | Loss: 1.5655\n",
      "Epoch 204 | Loss: 1.4039\n",
      "Epoch 205 | Loss: 1.5019\n",
      "Epoch 206 | Loss: 1.7550\n",
      "Epoch 207 | Loss: 1.5434\n",
      "Epoch 208 | Loss: 1.3516\n",
      "Epoch 209 | Loss: 1.3104\n",
      "Epoch 210 | Loss: 1.3611\n",
      "Epoch 211 | Loss: 1.2836\n",
      "Epoch 212 | Loss: 1.3150\n",
      "Epoch 213 | Loss: 1.8856\n",
      "Epoch 214 | Loss: 1.3901\n",
      "Epoch 215 | Loss: 1.2075\n",
      "Epoch 216 | Loss: 1.1569\n",
      "Epoch 217 | Loss: 1.2281\n",
      "Epoch 218 | Loss: 1.1119\n",
      "Epoch 219 | Loss: 1.0734\n",
      "Epoch 220 | Loss: 1.2373\n",
      "Epoch 221 | Loss: 1.2671\n",
      "Epoch 222 | Loss: 1.1398\n",
      "Epoch 223 | Loss: 1.1127\n",
      "Epoch 224 | Loss: 1.1216\n",
      "Epoch 225 | Loss: 1.0429\n",
      "Epoch 226 | Loss: 0.9860\n",
      "Epoch 227 | Loss: 1.0847\n",
      "Epoch 228 | Loss: 1.0862\n",
      "Epoch 229 | Loss: 1.0782\n",
      "Epoch 230 | Loss: 0.9686\n",
      "Epoch 231 | Loss: 1.1334\n",
      "Epoch 232 | Loss: 1.0721\n",
      "Epoch 233 | Loss: 1.1770\n",
      "Epoch 234 | Loss: 0.9049\n",
      "Epoch 235 | Loss: 1.0003\n",
      "Epoch 236 | Loss: 1.1308\n",
      "Epoch 237 | Loss: 1.0152\n",
      "Epoch 238 | Loss: 0.8966\n",
      "Epoch 239 | Loss: 0.8583\n",
      "Epoch 240 | Loss: 1.0482\n",
      "Epoch 241 | Loss: 1.0139\n",
      "Epoch 242 | Loss: 0.7587\n",
      "Epoch 243 | Loss: 0.6458\n",
      "Epoch 244 | Loss: 0.6767\n",
      "Epoch 245 | Loss: 0.7450\n",
      "Epoch 246 | Loss: 0.7253\n",
      "Epoch 247 | Loss: 0.6962\n",
      "Epoch 248 | Loss: 0.9118\n",
      "Epoch 249 | Loss: 1.2903\n",
      "Epoch 250 | Loss: 0.8315\n",
      "Epoch 251 | Loss: 0.6860\n",
      "Epoch 252 | Loss: 0.6012\n",
      "Epoch 253 | Loss: 0.6153\n",
      "Epoch 254 | Loss: 0.7275\n",
      "Epoch 255 | Loss: 0.6920\n",
      "Epoch 256 | Loss: 0.9126\n",
      "Epoch 257 | Loss: 0.6512\n",
      "Epoch 258 | Loss: 0.6006\n",
      "Epoch 259 | Loss: 0.5733\n",
      "Epoch 260 | Loss: 0.5667\n",
      "Epoch 261 | Loss: 2.1429\n",
      "Epoch 262 | Loss: 1.8435\n",
      "Epoch 263 | Loss: 0.8024\n",
      "Epoch 264 | Loss: 0.5690\n",
      "Epoch 265 | Loss: 0.4955\n",
      "Epoch 266 | Loss: 0.4910\n",
      "Epoch 267 | Loss: 0.5072\n",
      "Epoch 268 | Loss: 0.6808\n",
      "Epoch 269 | Loss: 0.8837\n",
      "Epoch 270 | Loss: 0.6656\n",
      "Epoch 271 | Loss: 0.4881\n",
      "Epoch 272 | Loss: 0.4180\n",
      "Epoch 273 | Loss: 0.4260\n",
      "Epoch 274 | Loss: 0.4461\n",
      "Epoch 275 | Loss: 0.6295\n",
      "Epoch 276 | Loss: 0.8261\n",
      "Epoch 277 | Loss: 0.7300\n",
      "Epoch 278 | Loss: 0.5937\n",
      "Epoch 279 | Loss: 0.5342\n",
      "Epoch 280 | Loss: 0.5947\n",
      "Epoch 281 | Loss: 0.6122\n",
      "Epoch 282 | Loss: 0.9965\n",
      "Epoch 283 | Loss: 0.5183\n",
      "Epoch 284 | Loss: 0.3644\n",
      "Epoch 285 | Loss: 0.3284\n",
      "Epoch 286 | Loss: 0.3138\n",
      "Epoch 287 | Loss: 0.2928\n",
      "Epoch 288 | Loss: 0.2914\n",
      "Epoch 289 | Loss: 0.2918\n",
      "Epoch 290 | Loss: 0.4737\n",
      "Epoch 291 | Loss: 0.9627\n",
      "Epoch 292 | Loss: 0.6892\n",
      "Epoch 293 | Loss: 0.4373\n",
      "Epoch 294 | Loss: 0.3898\n",
      "Epoch 295 | Loss: 0.4324\n",
      "Epoch 296 | Loss: 0.4808\n",
      "Epoch 297 | Loss: 0.8340\n",
      "Epoch 298 | Loss: 0.6166\n",
      "Epoch 299 | Loss: 0.6065\n",
      "Epoch 300 | Loss: 0.3682\n",
      "Epoch 301 | Loss: 0.3412\n",
      "Epoch 302 | Loss: 0.3279\n",
      "Epoch 303 | Loss: 0.4204\n",
      "Epoch 304 | Loss: 0.9388\n",
      "Epoch 305 | Loss: 0.5652\n",
      "Epoch 306 | Loss: 0.3245\n",
      "Epoch 307 | Loss: 0.2841\n",
      "Epoch 308 | Loss: 0.3220\n",
      "Epoch 309 | Loss: 0.2585\n",
      "Epoch 310 | Loss: 0.2426\n",
      "Epoch 311 | Loss: 0.2418\n",
      "Epoch 312 | Loss: 0.3277\n",
      "Epoch 313 | Loss: 0.2757\n",
      "Epoch 314 | Loss: 0.2460\n",
      "Epoch 315 | Loss: 0.2329\n",
      "Epoch 316 | Loss: 0.2440\n",
      "Epoch 317 | Loss: 0.3497\n",
      "Epoch 318 | Loss: 0.6504\n",
      "Epoch 319 | Loss: 1.0342\n",
      "Epoch 320 | Loss: 0.9246\n",
      "Epoch 321 | Loss: 0.5763\n",
      "Epoch 322 | Loss: 0.4036\n",
      "Epoch 323 | Loss: 0.4239\n",
      "Epoch 324 | Loss: 0.2928\n",
      "Epoch 325 | Loss: 0.2149\n",
      "Epoch 326 | Loss: 0.1844\n",
      "Epoch 327 | Loss: 0.1854\n",
      "Epoch 328 | Loss: 0.1756\n",
      "Epoch 329 | Loss: 0.2722\n",
      "Epoch 330 | Loss: 0.2274\n",
      "Epoch 331 | Loss: 0.2028\n",
      "Epoch 332 | Loss: 0.1740\n",
      "Epoch 333 | Loss: 0.1786\n",
      "Epoch 334 | Loss: 0.1591\n",
      "Epoch 335 | Loss: 0.1553\n",
      "Epoch 336 | Loss: 0.2433\n",
      "Epoch 337 | Loss: 0.3846\n",
      "Epoch 338 | Loss: 2.1014\n",
      "Epoch 339 | Loss: 1.3367\n",
      "Epoch 340 | Loss: 0.6469\n",
      "Epoch 341 | Loss: 0.3869\n",
      "Epoch 342 | Loss: 0.3156\n",
      "Epoch 343 | Loss: 0.3074\n",
      "Epoch 344 | Loss: 0.2831\n",
      "Epoch 345 | Loss: 0.2056\n",
      "Epoch 346 | Loss: 0.1834\n",
      "Epoch 347 | Loss: 0.1691\n",
      "Epoch 348 | Loss: 0.1713\n",
      "Epoch 349 | Loss: 0.1665\n",
      "Epoch 350 | Loss: 0.1549\n",
      "Epoch 351 | Loss: 0.1854\n",
      "Epoch 352 | Loss: 2.0845\n",
      "Epoch 353 | Loss: 0.8708\n",
      "Epoch 354 | Loss: 0.5136\n",
      "Epoch 355 | Loss: 0.4014\n",
      "Epoch 356 | Loss: 0.4735\n",
      "Epoch 357 | Loss: 0.3951\n",
      "Epoch 358 | Loss: 0.2449\n",
      "Epoch 359 | Loss: 0.1858\n",
      "Epoch 360 | Loss: 0.4595\n",
      "Epoch 361 | Loss: 0.4030\n",
      "Epoch 362 | Loss: 0.2427\n",
      "Epoch 363 | Loss: 0.1891\n",
      "Epoch 364 | Loss: 0.1585\n",
      "Epoch 365 | Loss: 0.1556\n",
      "Epoch 366 | Loss: 0.1366\n",
      "Epoch 367 | Loss: 0.1315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_dxdy \u001b[38;5;241m+\u001b[39m loss_eos_eod\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 37\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     40\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = DigitToStrokeLSTM().to(device)\n",
    "dx_dy_loss_fn = nn.MSELoss()\n",
    "eos_eod_loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 500\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (dig, input_seq, output_seq) in loader:\n",
    "        # stroke_seq: [batch, seq_len, 4]\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = output_seq.to(device)\n",
    "        dig = dig.to(device)\n",
    "\n",
    "        pred_seq, hidden = model(input_seq, onehot_digit = dig)  # [batch, seq_len-1, 4]\n",
    "\n",
    "        # Separate predictions\n",
    "        pred_dxdy = pred_seq[..., :2]         # [batch, seq_len-1, 2]\n",
    "        pred_eos_eod = pred_seq[..., 2:]      # [batch, seq_len-1, 2]\n",
    "\n",
    "        # Separate targets\n",
    "        target_dxdy = target_seq[..., :2]\n",
    "        target_eos_eod = target_seq[..., 2:]\n",
    "\n",
    "        # Compute losses\n",
    "        loss_dxdy = dx_dy_loss_fn(pred_dxdy, target_dxdy)\n",
    "        loss_eos_eod = eos_eod_loss_fn(pred_eos_eod, target_eos_eod)\n",
    "\n",
    "        loss = loss_dxdy + loss_eos_eod\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f0d776b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(initial_input, onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(max_length-1):\n",
    "        output, hidden = model(output, hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8e548538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAK8AAACuCAYAAABAzl3QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAQSklEQVR4nO3de3RU1b0H8O9kJsnkQSZvEvJ+EEhCIAQQAXkoCFrwvQq2xVZtkWsL6vLict17kYu9rde6KKVXhVKvbVGsWq+IPIoUNQQQFSEhEAIJCU0g70lIMplk3nPuHzM5mZOZIZOQzDmH/fv8NfvMyVqb5Mte++yzHwqO4zgQIkMBYleAkJGi8BLZovAS2aLwEtmi8BLZovAS2aLwEtmi8BLZovAS2aLwEtmi8BLZovAS2aLwEtmi8BLZUoldAXJjHMfhq5oOXO8zIy48GHHjghEXHoyIEBUUCoXY1RMVhVfith6pxutf1rhdVwUoEBkahNmZ0XjtkakIC2bvT0ndBgnbdbLOY3ABwGrn0K434eC5ZpRUa/1cM2lg77+rTBw414TN+y/w5dW3pyIsSAVtjwlavQk1bXo0dxsBOILMIgqvBJ2sacfzH5ajf4HWL+7MwgvLJgvu+f3nl/G7z6sBAOMY7DIA1G2QnIrGbjz17hmYbXYAwMqZydiwdJLbfXqThf/MYn8XoPBKSq/JiqfeOQ29yQoAWJIbj1ceKvA4qtB/DwCEU3iJ2F7/sgZNzn5sUWokXv9BEVRK9z+R1WbHd3WdfDkyNNBvdZQSCq9E1LTp8faJKwCAIGUAtq4sREiQ0uO9fzvdgJo2PQBgemokEjVqv9VTSii8EsBxHDbvuwCLzfGEtnZhJtJjwzzeqzdZsfVIFV/euDyX2ZcVFF4JOFTRghM17QCApMgQ/HxRttd7d5bUol1vBgB8ryABM9Ki/VJHKaLwiqzPbMWvDlTy5U335XntLjR3G/DWcUfXIlCpwIv3TPZ4HysovCJ7w+UhbWFOHJbmjfd675bD1TBaHENoP56TjrQYz10LVlB4RVSr1fMtaZAyAJvvz/faf61o7MaesgYAgCYkEOvv8t61YAWFVySDH9LWLMhAhpeHNI7j8MrfL/Jv3NbflY3I0CB/VVWyKLwiOXyhBccvDzyk/eJO7y3prpN1OFnbAQBIjQ7FY3PS/FJHqaPwiqDPbMUv9w88pL20IhehQZ7fkp2uu45fHbzIlzetyEOwyvMDHWsovCJ4s3jgIW3+xFgsy0/weF9bjxE/f6+UnzW2dkEmltzggY41FF4/u6LV461j/wTgGO562ctDmsVmx7r3ytDWYwIAzMmMwQvL3CfosIzC60ccx2Hz/kp+xtia+ZnIjAv3eO+rhy7hVN11AEBChBqv/3C6x3kOLKPfhh8drdLimHPVwwSNGuu8DHftL2/C2ycGWuftq4sQGx7st3rKBYXXT2x2Dr/57BJf/vflnh/Sqlt78OLH5/jypvvyUZQa5Zc6yg2F108+PduISy09AIBpyRosL0h0u6fHaMG/vHsGfWYbAODhoiSsnp3q13rKCYXXD4wWG377j2q+/OK9k90e0jiOw4aPynGlvRcAkJsYgV8/6HkiOnGg8PrB7m/q0dhlAOCYvzA3K9btnh0ltTh8oRUAEKFW4Q+ri7xO0CEOFN4xpjNa8EaxY/m6QgGPM8H2ljXitc8G5uhue7SQ+Uk3vqDwjrGdJbXo6nMslnywMAl5EyIE35dUa7Hho3K+/PzdObhrMr2I8AWFdwy16Yz8kFeQMgDP350j+L78Whee3n2Gf4P2o9mpNFtsGCi8Y2jbF5f5+bc/uj0VKdGh/HdXtHo88Zfv+JGFe/IT8MsHptAD2jBQeMdIrVaPD7+7BsCxNH2dy6yxVp0Rj719Ctd7Hct5ZmdEY9ujhVAGUHCHg8I7RrYcroLNZUJNjPMNWbfBgp/86RQ/+pCbGIG3fjIT6kAaWRguCu8YKLvaiUMVLQCA2PBg/HR+BgDHeO+ad07zLyuSo0Kw64lZiFCzue/CzaLwjjKO4/DqoYHXwM8tmYjQIBVsdg7PflCGU/90TLaJCQvCuz+djfgINvdcGA0U3lF2tFqLb50BzYgNw6pZKeA4Dhv3VvAvIUKDlPjzE7O8LvshvqHwjiKbncNvXFrdDUsnIVAZgG2fX8b7p64CcMwS2/nYDExNjhSplrcOCu8oGjz55nsFCdhb1ojff3GZv2fL96dh/sQ4sap4S6HwjhKT1X3yTUOnARv3VvDXNq3IwwOFSWJU75bE5t6YY+Bvpxv44a8FOXG4LT0aj/7xG34r0keKkvHkHRliVvGWQy3vKDBZbdhePHB2xIalOdhxtBan6x3bkKZEh2Dz/XliVe+WReEdBR9+d40/H2JJbjzsnOPVMAAEKIDfrSzEOBrLHXUU3ptktNjwpkur+9SCLDz3QRn/dm3dndmYmc7uTo5jicJ7k94/dRWtOsfy9KV547GntAF1HX0AgGkpkVi/eKKY1bulUXhvgtFiw/ajtXw5b0IEPnBOxgkNUmLbqkIE0nL1MUO/2Zuw+5t6aJ2bgsxIi8Kuk3X8d5tW5NEbtDFG4R2hPrMVfygZaHUbOw3odK6YWJo3HqtmpYhVNWZQeEdo9zf1/Pb6ANCic4w2xI8LxquPTKVJ5X5A4R2BPrMVO0uuePxuy/enITqM9s71BwrvCLzzdT06es1u15+Yl44FOTRvwV8ovMOkN1mx06Wv2y9nfDjzB5z4G4V3mHadrOMfzFz9+qECWsrjZxTeYegxWvgDUFwVpkRiZhpthudvFN5h+MtXdfwGIq7WzM+k0QURUHh9pPPS6iZHhWBZPu1wIwYKr4/+fKIOOqPV7fqT8zJox3KR0G/dB90GC/73hHurO06twkp6kyYaCq8P3v26Dj0eWt0f3paK8GBajCIWCq8PzjhXRLhSBSjw+Lx0/1eG8Ci8PugyuI8wrJiaiERNiAi1If0ovD7o9jA89rP5mSLUhLii8PpgcMs7JzMGU5I0ItWG9KPwDsFu59DVJ5yEs2YBLWGXAgrvEHpMVjjXUgIAsuLCsCgnXrwKER6FdwiD+7s/m5+JANoEWhIovENo7TEKyg9Np+2apILCO4R9Z5sEZZr2KB0U3iG89209//mObPfD/4h4KLw3YLTYBA9ry6e6nxdMxEPhvYHiS22CcpzzUBQiDRTeGzhwvllQjgqjzfKkhMLrRZ/Zii8vClteTQgtaZcSCq8XX1xsg8FiE1yLDKWWV0oovF4cONfkdk0TQuGVEgqvB3qTFcVVWsG18GAV7fgoMfTX8ODzylaYrXbBNWp1pYfC64GnLgP1d6WHwjtIt8GCY9XtACBYn0bhlR4K7yBHKlthtjm6DHOzYvjrkTRMJjkU3kH2lQ90GeZPHJjLQC2v9FB4XbTqjDhx2THKkBwVApPLQ1sMvRqWHAqvi0/PNvITce7JT8AOl8NS7p2SIFKtiDcUXieO4/DxmUa+3G2w8BtIr5iaiNzECLGqRryg8DpVNutQ1eo4sT07PhyHL7QAcJxg+dySHDGrRryg8DrtKR1odTt7zfymeg8XJSM7PlysapEboPACsNrs+NRluU9/dyFQqcCzdIKlZFF4ARyvaUe73uR2fdWsFKREh4pQI+ILCi+EXYZ+waoArL+LWl0pYz68OqMF/3A+nLl67PY0jI9Qi1Aj4ivmw3vofLPgZQTgOPT66UVZItWI+Ir58B441+x27cl5GfRGTQaYD+/F5h5BOUKtwpoFtH2pHDAd3h6jxW2UYe3CLJp4LhNMh7e+o09QjgkLwuNz08WpDBk2psNb19ErKD+9KAthdECKbLAd3nZheB+9LVWkmpCRYDq8V7TC8NKxVPLCdHiP17Tzn2kHSPlhOrzanoGRhqLUSPEqQkaE2fD2moQnWk5KoMnmcsNseFVK4bkSkxLGiVQTMlLMhjdYJdyePz2Gpj7KDbPhNQ7aAVJJJ/zIDrPhrWnTC8q9ZpuXO4lUMRvei806QblNZ/RyJ5EqZsNb1SKcTeY6bEbkgd3wtg4Kr4c1bETamA3vpUEt71cub9uIPDAZ3uu9ZrduwvunruGTsgaRakRGgsnwXmoZeFgLUg38Cv5tz3lUNuk8/QiRICbD23DdwH/euDwXq2amAACMFjvW7j6Nrj6zWFUjw8BkeAe/Gn75gXxMS9YAAK5dN+DZD87C5npuK5EkJsPrenK70WKDOlCJHatnICbMsft5SbUWfz11VazqER8xGt6Bf7bR4tizYUJkCP7nB9P56zuKa9xOBCLSwmZ4VcKWt9+87FgsnhwPAGjqNuLjUhp9kDImwxvsoeXtt95lV8g3i2tgsVHrK1Vshtel5TVZhRNyClMisTAnDgDQ0GnAJ2Xum/ARaWAyvMIHNveW9ZnF2fznN4trYKXWV5IYDa9Lt8HqPhVyRlo05mU7zmCr7+gTHG9FpIPR8Lp0Gyye5/E+47I37xtf1tC4rwQxGd5glfcHtn6zM2MwOyMaAHClvRcHz7vvJknExWR4BS2vh25DP9fzKLYX14DjqPWVEibDG6gM4NeseWt5AWBOVgwKUyIBOKZQDt4OlYiLyfACgNrZdRi8ENOVQqHAI0VJfJke3KSF3fA6uw6eRhtc3VuQyLfS+8ubqOsgIcyH13SDbgMAxIYHY26WY9isscuA0qudY1434htmwxvsQ7eh3/3TJvCf952lroNUsBtevtsw9NuzZVMS+BUXB883w2qzQz9orzPif8yGNzbcMXfXbLWjqctww3sj1IG4c5JjvkO73owHt3+Fxb89iuu9tOJCTMyGt38IDADO1A/dj71/2sCoQ0WjDq06EzbuPU8PcCJiNrwz0qL4z76Ed3FuPMKChJvz/f18Cw2fiYjZ8E5PHV541YFKLM1PcLv+0t4KtHTTVlFiYDa8mpBA5IwPBwBUNuvQZx76Acx11KGfzmjFC/9XTt0HETAbXsAx9REAbHYOZ691DXn/HRNjERU6cMDgOOcBLMcvt2P3t7Rg098YD+9A16HUh65DoDIA9xYk8uW788fzn185eNHtaCwytii8Tqd9CC8g7Dp09Vmw+nbH2W0Giw3/+lE5zfv1I6bDmx4Tyu/VUFrfCbsPwbstPRoJEWoAwLFqLZ5elI0055EAZ+o78cdjV8auwkSA6fAqFAoUOVtfndGKWq1+iJ8AAgIUWDHV0XWw2jmUVGnx3w8X8N9vPVKF7j7L2FSYCDAdXmCEXYdCl7kO5Y3YcriKLyuggMGH+RLk5jEf3pku4f26tsOnnylI0uCZxROx+b481LX3ofRqFwDHws6tq6YhQaMei6qSQZg/bHdKkgZBygCYbXbsK2+CxWbHyw/kI36c9wAqFApkx4fjhY/KYXJO7EnUqPHWj2diSpLGX1VnHvMtrzpQibULM/nyoYoW3L31GPaUNnh88WC3c3jts0t45v0yPrgz0qKwb90dFFw/U3D0aggcx+HAuWb8574LgpliiybFYWpyJJq7DGjRGdHcbURzl0Fw7NXKmcn4rwenuB1KSMYehddFh96El/dX+jTZJkABvLQiD4/PTYdCQQcQioHC68GRylb8xyfn0Tbo3Ap1YAAmaEKQFhOKNQsyMTcrVqQaEoDC61WP0YKTtR0IVCqQqAlBokYNTUggtbISQuElssX8aAORLwovkS0KL5EtCi+RLQovkS0KL5EtCi+RLQovkS0KL5EtCi+RLQovkS0KL5EtCi+RLQovka3/B7aMNhE2WrAdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ac7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
