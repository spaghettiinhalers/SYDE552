{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence, save_path=None, show=True):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.plot(xs, ys, linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162670",
   "metadata": {},
   "source": [
    "# 1. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c63c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.loadtxt('good_indexes.txt').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ded58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [[] for _ in range(10)]\n",
    "\n",
    "for i in indexes:\n",
    "    try:\n",
    "        data = np.loadtxt(f'../sequences/testimg-{i}-targetdata.txt', delimiter=' ')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found at path: {i}\")\n",
    "        continue\n",
    "    \n",
    "    inputOneshot = data[0, 0:10]\n",
    "    outputStrokes = data[:, 10:]\n",
    "    outputStrokes[:, 0] = outputStrokes[:, 0]/28\n",
    "    outputStrokes[:, 1] = outputStrokes[:, 1]/28\n",
    "    \n",
    "    datas[getNumFromOneHot(inputOneshot)].append(outputStrokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a163eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount = min([len(x) for x in datas])\n",
    "amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd22b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[i] = 1\n",
    "    \n",
    "    smallest_10 = sorted(datas[i], key=len)[:amount]\n",
    "    for k in smallest_10:\n",
    "        input_data.append(temp_onehot)\n",
    "        output_data.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cd2c493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the max length of a sequence\n",
    "max_length = 0\n",
    "j = 0\n",
    "for i in range(len(output_data)):\n",
    "    if len(output_data[i]) > max_length:\n",
    "        max_length = len(output_data[i])\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "642bf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences so that they are all the same size (good for batching)\n",
    "padded_output_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_output_data[i, :len(output_data[i]), :] = output_data[i]\n",
    "    padded_output_data[i, len(output_data[i]):, :] = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37cfe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_input_data[i, 0, :] = [0, 0, 0, 0]\n",
    "    padded_input_data[i, 1:, :] = padded_output_data[i, :max_length-1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa50ea8",
   "metadata": {},
   "source": [
    "# 2. Setting up the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8523173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, onehot, inputs, outputstroke):\n",
    "        self.digit = onehot                     # shape: [N]\n",
    "        self.inputstroke = inputs               # list of [seq_len, 4] arrays\n",
    "        self.outputstroke = outputstroke        # list of [seq_len, 4] arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digit)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digit[idx]\n",
    "        inputs = self.inputstroke[idx]\n",
    "        outputs = self.outputstroke[idx]\n",
    "        return torch.tensor(digit, dtype=torch.float32), torch.tensor(inputs, dtype=torch.float32), torch.tensor(outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1171d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokeDataset = StrokeDataset(input_data, padded_input_data, padded_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d67af144",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(strokeDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0385e",
   "metadata": {},
   "source": [
    "# 3. Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e58d8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97fe67",
   "metadata": {},
   "source": [
    "# 4. Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92069110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "dx_dy_loss_fn = nn.MSELoss()\n",
    "eos_eod_loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7354307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.0020\n",
      "Epoch 2 | Loss: 0.0018\n",
      "Epoch 3 | Loss: 0.0023\n",
      "Epoch 4 | Loss: 0.0021\n",
      "Epoch 5 | Loss: 0.0014\n",
      "Epoch 6 | Loss: 0.0015\n",
      "Epoch 7 | Loss: 0.0018\n",
      "Epoch 8 | Loss: 0.0016\n",
      "Epoch 9 | Loss: 0.0022\n",
      "Epoch 10 | Loss: 0.0021\n",
      "Epoch 11 | Loss: 0.0017\n",
      "Epoch 12 | Loss: 0.0021\n",
      "Epoch 13 | Loss: 0.0022\n",
      "Epoch 14 | Loss: 0.0018\n",
      "Epoch 15 | Loss: 0.0016\n",
      "Epoch 16 | Loss: 0.0023\n",
      "Epoch 17 | Loss: 0.0016\n",
      "Epoch 18 | Loss: 0.0016\n",
      "Epoch 19 | Loss: 0.0021\n",
      "Epoch 20 | Loss: 0.0019\n",
      "Epoch 21 | Loss: 0.0016\n",
      "Epoch 22 | Loss: 0.0022\n",
      "Epoch 23 | Loss: 0.0019\n",
      "Epoch 24 | Loss: 0.0017\n",
      "Epoch 25 | Loss: 0.0020\n",
      "Epoch 26 | Loss: 0.0015\n",
      "Epoch 27 | Loss: 0.0015\n",
      "Epoch 28 | Loss: 0.0017\n",
      "Epoch 29 | Loss: 0.0016\n",
      "Epoch 30 | Loss: 0.0025\n",
      "Epoch 31 | Loss: 0.0017\n",
      "Epoch 32 | Loss: 0.0020\n",
      "Epoch 33 | Loss: 0.0020\n",
      "Epoch 34 | Loss: 0.0019\n",
      "Epoch 35 | Loss: 0.0019\n",
      "Epoch 36 | Loss: 0.0019\n",
      "Epoch 37 | Loss: 0.0017\n",
      "Epoch 38 | Loss: 0.0014\n",
      "Epoch 39 | Loss: 0.0019\n",
      "Epoch 40 | Loss: 0.0021\n",
      "Epoch 41 | Loss: 0.0062\n",
      "Epoch 42 | Loss: 0.0059\n",
      "Epoch 43 | Loss: 0.0084\n",
      "Epoch 44 | Loss: 0.0320\n",
      "Epoch 45 | Loss: 0.0746\n",
      "Epoch 46 | Loss: 0.1144\n",
      "Epoch 47 | Loss: 0.0810\n",
      "Epoch 48 | Loss: 0.0639\n",
      "Epoch 49 | Loss: 0.0410\n",
      "Epoch 50 | Loss: 0.0306\n",
      "Epoch 51 | Loss: 0.0238\n",
      "Epoch 52 | Loss: 0.0143\n",
      "Epoch 53 | Loss: 0.0166\n",
      "Epoch 54 | Loss: 0.0115\n",
      "Epoch 55 | Loss: 0.0084\n",
      "Epoch 56 | Loss: 0.0058\n",
      "Epoch 57 | Loss: 0.0053\n",
      "Epoch 58 | Loss: 0.0051\n",
      "Epoch 59 | Loss: 0.0052\n",
      "Epoch 60 | Loss: 0.0040\n",
      "Epoch 61 | Loss: 0.0030\n",
      "Epoch 62 | Loss: 0.0034\n",
      "Epoch 63 | Loss: 0.0034\n",
      "Epoch 64 | Loss: 0.0031\n",
      "Epoch 65 | Loss: 0.0035\n",
      "Epoch 66 | Loss: 0.0042\n",
      "Epoch 67 | Loss: 0.0027\n",
      "Epoch 68 | Loss: 0.0043\n",
      "Epoch 69 | Loss: 0.0025\n",
      "Epoch 70 | Loss: 0.0026\n",
      "Epoch 71 | Loss: 0.0027\n",
      "Epoch 72 | Loss: 0.0025\n",
      "Epoch 73 | Loss: 0.0024\n",
      "Epoch 74 | Loss: 0.0023\n",
      "Epoch 75 | Loss: 0.0027\n",
      "Epoch 76 | Loss: 0.0027\n",
      "Epoch 77 | Loss: 0.0023\n",
      "Epoch 78 | Loss: 0.0023\n",
      "Epoch 79 | Loss: 0.0025\n",
      "Epoch 80 | Loss: 0.0028\n",
      "Epoch 81 | Loss: 0.0022\n",
      "Epoch 82 | Loss: 0.0030\n",
      "Epoch 83 | Loss: 0.0028\n",
      "Epoch 84 | Loss: 0.0028\n",
      "Epoch 85 | Loss: 0.0023\n",
      "Epoch 86 | Loss: 0.0042\n",
      "Epoch 87 | Loss: 0.0026\n",
      "Epoch 88 | Loss: 0.0028\n",
      "Epoch 89 | Loss: 0.0025\n",
      "Epoch 90 | Loss: 0.0021\n",
      "Epoch 91 | Loss: 0.0022\n",
      "Epoch 92 | Loss: 0.0020\n",
      "Epoch 93 | Loss: 0.0023\n",
      "Epoch 94 | Loss: 0.0025\n",
      "Epoch 95 | Loss: 0.0021\n",
      "Epoch 96 | Loss: 0.0027\n",
      "Epoch 97 | Loss: 0.0022\n",
      "Epoch 98 | Loss: 0.0021\n",
      "Epoch 99 | Loss: 0.0028\n",
      "Epoch 100 | Loss: 0.0033\n",
      "Epoch 101 | Loss: 0.0020\n",
      "Epoch 102 | Loss: 0.0028\n",
      "Epoch 103 | Loss: 0.0029\n",
      "Epoch 104 | Loss: 0.0023\n",
      "Epoch 105 | Loss: 0.0021\n",
      "Epoch 106 | Loss: 0.0022\n",
      "Epoch 107 | Loss: 0.0027\n",
      "Epoch 108 | Loss: 0.0019\n",
      "Epoch 109 | Loss: 0.0018\n",
      "Epoch 110 | Loss: 0.0021\n",
      "Epoch 111 | Loss: 0.0026\n",
      "Epoch 112 | Loss: 0.0024\n",
      "Epoch 113 | Loss: 0.0024\n",
      "Epoch 114 | Loss: 0.0038\n",
      "Epoch 115 | Loss: 0.0019\n",
      "Epoch 116 | Loss: 0.0020\n",
      "Epoch 117 | Loss: 0.0029\n",
      "Epoch 118 | Loss: 0.0024\n",
      "Epoch 119 | Loss: 0.0019\n",
      "Epoch 120 | Loss: 0.0025\n",
      "Epoch 121 | Loss: 0.0021\n",
      "Epoch 122 | Loss: 0.0022\n",
      "Epoch 123 | Loss: 0.0028\n",
      "Epoch 124 | Loss: 0.0018\n",
      "Epoch 125 | Loss: 0.0020\n",
      "Epoch 126 | Loss: 0.0018\n",
      "Epoch 127 | Loss: 0.0021\n",
      "Epoch 128 | Loss: 0.0030\n",
      "Epoch 129 | Loss: 0.0028\n",
      "Epoch 130 | Loss: 0.0021\n",
      "Epoch 131 | Loss: 0.0023\n",
      "Epoch 132 | Loss: 0.0020\n",
      "Epoch 133 | Loss: 0.0025\n",
      "Epoch 134 | Loss: 0.0022\n",
      "Epoch 135 | Loss: 0.0043\n",
      "Epoch 136 | Loss: 0.0262\n",
      "Epoch 137 | Loss: 0.0535\n",
      "Epoch 138 | Loss: 0.0396\n",
      "Epoch 139 | Loss: 0.0235\n",
      "Epoch 140 | Loss: 0.0311\n",
      "Epoch 141 | Loss: 0.0202\n",
      "Epoch 142 | Loss: 0.0205\n",
      "Epoch 143 | Loss: 0.0182\n",
      "Epoch 144 | Loss: 0.0318\n",
      "Epoch 145 | Loss: 0.0188\n",
      "Epoch 146 | Loss: 0.0148\n",
      "Epoch 147 | Loss: 0.0084\n",
      "Epoch 148 | Loss: 0.0100\n",
      "Epoch 149 | Loss: 0.0096\n",
      "Epoch 150 | Loss: 0.0093\n",
      "Epoch 151 | Loss: 0.0053\n",
      "Epoch 152 | Loss: 0.0049\n",
      "Epoch 153 | Loss: 0.0040\n",
      "Epoch 154 | Loss: 0.0037\n",
      "Epoch 155 | Loss: 0.0038\n",
      "Epoch 156 | Loss: 0.0038\n",
      "Epoch 157 | Loss: 0.0056\n",
      "Epoch 158 | Loss: 0.0059\n",
      "Epoch 159 | Loss: 0.0035\n",
      "Epoch 160 | Loss: 0.0029\n",
      "Epoch 161 | Loss: 0.0035\n",
      "Epoch 162 | Loss: 0.0031\n",
      "Epoch 163 | Loss: 0.0029\n",
      "Epoch 164 | Loss: 0.0041\n",
      "Epoch 165 | Loss: 0.0023\n",
      "Epoch 166 | Loss: 0.0038\n",
      "Epoch 167 | Loss: 0.0033\n",
      "Epoch 168 | Loss: 0.0022\n",
      "Epoch 169 | Loss: 0.0020\n",
      "Epoch 170 | Loss: 0.0021\n",
      "Epoch 171 | Loss: 0.0022\n",
      "Epoch 172 | Loss: 0.0025\n",
      "Epoch 173 | Loss: 0.0023\n",
      "Epoch 174 | Loss: 0.0034\n",
      "Epoch 175 | Loss: 0.0020\n",
      "Epoch 176 | Loss: 0.0035\n",
      "Epoch 177 | Loss: 0.0021\n",
      "Epoch 178 | Loss: 0.0025\n",
      "Epoch 179 | Loss: 0.0022\n",
      "Epoch 180 | Loss: 0.0020\n",
      "Epoch 181 | Loss: 0.0024\n",
      "Epoch 182 | Loss: 0.0023\n",
      "Epoch 183 | Loss: 0.0019\n",
      "Epoch 184 | Loss: 0.0021\n",
      "Epoch 185 | Loss: 0.0022\n",
      "Epoch 186 | Loss: 0.0018\n",
      "Epoch 187 | Loss: 0.0023\n",
      "Epoch 188 | Loss: 0.0024\n",
      "Epoch 189 | Loss: 0.0019\n",
      "Epoch 190 | Loss: 0.0031\n",
      "Epoch 191 | Loss: 0.0022\n",
      "Epoch 192 | Loss: 0.0021\n",
      "Epoch 193 | Loss: 0.0018\n",
      "Epoch 194 | Loss: 0.0019\n",
      "Epoch 195 | Loss: 0.0023\n",
      "Epoch 196 | Loss: 0.0019\n",
      "Epoch 197 | Loss: 0.0022\n",
      "Epoch 198 | Loss: 0.0020\n",
      "Epoch 199 | Loss: 0.0021\n",
      "Epoch 200 | Loss: 0.0024\n",
      "Epoch 201 | Loss: 0.0020\n",
      "Epoch 202 | Loss: 0.0023\n",
      "Epoch 203 | Loss: 0.0022\n",
      "Epoch 204 | Loss: 0.0022\n",
      "Epoch 205 | Loss: 0.0025\n",
      "Epoch 206 | Loss: 0.0017\n",
      "Epoch 207 | Loss: 0.0021\n",
      "Epoch 208 | Loss: 0.0019\n",
      "Epoch 209 | Loss: 0.0018\n",
      "Epoch 210 | Loss: 0.0018\n",
      "Epoch 211 | Loss: 0.0017\n",
      "Epoch 212 | Loss: 0.0017\n",
      "Epoch 213 | Loss: 0.0021\n",
      "Epoch 214 | Loss: 0.0021\n",
      "Epoch 215 | Loss: 0.0017\n",
      "Epoch 216 | Loss: 0.0022\n",
      "Epoch 217 | Loss: 0.0025\n",
      "Epoch 218 | Loss: 0.0019\n",
      "Epoch 219 | Loss: 0.0019\n",
      "Epoch 220 | Loss: 0.0022\n",
      "Epoch 221 | Loss: 0.0020\n",
      "Epoch 222 | Loss: 0.0018\n",
      "Epoch 223 | Loss: 0.0026\n",
      "Epoch 224 | Loss: 0.0023\n",
      "Epoch 225 | Loss: 0.0020\n",
      "Epoch 226 | Loss: 0.0022\n",
      "Epoch 227 | Loss: 0.0020\n",
      "Epoch 228 | Loss: 0.0019\n",
      "Epoch 229 | Loss: 0.0028\n",
      "Epoch 230 | Loss: 0.0017\n",
      "Epoch 231 | Loss: 0.0026\n",
      "Epoch 232 | Loss: 0.0020\n",
      "Epoch 233 | Loss: 0.0019\n",
      "Epoch 234 | Loss: 0.0017\n",
      "Epoch 235 | Loss: 0.0023\n",
      "Epoch 236 | Loss: 0.0021\n",
      "Epoch 237 | Loss: 0.0021\n",
      "Epoch 238 | Loss: 0.0026\n",
      "Epoch 239 | Loss: 0.0019\n",
      "Epoch 240 | Loss: 0.0018\n",
      "Epoch 241 | Loss: 0.0020\n",
      "Epoch 242 | Loss: 0.0017\n",
      "Epoch 243 | Loss: 0.0026\n",
      "Epoch 244 | Loss: 0.0017\n",
      "Epoch 245 | Loss: 0.0019\n",
      "Epoch 246 | Loss: 0.0018\n",
      "Epoch 247 | Loss: 0.0018\n",
      "Epoch 248 | Loss: 0.0021\n",
      "Epoch 249 | Loss: 0.0026\n",
      "Epoch 250 | Loss: 0.0032\n",
      "Epoch 251 | Loss: 0.0021\n",
      "Epoch 252 | Loss: 0.0020\n",
      "Epoch 253 | Loss: 0.0024\n",
      "Epoch 254 | Loss: 0.0019\n",
      "Epoch 255 | Loss: 0.0019\n",
      "Epoch 256 | Loss: 0.0022\n",
      "Epoch 257 | Loss: 0.0019\n",
      "Epoch 258 | Loss: 0.0026\n",
      "Epoch 259 | Loss: 0.0021\n",
      "Epoch 260 | Loss: 0.0025\n",
      "Epoch 261 | Loss: 0.0023\n",
      "Epoch 262 | Loss: 0.0021\n",
      "Epoch 263 | Loss: 0.0016\n",
      "Epoch 264 | Loss: 0.0017\n",
      "Epoch 265 | Loss: 0.0018\n",
      "Epoch 266 | Loss: 0.0020\n",
      "Epoch 267 | Loss: 0.0017\n",
      "Epoch 268 | Loss: 0.0017\n",
      "Epoch 269 | Loss: 0.0015\n",
      "Epoch 270 | Loss: 0.0017\n",
      "Epoch 271 | Loss: 0.0019\n",
      "Epoch 272 | Loss: 0.0022\n",
      "Epoch 273 | Loss: 0.0019\n",
      "Epoch 274 | Loss: 0.0020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m output_seq\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m dig \u001b[38;5;241m=\u001b[39m dig\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 16\u001b[0m pred_seq, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_digit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, seq_len-1, 4]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Separate predictions\u001b[39;00m\n\u001b[0;32m     19\u001b[0m pred_dxdy \u001b[38;5;241m=\u001b[39m pred_seq[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m2\u001b[39m]         \u001b[38;5;66;03m# [batch, seq_len-1, 2]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m, in \u001b[0;36mDigitToStrokeLSTM.forward\u001b[1;34m(self, x, hidden, onehot_digit)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hidden \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m onehot_digit \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size),\n\u001b[0;32m     43\u001b[0m               torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m---> 45\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n\u001b[0;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_head(out)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "best_model_wts = None\n",
    "best_loss = 99999\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (dig, input_seq, output_seq) in loader:\n",
    "        # stroke_seq: [batch, seq_len, 4]\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = output_seq.to(device)\n",
    "        dig = dig.to(device)\n",
    "\n",
    "        pred_seq, hidden = model(input_seq, onehot_digit = dig)  # [batch, seq_len-1, 4]\n",
    "\n",
    "        # Separate predictions\n",
    "        pred_dxdy = pred_seq[..., :2]         # [batch, seq_len-1, 2]\n",
    "        pred_eos_eod = pred_seq[..., 2:]      # [batch, seq_len-1, 2]\n",
    "\n",
    "        # Separate targets\n",
    "        target_dxdy = target_seq[..., :2]\n",
    "        target_eos_eod = target_seq[..., 2:]\n",
    "\n",
    "        # Compute losses\n",
    "        loss_dxdy = dx_dy_loss_fn(pred_dxdy, target_dxdy)\n",
    "        loss_eos_eod = eos_eod_loss_fn(pred_eos_eod, target_eos_eod)\n",
    "\n",
    "        loss = loss_dxdy + loss_eos_eod\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        if total_loss < best_loss:\n",
    "            best_loss = total_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c3b849d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0d776b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(initial_input, onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(max_length-1):\n",
    "        output, hidden = model(output, hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e548538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8c1ac7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if good\n",
    "torch.save(model.state_dict(), 'model_weights/sketch_model_weights2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8e579b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitToStrokeLSTM(\n",
       "  (embedding): Linear(in_features=10, out_features=512, bias=True)\n",
       "  (lstm): LSTM(4, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (output_head): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "model.load_state_dict(torch.load('model_weights/sketch_model_weights2.pth', weights_only=True))\n",
    "model.eval()  # set to evaluation mode if you're doing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39ec9780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
