{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from PIL import Image\n",
    "import io\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    # Load onto variable img_array\n",
    "    plt.figure(figsize=(1, 1), dpi=28)  # 1 inch * 28 dpi = 28 pixels\n",
    "    plt.plot(xs, ys, color = \"black\", linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    # Use a BytesIO buffer to save the plot into memory\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', transparent=False, facecolor='white')\n",
    "    plt.close()\n",
    "    buf.seek(0)  # Rewind the buffer to the beginning\n",
    "    img = Image.open(buf)  # Open the image from the buffer\n",
    "    \n",
    "    img_array = np.array(img.convert('L'))  # Convert to grayscale (1 channel) as a numpy array\n",
    "    \n",
    "    buf.close()  # Close the buffer\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162670",
   "metadata": {},
   "source": [
    "# 1. Loading the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28e6bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e964c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # First Convolution Block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Second Convolution Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Fully Connected (Dense) layers\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64 * 7 * 7, 512),  # Input size depends on the output of convolutional layers\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 10)  # Output layer (10 classes)\n",
    "        )\n",
    "            \n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94ed0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw the number using the RNN\n",
    "\n",
    "def generate_text(model, number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(initial_input, onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(62-1):\n",
    "        output, hidden = model(output, hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    return draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5611a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read an image and return the predicted digit\n",
    "\n",
    "def evaluate_img(model, img, display=False):\n",
    "    # Step 2: Preprocess the image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()  # Convert to PyTorch tensor (scales pixels to [0, 1])\n",
    "    ])\n",
    "\n",
    "    img_tensor = transform(img).to(device)\n",
    "\n",
    "    img_tensor[img_tensor<0.6] = 0\n",
    "    img_tensor[img_tensor>=0.6] = 1\n",
    "\n",
    "    img_tensor = 1-img_tensor\n",
    "\n",
    "    if display:\n",
    "        imgDisplay = img_tensor.squeeze(0).squeeze(0).cpu().numpy()  # Remove batch and channel dimensions\n",
    "\n",
    "        # Display the image using matplotlib\n",
    "        plt.imshow(imgDisplay, cmap='gray')\n",
    "        plt.axis('off')  # Turn off axis labels\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Step 3: Add batch dimension (PyTorch models expect a batch of images)\n",
    "    img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension (1, 1, 28, 28)\n",
    "\n",
    "    # Step 4: Pass the tensor to the model\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    with torch.no_grad():  # Turn off gradients since we're in inference mode\n",
    "        output = model(img_tensor)  # Pass the image tensor to the model for prediction\n",
    "\n",
    "    # Step 5: Interpret the output\n",
    "    _, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "    \n",
    "    # First return is the predicted class (int), and the second is an array containing confidences for each digit\n",
    "    return predicted_class.item(), nn.Softmax(dim=1)(output).detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9afaa2e",
   "metadata": {},
   "source": [
    "### Draw and Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a163eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 3\n",
      "Confidence of desired_digit = 0.7381200194358826\n"
     ]
    }
   ],
   "source": [
    "rnnModel = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "rnnModel.load_state_dict(torch.load(f'model_weights/sketch_model_weights1.pth', weights_only=True))\n",
    "rnnModel.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "cnnEvaluator = CNNModel().to(device)\n",
    "cnnEvaluator.load_state_dict(torch.load('../classify_cnn/cnn_weights/cnn_weights1.pth', weights_only=True))\n",
    "cnnEvaluator.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "\n",
    "# Test if the loaded model works\n",
    "desired_digit = 3 # <----- Change number and see it print it :)\n",
    "example_img = generate_text(rnnModel, desired_digit)\n",
    "predicted_class, confidence = evaluate_img(cnnEvaluator, example_img, display=True)\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "print(f'Confidence of desired_digit = {confidence[desired_digit]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b2677",
   "metadata": {},
   "source": [
    "### Check evaluate hand-writing with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4c72ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1 Desired Class: 0 Predicted Class: 7\n",
      "Model: 1 Desired Class: 6 Predicted Class: 5\n",
      "Model: 1 Desired Class: 9 Predicted Class: 4\n",
      "Model: 2 Desired Class: 8 Predicted Class: 7\n",
      "Model: 3 Desired Class: 6 Predicted Class: 5\n",
      "Model: 3 Desired Class: 9 Predicted Class: 4\n",
      "Model: 4 Desired Class: 9 Predicted Class: 7\n",
      "Model: 5 Desired Class: 6 Predicted Class: 5\n",
      "Model: 5 Desired Class: 9 Predicted Class: 7\n",
      "Accuracy 0.82\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "sum = 0\n",
    "\n",
    "for i in range(1, 6):\n",
    "    rnnModel = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "    rnnModel.load_state_dict(torch.load(f'model_weights/sketch_model_weights{i}.pth', weights_only=True))\n",
    "    rnnModel.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "    cnnEvaluator = CNNModel().to(device)\n",
    "    cnnEvaluator.load_state_dict(torch.load(f'../classify_cnn/cnn_weights/cnn_weights{i}.pth', weights_only=True))\n",
    "    cnnEvaluator.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "    for j in range(10):\n",
    "        # Test if the loaded model works\n",
    "        img = generate_text(rnnModel, j) # <----- Change number and see it print it :)\n",
    "        \n",
    "        predicted_class, _ = evaluate_img(cnnEvaluator, img, display=False)\n",
    "        \n",
    "        \n",
    "        if predicted_class == j:\n",
    "            sum += 1\n",
    "        else:\n",
    "            print(f'Model: {i} Desired Class: {j} Predicted Class: {predicted_class}')\n",
    "        total+= 1\n",
    "        \n",
    "print(f\"Accuracy {sum/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5455727",
   "metadata": {},
   "source": [
    "# 2. Lesion the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3ce103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAMAGE WEIGHTS\n",
    "\n",
    "def damage_smallest(model, p_smallest): # energy constraint\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_smallest == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            weight_magnitudes = tensor.abs().view(-1)\n",
    "            k = int(weight_magnitudes.numel() * p_smallest)\n",
    "\n",
    "            if k == 0:\n",
    "                continue\n",
    "            threshold = weight_magnitudes.kthvalue(k).values.item()\n",
    "\n",
    "            mask = tensor.abs() >= threshold\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "def damage_fas(model, p_block, p_reflect, p_filter):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_block + p_reflect + p_filter == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            flat_weights = tensor.view(-1)\n",
    "            nonzero_indices = (flat_weights!=0).nonzero(as_tuple=True)[0]\n",
    "            num_nonzero_indices = nonzero_indices.numel()\n",
    "            if num_nonzero_indices == 0:\n",
    "                continue\n",
    "\n",
    "            # percentage of weights damaged will be taken from the number of nonzero weights\n",
    "            # simulated fas damage occurs after energy constraint blockage\n",
    "            num_block = int(num_nonzero_indices * p_block)\n",
    "            num_reflect = int(num_nonzero_indices * p_reflect)\n",
    "            num_filter = int(num_nonzero_indices * p_filter)\n",
    "\n",
    "            shuffled_indices = nonzero_indices[torch.randperm(num_nonzero_indices, device=flat_weights.device)]\n",
    "\n",
    "            indices_block = shuffled_indices[:num_block]\n",
    "            indices_reflect = shuffled_indices[num_block:num_block+num_reflect]\n",
    "            indices_filter = shuffled_indices[num_block+num_reflect:num_block+num_reflect+num_filter]\n",
    "\n",
    "            # do damage\n",
    "            # blockage: set weights to 0\n",
    "            if p_block != 0:\n",
    "                flat_weights[indices_block] = 0\n",
    "\n",
    "            # reflect: halve weights\n",
    "            if p_reflect != 0:\n",
    "                flat_weights[indices_reflect] *= 0.5\n",
    "\n",
    "            # filter: low pass filter (lusch et al)\n",
    "            if p_filter != 0:\n",
    "                weights_to_filter = flat_weights[indices_filter]            # get weights before transformation\n",
    "                signs = torch.sign(weights_to_filter)                       # get signs of weights\n",
    "                abs_weights_to_filter = weights_to_filter.abs()             # get high_weight, should be in the 95th percentile for all weights\n",
    "                high_weight = torch.quantile(flat_weights.abs(), 0.95)      # scale weights to mostly between -1 and 1\n",
    "                x = abs_weights_to_filter / high_weight\n",
    "                transformed_weights = -0.2744 * x**2 + 0.9094 * x - 0.0192\n",
    "                gaussian_noise = torch.randn_like(transformed_weights) * 0.05\n",
    "                transformed_weights += gaussian_noise\n",
    "                transformed_weights = transformed_weights * signs * high_weight # rescale\n",
    "                flat_weights[indices_filter] = transformed_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d389cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAFvElEQVR4nO3cwWrcMBhGUavM+7+yuimXQikdi4zsuOesM0RJFpd/kW/MOecBAMdx/Lj6AQDchygAEFEAIKIAQEQBgIgCABEFACIKAOT17heOMT75DgA+7J3/VXYpABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYC8rn4AX2vOefUT+IcxxtVPgL9yKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgBjEexhja/e3Olrob8sOLgUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABCDeLDZ6rDdypCeET3OcikAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYAYxINvYmXczogeZ7kUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUA8rr6AcB75pynPzPG+MBLeDKXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQDyuvoB8L+Zcy59bozxxS+BP7kUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBADOLBL6tDdWcZtuPOXAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAG8XiklXE7Q3XgUgDgN6IAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgAxiMc2KyN1q4zbwRqXAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiEE8lhi3g2dyKQAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCALGSyjbWTuH+XAoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAG8TjmnFc/AbgJlwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIhBPI4xxpbvs3N4b9fPBE/jUgAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCADGIxzarI3UrQ3o7x/fOMtbHnbkUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBADOJxe08bkNs51ve03x2f51IAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQBiJRU2W10uXVlXXfmMZdX/m0sBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgDEIB58EytDdUb0OMulAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAYhAPHsxQHWe5FACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBAXu9+4Zzzk+8A4AZcCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoA5CfiH0Y9ZXp6KwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: 2\n",
      "Confidence of desired_digit = 4.8349477310694056e-08\n"
     ]
    }
   ],
   "source": [
    "copy_model = copy.deepcopy(rnnModel)\n",
    "\n",
    "# Do Damage\n",
    "damage_smallest(copy_model, 0.1)\n",
    "damage_fas(copy_model, 0, 0.3, 0)\n",
    "\n",
    "# Predict\n",
    "\n",
    "desired_digit = 3 # <----- Change number and see it print it :)\n",
    "test_img = generate_text(copy_model, desired_digit)\n",
    "predicted_class, confidence = evaluate_img(cnnEvaluator, test_img, display=True)\n",
    "print(f'Predicted Class: {predicted_class}')\n",
    "print(f'Confidence of desired_digit = {confidence[desired_digit]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6236228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
