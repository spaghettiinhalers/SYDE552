{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence, save_path=None, show=True):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.plot(xs, ys, linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9133b77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "997"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 13, 14, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 44, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 61, 65, 66, 68, 69, 70, 71, 74, 76, 79, 80, 81, 85, 86, 88, 89, 90, 91, 94, 98, 99, 100, 101, 102, 110, 111, 112, 113, 116, 120, 123, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 142, 143, 145, 146, 148, 154, 156, 157, 161, 162, 165, 168, 169, 170, 171, 172, 173, 175, 176, 178, 180, 182, 183, 186, 187, 188, 189, 191, 192, 196, 197, 201, 202, 203, 204, 205, 213, 215, 217, 219, 223, 224, 225, 228, 230, 231, 234, 239, 240, 246, 250, 251, 252, 254, 255, 259, 260, 261, 263, 264, 265, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, \n",
    "           281, 283, 284, 286, 289, 291, 292, 294, 295, 296, 298, 302, 306, 309, 311, 312, 313, 316, 319, 321, 323, 329, 330, 331, 332, 333, 334, 335, 336, 339, 342, 344, 345, 346, 347, 348, 351, 352, 353, 354, 356, 357, 361, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 383, 384, 388, 389, 392, 393, 397, 398, 401, 406, 407, 411, 413, 414, 416, 419, 422, 424, 426, 427, 430, 434, 440, 442, 444, 446, 455, 456, 459, 461, 463, 466, 469, 471, 472, 473, 478, 480, 481, 485, 487, 489, 491, 494, 496, 498, 501, 502, 505, 506, 507, 508, 509, 512, 517, 518, 521, 525, 528, 530, 531, 532, 534, 537, 540, 541, 545, 548,\n",
    "           548, 556, 558, 564, 568, 574, 575, 576, 577, 585, 586, 587, 588, 589, 594, 598, 599, 600, 604, 606, 607, 608, 609, 615, 616, 618, 619, 624, 626, 634, 645, 649, 651, 652, 656, 663, 665, 666, 667, 674, 677, 682, 683, 688, 705, 706, 710, 713, 715, 722, 724, 725, 727, 728, 729, 734, 739, 741, 745, 747, 749, 750, 751, 752, 754, 755, 757, 759, 760, 766, 767, 768, 769, 771, 772, 773, 774, 775, 778, 783, 788, 789, 790, 807, 809, 814, 820, 824, 826, 828, 833, 835, 837, 840, 843, 847, 849, 851, 853, 854, 857, 859, 860, 867, 869, 870, 871, 880, 884, 886, 887, 889, 890, 891, 897, 899, 906, 907, 911, 917, 920, 927, 931, 940, 941, 944, 945, 946, 948, 954, 955, 963, 970, 972, 973, 974, 976, 978, 979, 982, 984, 985, 986, 997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "08ded58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [[] for _ in range(10)]\n",
    "\n",
    "for i in indexes:\n",
    "    try:\n",
    "        data = np.loadtxt(f'../sequences/testimg-{i}-targetdata.txt', delimiter=' ')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found at path: {i}\")\n",
    "        continue\n",
    "    \n",
    "    inputOneshot = data[0, 0:10]\n",
    "    outputStrokes = data[:, 10:]\n",
    "    outputStrokes[:, 0] = outputStrokes[:, 0]/28\n",
    "    outputStrokes[:, 1] = outputStrokes[:, 1]/28\n",
    "    \n",
    "    datas[getNumFromOneHot(inputOneshot)].append(outputStrokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a163eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount = min([len(x) for x in datas])\n",
    "amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c84ec919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_stroke_sequence(datas[8][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4dd22b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[i] = 1\n",
    "    \n",
    "    smallest_10 = sorted(datas[i], key=len)[:amount]\n",
    "    for k in smallest_10:\n",
    "        input_data.append(temp_onehot)\n",
    "        output_data.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7e96a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = []\n",
    "# output_data = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     temp_onehot = np.zeros(10)\n",
    "#     temp_onehot[i] = 1\n",
    "    \n",
    "#     for k in range(amount):\n",
    "#         input_data.append(temp_onehot)\n",
    "#         output_data.append(datas[i][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04766e4",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6cd2c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Finding the max length of a sequence\n",
    "max_length = 0\n",
    "j = 0\n",
    "for i in range(len(output_data)):\n",
    "    if len(output_data[i]) > max_length:\n",
    "        max_length = len(output_data[i])\n",
    "    j += 1\n",
    "\n",
    "print(max_length)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "edeec1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "642bf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences so that they are all the same size (good for batching)\n",
    "padded_output_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_output_data[i, :len(output_data[i]), :] = output_data[i]\n",
    "    padded_output_data[i, len(output_data[i]):, :] = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "37cfe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_input_data[i, 0, :] = [0, 0, 0, 0]\n",
    "    padded_input_data[i, 1:, :] = padded_output_data[i, :max_length-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8523173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, onehot, inputs, outputstroke):\n",
    "        self.digit = onehot                     # shape: [N]\n",
    "        self.inputstroke = inputs               # list of [seq_len, 4] arrays\n",
    "        self.outputstroke = outputstroke        # list of [seq_len, 4] arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digit)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digit[idx]\n",
    "        inputs = self.inputstroke[idx]\n",
    "        outputs = self.outputstroke[idx]\n",
    "        return torch.tensor(digit, dtype=torch.float32), torch.tensor(inputs, dtype=torch.float32), torch.tensor(outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1171d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokeDataset = StrokeDataset(input_data, padded_input_data, padded_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d67af144",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(strokeDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8f994",
   "metadata": {},
   "source": [
    "Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2082a2d",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "\n",
    "RNN:\n",
    "input_size = output_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4+10,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAMAGE WEIGHTS\n",
    "\n",
    "def damage_smallest(model, p_smallest): # energy constraint\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_smallest == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            weight_magnitudes = tensor.abs().view(-1)\n",
    "            k = int(weight_magnitudes.numel() * p_smallest)\n",
    "\n",
    "            if k == 0:\n",
    "                continue\n",
    "            threshold = weight_magnitudes.kthvalue(k).values.item()\n",
    "\n",
    "            mask = tensor.abs() >= threshold\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "def damage_fas(model,  p_block, p_reflect, p_filter):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_block + p_reflect + p_filter == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            flat_weights = tensor.view(-1)\n",
    "            nonzero_indices = (flat_weights!=0).nonzero(as_tuple=True)[0]\n",
    "            num_nonzero_indices = nonzero_indices.numel()\n",
    "            if num_nonzero_indices == 0:\n",
    "                continue\n",
    "\n",
    "            # percentage of weights damaged will be taken from the number of nonzero weights\n",
    "            # simulated fas damage occurs after energy constraint blockage\n",
    "            num_block = int(num_nonzero_indices * p_block)\n",
    "            num_reflect = int(num_nonzero_indices * p_reflect)\n",
    "            num_filter = int(num_nonzero_indices * p_filter)\n",
    "\n",
    "            shuffled_indices = nonzero_indices[torch.randperm(num_nonzero_indices, device=flat_weights.device)]\n",
    "\n",
    "            indices_block = shuffled_indices[:num_block]\n",
    "            indices_reflect = shuffled_indices[num_block:num_block+num_reflect]\n",
    "            indices_filter = shuffled_indices[num_block+num_reflect:num_block+num_reflect+num_filter]\n",
    "\n",
    "            # do damage\n",
    "            # blockage: set weights to 0\n",
    "            if p_block != 0:\n",
    "                flat_weights[indices_block] = 0\n",
    "\n",
    "            # reflect: halve weights\n",
    "            if p_reflect != 0:\n",
    "                flat_weights[indices_reflect] *= 0.5\n",
    "\n",
    "            # filter: low pass filter (lusch et al)\n",
    "            if p_filter != 0:\n",
    "                weights_to_filter = flat_weights[indices_filter]            # get weights before transformation\n",
    "                signs = torch.sign(weights_to_filter)                       # get signs of weights\n",
    "                abs_weights_to_filter = weights_to_filter.abs()             # get high_weight, should be in the 95th percentile for all weights\n",
    "                high_weight = torch.quantile(flat_weights.abs(), 0.95)      # scale weights to mostly between -1 and 1\n",
    "                x = abs_weights_to_filter / high_weight\n",
    "                transformed_weights = -0.2744 * x**2 + 0.9094 * x - 0.0192\n",
    "                gaussian_noise = torch.randn_like(transformed_weights) * 0.05\n",
    "                transformed_weights += gaussian_noise\n",
    "                transformed_weights = transformed_weights * signs * high_weight # rescale\n",
    "                flat_weights[indices_filter] = transformed_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92069110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "\n",
    "# # damage weights to simulate energy constraint and lesioning\n",
    "# damage_smallest(model, 0.0)\n",
    "# damage_fas(model, 0.0, 0.0, 0.0)\n",
    "\n",
    "dx_dy_loss_fn = nn.MSELoss()\n",
    "eos_eod_loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "e7354307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.0012\n",
      "Epoch 2 | Loss: 0.0011\n",
      "Epoch 3 | Loss: 0.0011\n",
      "Epoch 4 | Loss: 0.0010\n",
      "Epoch 5 | Loss: 0.0010\n",
      "Epoch 6 | Loss: 0.0010\n",
      "Epoch 7 | Loss: 0.0010\n",
      "Epoch 8 | Loss: 0.0010\n",
      "Epoch 9 | Loss: 0.0009\n",
      "Epoch 10 | Loss: 0.0010\n",
      "Epoch 11 | Loss: 0.0009\n",
      "Epoch 12 | Loss: 0.0010\n",
      "Epoch 13 | Loss: 0.0010\n",
      "Epoch 14 | Loss: 0.0010\n",
      "Epoch 15 | Loss: 0.0009\n",
      "Epoch 16 | Loss: 0.0010\n",
      "Epoch 17 | Loss: 0.0009\n",
      "Epoch 18 | Loss: 0.0009\n",
      "Epoch 19 | Loss: 0.0009\n",
      "Epoch 20 | Loss: 0.0010\n",
      "Epoch 21 | Loss: 0.0010\n",
      "Epoch 22 | Loss: 0.0010\n",
      "Epoch 23 | Loss: 0.0010\n",
      "Epoch 24 | Loss: 0.0009\n",
      "Epoch 25 | Loss: 0.0009\n",
      "Epoch 26 | Loss: 0.0010\n",
      "Epoch 27 | Loss: 0.0009\n",
      "Epoch 28 | Loss: 0.0009\n",
      "Epoch 29 | Loss: 0.0009\n",
      "Epoch 30 | Loss: 0.0009\n",
      "Epoch 31 | Loss: 0.0010\n",
      "Epoch 32 | Loss: 0.0009\n",
      "Epoch 33 | Loss: 0.0010\n",
      "Epoch 34 | Loss: 0.0010\n",
      "Epoch 35 | Loss: 0.0010\n",
      "Epoch 36 | Loss: 0.0009\n",
      "Epoch 37 | Loss: 0.0009\n",
      "Epoch 38 | Loss: 0.0009\n",
      "Epoch 39 | Loss: 0.0010\n",
      "Epoch 40 | Loss: 0.0009\n",
      "Epoch 41 | Loss: 0.0009\n",
      "Epoch 42 | Loss: 0.0010\n",
      "Epoch 43 | Loss: 0.0010\n",
      "Epoch 44 | Loss: 0.0009\n",
      "Epoch 45 | Loss: 0.0010\n",
      "Epoch 46 | Loss: 0.0010\n",
      "Epoch 47 | Loss: 0.0011\n",
      "Epoch 48 | Loss: 0.0012\n",
      "Epoch 49 | Loss: 0.0011\n",
      "Epoch 50 | Loss: 0.0010\n",
      "Epoch 51 | Loss: 0.0010\n",
      "Epoch 52 | Loss: 0.0010\n",
      "Epoch 53 | Loss: 0.0009\n",
      "Epoch 54 | Loss: 0.0010\n",
      "Epoch 55 | Loss: 0.0010\n",
      "Epoch 56 | Loss: 0.0009\n",
      "Epoch 57 | Loss: 0.0009\n",
      "Epoch 58 | Loss: 0.0009\n",
      "Epoch 59 | Loss: 0.0010\n",
      "Epoch 60 | Loss: 0.0010\n",
      "Epoch 61 | Loss: 0.0010\n",
      "Epoch 62 | Loss: 0.0010\n",
      "Epoch 63 | Loss: 0.0010\n",
      "Epoch 64 | Loss: 0.0010\n",
      "Epoch 65 | Loss: 0.0010\n",
      "Epoch 66 | Loss: 0.0010\n",
      "Epoch 67 | Loss: 0.0010\n",
      "Epoch 68 | Loss: 0.0010\n",
      "Epoch 69 | Loss: 0.0010\n",
      "Epoch 70 | Loss: 0.0010\n",
      "Epoch 71 | Loss: 0.0009\n",
      "Epoch 72 | Loss: 0.0009\n",
      "Epoch 73 | Loss: 0.0010\n",
      "Epoch 74 | Loss: 0.0009\n",
      "Epoch 75 | Loss: 0.0010\n",
      "Epoch 76 | Loss: 0.0010\n",
      "Epoch 77 | Loss: 0.0010\n",
      "Epoch 78 | Loss: 0.0013\n",
      "Epoch 79 | Loss: 0.0013\n",
      "Epoch 80 | Loss: 0.0011\n",
      "Epoch 81 | Loss: 0.0011\n",
      "Epoch 82 | Loss: 0.0010\n",
      "Epoch 83 | Loss: 0.0010\n",
      "Epoch 84 | Loss: 0.0011\n",
      "Epoch 85 | Loss: 0.0010\n",
      "Epoch 86 | Loss: 0.0010\n",
      "Epoch 87 | Loss: 0.0010\n",
      "Epoch 88 | Loss: 0.0010\n",
      "Epoch 89 | Loss: 0.0010\n",
      "Epoch 90 | Loss: 0.0009\n",
      "Epoch 91 | Loss: 0.0009\n",
      "Epoch 92 | Loss: 0.0010\n",
      "Epoch 93 | Loss: 0.0009\n",
      "Epoch 94 | Loss: 0.0009\n",
      "Epoch 95 | Loss: 0.0010\n",
      "Epoch 96 | Loss: 0.0009\n",
      "Epoch 97 | Loss: 0.0010\n",
      "Epoch 98 | Loss: 0.0010\n",
      "Epoch 99 | Loss: 0.0010\n",
      "Epoch 100 | Loss: 0.0010\n",
      "Epoch 101 | Loss: 0.0010\n",
      "Epoch 102 | Loss: 0.0012\n",
      "Epoch 103 | Loss: 0.0010\n",
      "Epoch 104 | Loss: 0.0010\n",
      "Epoch 105 | Loss: 0.0009\n",
      "Epoch 106 | Loss: 0.0010\n",
      "Epoch 107 | Loss: 0.0009\n",
      "Epoch 108 | Loss: 0.0009\n",
      "Epoch 109 | Loss: 0.0010\n",
      "Epoch 110 | Loss: 0.0009\n",
      "Epoch 111 | Loss: 0.0010\n",
      "Epoch 112 | Loss: 0.0010\n",
      "Epoch 113 | Loss: 0.0010\n",
      "Epoch 114 | Loss: 0.0009\n",
      "Epoch 115 | Loss: 0.0009\n",
      "Epoch 116 | Loss: 0.0009\n",
      "Epoch 117 | Loss: 0.0009\n",
      "Epoch 118 | Loss: 0.0009\n",
      "Epoch 119 | Loss: 0.0009\n",
      "Epoch 120 | Loss: 0.0009\n",
      "Epoch 121 | Loss: 0.0009\n",
      "Epoch 122 | Loss: 0.0009\n",
      "Epoch 123 | Loss: 0.0009\n",
      "Epoch 124 | Loss: 0.0009\n",
      "Epoch 125 | Loss: 0.0010\n",
      "Epoch 126 | Loss: 0.0010\n",
      "Epoch 127 | Loss: 0.0010\n",
      "Epoch 128 | Loss: 0.0010\n",
      "Epoch 129 | Loss: 0.0010\n",
      "Epoch 130 | Loss: 0.0010\n",
      "Epoch 131 | Loss: 0.0010\n",
      "Epoch 132 | Loss: 0.0010\n",
      "Epoch 133 | Loss: 0.0011\n",
      "Epoch 134 | Loss: 0.0010\n",
      "Epoch 135 | Loss: 0.0010\n",
      "Epoch 136 | Loss: 0.0010\n",
      "Epoch 137 | Loss: 0.0010\n",
      "Epoch 138 | Loss: 0.0009\n",
      "Epoch 139 | Loss: 0.0011\n",
      "Epoch 140 | Loss: 0.0011\n",
      "Epoch 141 | Loss: 0.0010\n",
      "Epoch 142 | Loss: 0.0009\n",
      "Epoch 143 | Loss: 0.0010\n",
      "Epoch 144 | Loss: 0.0009\n",
      "Epoch 145 | Loss: 0.0009\n",
      "Epoch 146 | Loss: 0.0009\n",
      "Epoch 147 | Loss: 0.0009\n",
      "Epoch 148 | Loss: 0.0009\n",
      "Epoch 149 | Loss: 0.0009\n",
      "Epoch 150 | Loss: 0.0009\n",
      "Epoch 151 | Loss: 0.0009\n",
      "Epoch 152 | Loss: 0.0009\n",
      "Epoch 153 | Loss: 0.0009\n",
      "Epoch 154 | Loss: 0.0010\n",
      "Epoch 155 | Loss: 0.0012\n",
      "Epoch 156 | Loss: 0.0010\n",
      "Epoch 157 | Loss: 0.0010\n",
      "Epoch 158 | Loss: 0.0010\n",
      "Epoch 159 | Loss: 0.0011\n",
      "Epoch 160 | Loss: 0.0010\n",
      "Epoch 161 | Loss: 0.0009\n",
      "Epoch 162 | Loss: 0.0010\n",
      "Epoch 163 | Loss: 0.0010\n",
      "Epoch 164 | Loss: 0.0009\n",
      "Epoch 165 | Loss: 0.0010\n",
      "Epoch 166 | Loss: 0.0010\n",
      "Epoch 167 | Loss: 0.0009\n",
      "Epoch 168 | Loss: 0.0010\n",
      "Epoch 169 | Loss: 0.0010\n",
      "Epoch 170 | Loss: 0.0009\n",
      "Epoch 171 | Loss: 0.0010\n",
      "Epoch 172 | Loss: 0.0010\n",
      "Epoch 173 | Loss: 0.0010\n",
      "Epoch 174 | Loss: 0.0012\n",
      "Epoch 175 | Loss: 0.0010\n",
      "Epoch 176 | Loss: 0.0010\n",
      "Epoch 177 | Loss: 0.0010\n",
      "Epoch 178 | Loss: 0.0010\n",
      "Epoch 179 | Loss: 0.0009\n",
      "Epoch 180 | Loss: 0.0010\n",
      "Epoch 181 | Loss: 0.0009\n",
      "Epoch 182 | Loss: 0.0009\n",
      "Epoch 183 | Loss: 0.0009\n",
      "Epoch 184 | Loss: 0.0009\n",
      "Epoch 185 | Loss: 0.0009\n",
      "Epoch 186 | Loss: 0.0009\n",
      "Epoch 187 | Loss: 0.0009\n",
      "Epoch 188 | Loss: 0.0009\n",
      "Epoch 189 | Loss: 0.0010\n",
      "Epoch 190 | Loss: 0.0009\n",
      "Epoch 191 | Loss: 0.0010\n",
      "Epoch 192 | Loss: 0.0009\n",
      "Epoch 193 | Loss: 0.0009\n",
      "Epoch 194 | Loss: 0.0009\n",
      "Epoch 195 | Loss: 0.0009\n",
      "Epoch 196 | Loss: 0.0009\n",
      "Epoch 197 | Loss: 0.0009\n",
      "Epoch 198 | Loss: 0.0009\n",
      "Epoch 199 | Loss: 0.0009\n",
      "Epoch 200 | Loss: 0.0009\n",
      "Epoch 201 | Loss: 0.0009\n",
      "Epoch 202 | Loss: 0.0009\n",
      "Epoch 203 | Loss: 0.0009\n",
      "Epoch 204 | Loss: 0.0009\n",
      "Epoch 205 | Loss: 0.0009\n",
      "Epoch 206 | Loss: 0.0011\n",
      "Epoch 207 | Loss: 0.0010\n",
      "Epoch 208 | Loss: 0.0009\n",
      "Epoch 209 | Loss: 0.0009\n",
      "Epoch 210 | Loss: 0.0009\n",
      "Epoch 211 | Loss: 0.0009\n",
      "Epoch 212 | Loss: 0.0009\n",
      "Epoch 213 | Loss: 0.0009\n",
      "Epoch 214 | Loss: 0.0009\n",
      "Epoch 215 | Loss: 0.0012\n",
      "Epoch 216 | Loss: 0.0011\n",
      "Epoch 217 | Loss: 0.0011\n",
      "Epoch 218 | Loss: 0.0010\n",
      "Epoch 219 | Loss: 0.0010\n",
      "Epoch 220 | Loss: 0.0010\n",
      "Epoch 221 | Loss: 0.0010\n",
      "Epoch 222 | Loss: 0.0010\n",
      "Epoch 223 | Loss: 0.0010\n",
      "Epoch 224 | Loss: 0.0010\n",
      "Epoch 225 | Loss: 0.0010\n",
      "Epoch 226 | Loss: 0.0013\n",
      "Epoch 227 | Loss: 0.0011\n",
      "Epoch 228 | Loss: 0.0010\n",
      "Epoch 229 | Loss: 0.0010\n",
      "Epoch 230 | Loss: 0.0009\n",
      "Epoch 231 | Loss: 0.0009\n",
      "Epoch 232 | Loss: 0.0009\n",
      "Epoch 233 | Loss: 0.0009\n",
      "Epoch 234 | Loss: 0.0009\n",
      "Epoch 235 | Loss: 0.0009\n",
      "Epoch 236 | Loss: 0.0009\n",
      "Epoch 237 | Loss: 0.0009\n",
      "Epoch 238 | Loss: 0.0009\n",
      "Epoch 239 | Loss: 0.0009\n",
      "Epoch 240 | Loss: 0.0009\n",
      "Epoch 241 | Loss: 0.0009\n",
      "Epoch 242 | Loss: 0.0009\n",
      "Epoch 243 | Loss: 0.0009\n",
      "Epoch 244 | Loss: 0.0009\n",
      "Epoch 245 | Loss: 0.0009\n",
      "Epoch 246 | Loss: 0.0009\n",
      "Epoch 247 | Loss: 0.0009\n",
      "Epoch 248 | Loss: 0.0009\n",
      "Epoch 249 | Loss: 0.0009\n",
      "Epoch 250 | Loss: 0.0009\n",
      "Epoch 251 | Loss: 0.0009\n",
      "Epoch 252 | Loss: 0.0009\n",
      "Epoch 253 | Loss: 0.0009\n",
      "Epoch 254 | Loss: 0.0009\n",
      "Epoch 255 | Loss: 0.0009\n",
      "Epoch 256 | Loss: 0.0009\n",
      "Epoch 257 | Loss: 0.0010\n",
      "Epoch 258 | Loss: 0.0010\n",
      "Epoch 259 | Loss: 0.0010\n",
      "Epoch 260 | Loss: 0.0009\n",
      "Epoch 261 | Loss: 0.0009\n",
      "Epoch 262 | Loss: 0.0010\n",
      "Epoch 263 | Loss: 0.0009\n",
      "Epoch 264 | Loss: 0.0009\n",
      "Epoch 265 | Loss: 0.0009\n",
      "Epoch 266 | Loss: 0.0009\n",
      "Epoch 267 | Loss: 0.0009\n",
      "Epoch 268 | Loss: 0.0009\n",
      "Epoch 269 | Loss: 0.0009\n",
      "Epoch 270 | Loss: 0.0010\n",
      "Epoch 271 | Loss: 0.0014\n",
      "Epoch 272 | Loss: 0.0011\n",
      "Epoch 273 | Loss: 0.0011\n",
      "Epoch 274 | Loss: 0.0010\n",
      "Epoch 275 | Loss: 0.0010\n",
      "Epoch 276 | Loss: 0.0010\n",
      "Epoch 277 | Loss: 0.0010\n",
      "Epoch 278 | Loss: 0.0010\n",
      "Epoch 279 | Loss: 0.0010\n",
      "Epoch 280 | Loss: 0.0009\n",
      "Epoch 281 | Loss: 0.0010\n",
      "Epoch 282 | Loss: 0.0010\n",
      "Epoch 283 | Loss: 0.0011\n",
      "Epoch 284 | Loss: 0.0009\n",
      "Epoch 285 | Loss: 0.0010\n",
      "Epoch 286 | Loss: 0.0010\n",
      "Epoch 287 | Loss: 0.0009\n",
      "Epoch 288 | Loss: 0.0009\n",
      "Epoch 289 | Loss: 0.0009\n",
      "Epoch 290 | Loss: 0.0009\n",
      "Epoch 291 | Loss: 0.0009\n",
      "Epoch 292 | Loss: 0.0009\n",
      "Epoch 293 | Loss: 0.0009\n",
      "Epoch 294 | Loss: 0.0009\n",
      "Epoch 295 | Loss: 0.0009\n",
      "Epoch 296 | Loss: 0.0009\n",
      "Epoch 297 | Loss: 0.0010\n",
      "Epoch 298 | Loss: 0.0010\n",
      "Epoch 299 | Loss: 0.0010\n",
      "Epoch 300 | Loss: 0.0009\n",
      "Epoch 301 | Loss: 0.0009\n",
      "Epoch 302 | Loss: 0.0009\n",
      "Epoch 303 | Loss: 0.0009\n",
      "Epoch 304 | Loss: 0.0009\n",
      "Epoch 305 | Loss: 0.0009\n",
      "Epoch 306 | Loss: 0.0009\n",
      "Epoch 307 | Loss: 0.0009\n",
      "Epoch 308 | Loss: 0.0009\n",
      "Epoch 309 | Loss: 0.0009\n",
      "Epoch 310 | Loss: 0.0009\n",
      "Epoch 311 | Loss: 0.0009\n",
      "Epoch 312 | Loss: 0.0009\n",
      "Epoch 313 | Loss: 0.0009\n",
      "Epoch 314 | Loss: 0.0009\n",
      "Epoch 315 | Loss: 0.0009\n",
      "Epoch 316 | Loss: 0.0009\n",
      "Epoch 317 | Loss: 0.0009\n",
      "Epoch 318 | Loss: 0.0009\n",
      "Epoch 319 | Loss: 0.0009\n",
      "Epoch 320 | Loss: 0.0009\n",
      "Epoch 321 | Loss: 0.0009\n",
      "Epoch 322 | Loss: 0.0009\n",
      "Epoch 323 | Loss: 0.0009\n",
      "Epoch 324 | Loss: 0.0009\n",
      "Epoch 325 | Loss: 0.0009\n",
      "Epoch 326 | Loss: 0.0009\n",
      "Epoch 327 | Loss: 0.0009\n",
      "Epoch 328 | Loss: 0.0009\n",
      "Epoch 329 | Loss: 0.0009\n",
      "Epoch 330 | Loss: 0.0009\n",
      "Epoch 331 | Loss: 0.0009\n",
      "Epoch 332 | Loss: 0.0009\n",
      "Epoch 333 | Loss: 0.0010\n",
      "Epoch 334 | Loss: 0.0010\n",
      "Epoch 335 | Loss: 0.0009\n",
      "Epoch 336 | Loss: 0.0010\n",
      "Epoch 337 | Loss: 0.0010\n",
      "Epoch 338 | Loss: 0.0012\n",
      "Epoch 339 | Loss: 0.0010\n",
      "Epoch 340 | Loss: 0.0014\n",
      "Epoch 341 | Loss: 0.0011\n",
      "Epoch 342 | Loss: 0.0010\n",
      "Epoch 343 | Loss: 0.0010\n",
      "Epoch 344 | Loss: 0.0009\n",
      "Epoch 345 | Loss: 0.0009\n",
      "Epoch 346 | Loss: 0.0009\n",
      "Epoch 347 | Loss: 0.0009\n",
      "Epoch 348 | Loss: 0.0009\n",
      "Epoch 349 | Loss: 0.0010\n",
      "Epoch 350 | Loss: 0.0010\n",
      "Epoch 351 | Loss: 0.0009\n",
      "Epoch 352 | Loss: 0.0009\n",
      "Epoch 353 | Loss: 0.0009\n",
      "Epoch 354 | Loss: 0.0009\n",
      "Epoch 355 | Loss: 0.0009\n",
      "Epoch 356 | Loss: 0.0009\n",
      "Epoch 357 | Loss: 0.0009\n",
      "Epoch 358 | Loss: 0.0009\n",
      "Epoch 359 | Loss: 0.0010\n",
      "Epoch 360 | Loss: 0.0010\n",
      "Epoch 361 | Loss: 0.0009\n",
      "Epoch 362 | Loss: 0.0010\n",
      "Epoch 363 | Loss: 0.0009\n",
      "Epoch 364 | Loss: 0.0009\n",
      "Epoch 365 | Loss: 0.0009\n",
      "Epoch 366 | Loss: 0.0009\n",
      "Epoch 367 | Loss: 0.0009\n",
      "Epoch 368 | Loss: 0.0009\n",
      "Epoch 369 | Loss: 0.0009\n",
      "Epoch 370 | Loss: 0.0009\n",
      "Epoch 371 | Loss: 0.0009\n",
      "Epoch 372 | Loss: 0.0009\n",
      "Epoch 373 | Loss: 0.0009\n",
      "Epoch 374 | Loss: 0.0009\n",
      "Epoch 375 | Loss: 0.0009\n",
      "Epoch 376 | Loss: 0.0009\n",
      "Epoch 377 | Loss: 0.0009\n",
      "Epoch 378 | Loss: 0.0009\n",
      "Epoch 379 | Loss: 0.0009\n",
      "Epoch 380 | Loss: 0.0009\n",
      "Epoch 381 | Loss: 0.0009\n",
      "Epoch 382 | Loss: 0.0009\n",
      "Epoch 383 | Loss: 0.0009\n",
      "Epoch 384 | Loss: 0.0009\n",
      "Epoch 385 | Loss: 0.0009\n",
      "Epoch 386 | Loss: 0.0009\n",
      "Epoch 387 | Loss: 0.0009\n",
      "Epoch 388 | Loss: 0.0010\n",
      "Epoch 389 | Loss: 0.0009\n",
      "Epoch 390 | Loss: 0.0009\n",
      "Epoch 391 | Loss: 0.0010\n",
      "Epoch 392 | Loss: 0.0009\n",
      "Epoch 393 | Loss: 0.0009\n",
      "Epoch 394 | Loss: 0.0009\n",
      "Epoch 395 | Loss: 0.0009\n",
      "Epoch 396 | Loss: 0.0009\n",
      "Epoch 397 | Loss: 0.0009\n",
      "Epoch 398 | Loss: 0.0009\n",
      "Epoch 399 | Loss: 0.0009\n",
      "Epoch 400 | Loss: 0.0009\n",
      "Epoch 401 | Loss: 0.0009\n",
      "Epoch 402 | Loss: 0.0009\n",
      "Epoch 403 | Loss: 0.0009\n",
      "Epoch 404 | Loss: 0.0009\n",
      "Epoch 405 | Loss: 0.0011\n",
      "Epoch 406 | Loss: 0.0010\n",
      "Epoch 407 | Loss: 0.0010\n",
      "Epoch 408 | Loss: 0.0009\n",
      "Epoch 409 | Loss: 0.0012\n",
      "Epoch 410 | Loss: 0.0010\n",
      "Epoch 411 | Loss: 0.0009\n",
      "Epoch 412 | Loss: 0.0010\n",
      "Epoch 413 | Loss: 0.0010\n",
      "Epoch 414 | Loss: 0.0009\n",
      "Epoch 415 | Loss: 0.0009\n",
      "Epoch 416 | Loss: 0.0010\n",
      "Epoch 417 | Loss: 0.0010\n",
      "Epoch 418 | Loss: 0.0009\n",
      "Epoch 419 | Loss: 0.0009\n",
      "Epoch 420 | Loss: 0.0009\n",
      "Epoch 421 | Loss: 0.0009\n",
      "Epoch 422 | Loss: 0.0009\n",
      "Epoch 423 | Loss: 0.0009\n",
      "Epoch 424 | Loss: 0.0009\n",
      "Epoch 425 | Loss: 0.0009\n",
      "Epoch 426 | Loss: 0.0009\n",
      "Epoch 427 | Loss: 0.0009\n",
      "Epoch 428 | Loss: 0.0009\n",
      "Epoch 429 | Loss: 0.0009\n",
      "Epoch 430 | Loss: 0.0009\n",
      "Epoch 431 | Loss: 0.0009\n",
      "Epoch 432 | Loss: 0.0008\n",
      "Epoch 433 | Loss: 0.0009\n",
      "Epoch 434 | Loss: 0.0010\n",
      "Epoch 435 | Loss: 0.0009\n",
      "Epoch 436 | Loss: 0.0010\n",
      "Epoch 437 | Loss: 0.0009\n",
      "Epoch 438 | Loss: 0.0010\n",
      "Epoch 439 | Loss: 0.0009\n",
      "Epoch 440 | Loss: 0.0009\n",
      "Epoch 441 | Loss: 0.0009\n",
      "Epoch 442 | Loss: 0.0009\n",
      "Epoch 443 | Loss: 0.0009\n",
      "Epoch 444 | Loss: 0.0009\n",
      "Epoch 445 | Loss: 0.0009\n",
      "Epoch 446 | Loss: 0.0009\n",
      "Epoch 447 | Loss: 0.0009\n",
      "Epoch 448 | Loss: 0.0008\n",
      "Epoch 449 | Loss: 0.0009\n",
      "Epoch 450 | Loss: 0.0009\n",
      "Epoch 451 | Loss: 0.0008\n",
      "Epoch 452 | Loss: 0.0009\n",
      "Epoch 453 | Loss: 0.0008\n",
      "Epoch 454 | Loss: 0.0009\n",
      "Epoch 455 | Loss: 0.0008\n",
      "Epoch 456 | Loss: 0.0009\n",
      "Epoch 457 | Loss: 0.0009\n",
      "Epoch 458 | Loss: 0.0009\n",
      "Epoch 459 | Loss: 0.0009\n",
      "Epoch 460 | Loss: 0.0009\n",
      "Epoch 461 | Loss: 0.0009\n",
      "Epoch 462 | Loss: 0.0010\n",
      "Epoch 463 | Loss: 0.0010\n",
      "Epoch 464 | Loss: 0.0009\n",
      "Epoch 465 | Loss: 0.0011\n",
      "Epoch 466 | Loss: 0.0009\n",
      "Epoch 467 | Loss: 0.0011\n",
      "Epoch 468 | Loss: 0.0010\n",
      "Epoch 469 | Loss: 0.0009\n",
      "Epoch 470 | Loss: 0.0009\n",
      "Epoch 471 | Loss: 0.0009\n",
      "Epoch 472 | Loss: 0.0009\n",
      "Epoch 473 | Loss: 0.0010\n",
      "Epoch 474 | Loss: 0.0009\n",
      "Epoch 475 | Loss: 0.0009\n",
      "Epoch 476 | Loss: 0.0009\n",
      "Epoch 477 | Loss: 0.0009\n",
      "Epoch 478 | Loss: 0.0009\n",
      "Epoch 479 | Loss: 0.0009\n",
      "Epoch 480 | Loss: 0.0009\n",
      "Epoch 481 | Loss: 0.0009\n",
      "Epoch 482 | Loss: 0.0009\n",
      "Epoch 483 | Loss: 0.0009\n",
      "Epoch 484 | Loss: 0.0009\n",
      "Epoch 485 | Loss: 0.0009\n",
      "Epoch 486 | Loss: 0.0009\n",
      "Epoch 487 | Loss: 0.0008\n",
      "Epoch 488 | Loss: 0.0009\n",
      "Epoch 489 | Loss: 0.0009\n",
      "Epoch 490 | Loss: 0.0008\n",
      "Epoch 491 | Loss: 0.0009\n",
      "Epoch 492 | Loss: 0.0009\n",
      "Epoch 493 | Loss: 0.0009\n",
      "Epoch 494 | Loss: 0.0010\n",
      "Epoch 495 | Loss: 0.0009\n",
      "Epoch 496 | Loss: 0.0009\n",
      "Epoch 497 | Loss: 0.0009\n",
      "Epoch 498 | Loss: 0.0009\n",
      "Epoch 499 | Loss: 0.0009\n",
      "Epoch 500 | Loss: 0.0009\n",
      "Epoch 501 | Loss: 0.0009\n",
      "Epoch 502 | Loss: 0.0011\n",
      "Epoch 503 | Loss: 0.0009\n",
      "Epoch 504 | Loss: 0.0010\n",
      "Epoch 505 | Loss: 0.0010\n",
      "Epoch 506 | Loss: 0.0011\n",
      "Epoch 507 | Loss: 0.0009\n",
      "Epoch 508 | Loss: 0.0009\n",
      "Epoch 509 | Loss: 0.0009\n",
      "Epoch 510 | Loss: 0.0009\n",
      "Epoch 511 | Loss: 0.0009\n",
      "Epoch 512 | Loss: 0.0009\n",
      "Epoch 513 | Loss: 0.0008\n",
      "Epoch 514 | Loss: 0.0009\n",
      "Epoch 515 | Loss: 0.0009\n",
      "Epoch 516 | Loss: 0.0008\n",
      "Epoch 517 | Loss: 0.0009\n",
      "Epoch 518 | Loss: 0.0009\n",
      "Epoch 519 | Loss: 0.0009\n",
      "Epoch 520 | Loss: 0.0009\n",
      "Epoch 521 | Loss: 0.0009\n",
      "Epoch 522 | Loss: 0.0009\n",
      "Epoch 523 | Loss: 0.0009\n",
      "Epoch 524 | Loss: 0.0008\n",
      "Epoch 525 | Loss: 0.0008\n",
      "Epoch 526 | Loss: 0.0008\n",
      "Epoch 527 | Loss: 0.0008\n",
      "Epoch 528 | Loss: 0.0008\n",
      "Epoch 529 | Loss: 0.0008\n",
      "Epoch 530 | Loss: 0.0009\n",
      "Epoch 531 | Loss: 0.0008\n",
      "Epoch 532 | Loss: 0.0008\n",
      "Epoch 533 | Loss: 0.0009\n",
      "Epoch 534 | Loss: 0.0011\n",
      "Epoch 535 | Loss: 0.0011\n",
      "Epoch 536 | Loss: 0.0010\n",
      "Epoch 537 | Loss: 0.0010\n",
      "Epoch 538 | Loss: 0.0010\n",
      "Epoch 539 | Loss: 0.0010\n",
      "Epoch 540 | Loss: 0.0010\n",
      "Epoch 541 | Loss: 0.0011\n",
      "Epoch 542 | Loss: 0.5079\n",
      "Epoch 543 | Loss: 0.5100\n",
      "Epoch 544 | Loss: 0.2429\n",
      "Epoch 545 | Loss: 0.2168\n",
      "Epoch 546 | Loss: 0.1933\n",
      "Epoch 547 | Loss: 0.1809\n",
      "Epoch 548 | Loss: 0.1730\n",
      "Epoch 549 | Loss: 0.1669\n",
      "Epoch 550 | Loss: 0.1692\n",
      "Epoch 551 | Loss: 0.1759\n",
      "Epoch 552 | Loss: 0.1568\n",
      "Epoch 553 | Loss: 0.1541\n",
      "Epoch 554 | Loss: 0.1539\n",
      "Epoch 555 | Loss: 0.1646\n",
      "Epoch 556 | Loss: 0.1526\n",
      "Epoch 557 | Loss: 0.1458\n",
      "Epoch 558 | Loss: 0.1444\n",
      "Epoch 559 | Loss: 0.1396\n",
      "Epoch 560 | Loss: 0.1375\n",
      "Epoch 561 | Loss: 0.1361\n",
      "Epoch 562 | Loss: 0.1309\n",
      "Epoch 563 | Loss: 0.1278\n",
      "Epoch 564 | Loss: 0.1261\n",
      "Epoch 565 | Loss: 0.1251\n",
      "Epoch 566 | Loss: 0.1296\n",
      "Epoch 567 | Loss: 0.1463\n",
      "Epoch 568 | Loss: 0.1232\n",
      "Epoch 569 | Loss: 0.1255\n",
      "Epoch 570 | Loss: 0.1288\n",
      "Epoch 571 | Loss: 0.1291\n",
      "Epoch 572 | Loss: 0.1223\n",
      "Epoch 573 | Loss: 0.1210\n",
      "Epoch 574 | Loss: 0.1178\n",
      "Epoch 575 | Loss: 0.1197\n",
      "Epoch 576 | Loss: 0.1128\n",
      "Epoch 577 | Loss: 0.1120\n",
      "Epoch 578 | Loss: 0.1175\n",
      "Epoch 579 | Loss: 0.1295\n",
      "Epoch 580 | Loss: 0.1253\n",
      "Epoch 581 | Loss: 0.1113\n",
      "Epoch 582 | Loss: 0.1075\n",
      "Epoch 583 | Loss: 0.1042\n",
      "Epoch 584 | Loss: 0.1002\n",
      "Epoch 585 | Loss: 0.0975\n",
      "Epoch 586 | Loss: 0.0911\n",
      "Epoch 587 | Loss: 0.0919\n",
      "Epoch 588 | Loss: 0.0850\n",
      "Epoch 589 | Loss: 0.0922\n",
      "Epoch 590 | Loss: 0.0965\n",
      "Epoch 591 | Loss: 0.0990\n",
      "Epoch 592 | Loss: 0.0884\n",
      "Epoch 593 | Loss: 0.0898\n",
      "Epoch 594 | Loss: 0.0859\n",
      "Epoch 595 | Loss: 0.0807\n",
      "Epoch 596 | Loss: 0.0781\n",
      "Epoch 597 | Loss: 0.0742\n",
      "Epoch 598 | Loss: 0.0727\n",
      "Epoch 599 | Loss: 0.0703\n",
      "Epoch 600 | Loss: 0.0698\n",
      "Epoch 601 | Loss: 0.0753\n",
      "Epoch 602 | Loss: 0.0840\n",
      "Epoch 603 | Loss: 0.0925\n",
      "Epoch 604 | Loss: 0.0875\n",
      "Epoch 605 | Loss: 0.0777\n",
      "Epoch 606 | Loss: 0.0773\n",
      "Epoch 607 | Loss: 0.0670\n",
      "Epoch 608 | Loss: 0.0599\n",
      "Epoch 609 | Loss: 0.0546\n",
      "Epoch 610 | Loss: 0.0531\n",
      "Epoch 611 | Loss: 0.0492\n",
      "Epoch 612 | Loss: 0.0623\n",
      "Epoch 613 | Loss: 0.0564\n",
      "Epoch 614 | Loss: 0.0486\n",
      "Epoch 615 | Loss: 0.0526\n",
      "Epoch 616 | Loss: 0.0703\n",
      "Epoch 617 | Loss: 0.0875\n",
      "Epoch 618 | Loss: 0.0785\n",
      "Epoch 619 | Loss: 0.0664\n",
      "Epoch 620 | Loss: 0.0696\n",
      "Epoch 621 | Loss: 0.0549\n",
      "Epoch 622 | Loss: 0.0529\n",
      "Epoch 623 | Loss: 0.0455\n",
      "Epoch 624 | Loss: 0.0445\n",
      "Epoch 625 | Loss: 0.0379\n",
      "Epoch 626 | Loss: 0.0347\n",
      "Epoch 627 | Loss: 0.0309\n",
      "Epoch 628 | Loss: 0.0330\n",
      "Epoch 629 | Loss: 0.0314\n",
      "Epoch 630 | Loss: 0.0336\n",
      "Epoch 631 | Loss: 0.0318\n",
      "Epoch 632 | Loss: 0.0358\n",
      "Epoch 633 | Loss: 0.0396\n",
      "Epoch 634 | Loss: 0.0392\n",
      "Epoch 635 | Loss: 0.0428\n",
      "Epoch 636 | Loss: 0.0381\n",
      "Epoch 637 | Loss: 0.0489\n",
      "Epoch 638 | Loss: 0.0447\n",
      "Epoch 639 | Loss: 0.0455\n",
      "Epoch 640 | Loss: 0.0392\n",
      "Epoch 641 | Loss: 0.0380\n",
      "Epoch 642 | Loss: 0.0388\n",
      "Epoch 643 | Loss: 0.0332\n",
      "Epoch 644 | Loss: 0.0286\n",
      "Epoch 645 | Loss: 0.0306\n",
      "Epoch 646 | Loss: 0.0277\n",
      "Epoch 647 | Loss: 0.0229\n",
      "Epoch 648 | Loss: 0.0227\n",
      "Epoch 649 | Loss: 0.0232\n",
      "Epoch 650 | Loss: 0.0283\n",
      "Epoch 651 | Loss: 0.0269\n",
      "Epoch 652 | Loss: 0.0366\n",
      "Epoch 653 | Loss: 0.0283\n",
      "Epoch 654 | Loss: 0.0277\n",
      "Epoch 655 | Loss: 0.0261\n",
      "Epoch 656 | Loss: 0.0267\n",
      "Epoch 657 | Loss: 0.0277\n",
      "Epoch 658 | Loss: 0.0576\n",
      "Epoch 659 | Loss: 0.0474\n",
      "Epoch 660 | Loss: 0.0483\n",
      "Epoch 661 | Loss: 0.0469\n",
      "Epoch 662 | Loss: 0.0425\n",
      "Epoch 663 | Loss: 0.0314\n",
      "Epoch 664 | Loss: 0.0286\n",
      "Epoch 665 | Loss: 0.0237\n",
      "Epoch 666 | Loss: 0.0219\n",
      "Epoch 667 | Loss: 0.0221\n",
      "Epoch 668 | Loss: 0.0193\n",
      "Epoch 669 | Loss: 0.0261\n",
      "Epoch 670 | Loss: 0.0276\n",
      "Epoch 671 | Loss: 0.0292\n",
      "Epoch 672 | Loss: 0.0256\n",
      "Epoch 673 | Loss: 0.0267\n",
      "Epoch 674 | Loss: 0.0314\n",
      "Epoch 675 | Loss: 0.0331\n",
      "Epoch 676 | Loss: 0.0411\n",
      "Epoch 677 | Loss: 0.0467\n",
      "Epoch 678 | Loss: 0.0555\n",
      "Epoch 679 | Loss: 0.0458\n",
      "Epoch 680 | Loss: 0.0381\n",
      "Epoch 681 | Loss: 0.0342\n",
      "Epoch 682 | Loss: 0.0305\n",
      "Epoch 683 | Loss: 0.0238\n",
      "Epoch 684 | Loss: 0.0252\n",
      "Epoch 685 | Loss: 0.0208\n",
      "Epoch 686 | Loss: 0.0165\n",
      "Epoch 687 | Loss: 0.0155\n",
      "Epoch 688 | Loss: 0.0146\n",
      "Epoch 689 | Loss: 0.0132\n",
      "Epoch 690 | Loss: 0.0130\n",
      "Epoch 691 | Loss: 0.0122\n",
      "Epoch 692 | Loss: 0.0123\n",
      "Epoch 693 | Loss: 0.0116\n",
      "Epoch 694 | Loss: 0.0115\n",
      "Epoch 695 | Loss: 0.0103\n",
      "Epoch 696 | Loss: 0.0103\n",
      "Epoch 697 | Loss: 0.0105\n",
      "Epoch 698 | Loss: 0.0092\n",
      "Epoch 699 | Loss: 0.0101\n",
      "Epoch 700 | Loss: 0.0093\n",
      "Epoch 701 | Loss: 0.0093\n",
      "Epoch 702 | Loss: 0.0087\n",
      "Epoch 703 | Loss: 0.0095\n",
      "Epoch 704 | Loss: 0.0082\n",
      "Epoch 705 | Loss: 0.0080\n",
      "Epoch 706 | Loss: 0.0090\n",
      "Epoch 707 | Loss: 0.0082\n",
      "Epoch 708 | Loss: 0.0083\n",
      "Epoch 709 | Loss: 0.0086\n",
      "Epoch 710 | Loss: 0.0082\n",
      "Epoch 711 | Loss: 0.0079\n",
      "Epoch 712 | Loss: 0.0077\n",
      "Epoch 713 | Loss: 0.0122\n",
      "Epoch 714 | Loss: 0.0124\n",
      "Epoch 715 | Loss: 0.0115\n",
      "Epoch 716 | Loss: 0.0119\n",
      "Epoch 717 | Loss: 0.0138\n",
      "Epoch 718 | Loss: 0.0101\n",
      "Epoch 719 | Loss: 0.0114\n",
      "Epoch 720 | Loss: 0.0113\n",
      "Epoch 721 | Loss: 0.0321\n",
      "Epoch 722 | Loss: 0.0522\n",
      "Epoch 723 | Loss: 0.0557\n",
      "Epoch 724 | Loss: 0.0321\n",
      "Epoch 725 | Loss: 0.0401\n",
      "Epoch 726 | Loss: 0.0371\n",
      "Epoch 727 | Loss: 0.0426\n",
      "Epoch 728 | Loss: 0.0281\n",
      "Epoch 729 | Loss: 0.0234\n",
      "Epoch 730 | Loss: 0.0198\n",
      "Epoch 731 | Loss: 0.0175\n",
      "Epoch 732 | Loss: 0.0142\n",
      "Epoch 733 | Loss: 0.0144\n",
      "Epoch 734 | Loss: 0.0139\n",
      "Epoch 735 | Loss: 0.0112\n",
      "Epoch 736 | Loss: 0.0120\n",
      "Epoch 737 | Loss: 0.0131\n",
      "Epoch 738 | Loss: 0.0105\n",
      "Epoch 739 | Loss: 0.0128\n",
      "Epoch 740 | Loss: 0.0112\n",
      "Epoch 741 | Loss: 0.0097\n",
      "Epoch 742 | Loss: 0.0103\n",
      "Epoch 743 | Loss: 0.0100\n",
      "Epoch 744 | Loss: 0.0087\n",
      "Epoch 745 | Loss: 0.0085\n",
      "Epoch 746 | Loss: 0.0081\n",
      "Epoch 747 | Loss: 0.0079\n",
      "Epoch 748 | Loss: 0.0080\n",
      "Epoch 749 | Loss: 0.0081\n",
      "Epoch 750 | Loss: 0.0086\n",
      "Epoch 751 | Loss: 0.0072\n",
      "Epoch 752 | Loss: 0.0070\n",
      "Epoch 753 | Loss: 0.0067\n",
      "Epoch 754 | Loss: 0.0077\n",
      "Epoch 755 | Loss: 0.0151\n",
      "Epoch 756 | Loss: 0.0091\n",
      "Epoch 757 | Loss: 0.0086\n",
      "Epoch 758 | Loss: 0.0079\n",
      "Epoch 759 | Loss: 0.0080\n",
      "Epoch 760 | Loss: 0.0070\n",
      "Epoch 761 | Loss: 0.0068\n",
      "Epoch 762 | Loss: 0.0081\n",
      "Epoch 763 | Loss: 0.0061\n",
      "Epoch 764 | Loss: 0.0066\n",
      "Epoch 765 | Loss: 0.0057\n",
      "Epoch 766 | Loss: 0.0061\n",
      "Epoch 767 | Loss: 0.0052\n",
      "Epoch 768 | Loss: 0.0054\n",
      "Epoch 769 | Loss: 0.0049\n",
      "Epoch 770 | Loss: 0.0056\n",
      "Epoch 771 | Loss: 0.0052\n",
      "Epoch 772 | Loss: 0.0049\n",
      "Epoch 773 | Loss: 0.0046\n",
      "Epoch 774 | Loss: 0.0050\n",
      "Epoch 775 | Loss: 0.0055\n",
      "Epoch 776 | Loss: 0.0048\n",
      "Epoch 777 | Loss: 0.0066\n",
      "Epoch 778 | Loss: 0.0081\n",
      "Epoch 779 | Loss: 0.0109\n",
      "Epoch 780 | Loss: 0.0102\n",
      "Epoch 781 | Loss: 0.0066\n",
      "Epoch 782 | Loss: 0.0072\n",
      "Epoch 783 | Loss: 0.0073\n",
      "Epoch 784 | Loss: 0.0086\n",
      "Epoch 785 | Loss: 0.0084\n",
      "Epoch 786 | Loss: 0.0096\n",
      "Epoch 787 | Loss: 0.0072\n",
      "Epoch 788 | Loss: 0.0139\n",
      "Epoch 789 | Loss: 0.0138\n",
      "Epoch 790 | Loss: 0.0146\n",
      "Epoch 791 | Loss: 0.0190\n",
      "Epoch 792 | Loss: 0.0185\n",
      "Epoch 793 | Loss: 0.0289\n",
      "Epoch 794 | Loss: 0.0345\n",
      "Epoch 795 | Loss: 0.0382\n",
      "Epoch 796 | Loss: 0.0334\n",
      "Epoch 797 | Loss: 0.0557\n",
      "Epoch 798 | Loss: 0.0668\n",
      "Epoch 799 | Loss: 0.0769\n",
      "Epoch 800 | Loss: 0.0492\n",
      "Epoch 801 | Loss: 0.0423\n",
      "Epoch 802 | Loss: 0.0266\n",
      "Epoch 803 | Loss: 0.0216\n",
      "Epoch 804 | Loss: 0.0204\n",
      "Epoch 805 | Loss: 0.0141\n",
      "Epoch 806 | Loss: 0.0127\n",
      "Epoch 807 | Loss: 0.0108\n",
      "Epoch 808 | Loss: 0.0089\n",
      "Epoch 809 | Loss: 0.0085\n",
      "Epoch 810 | Loss: 0.0074\n",
      "Epoch 811 | Loss: 0.0065\n",
      "Epoch 812 | Loss: 0.0067\n",
      "Epoch 813 | Loss: 0.0062\n",
      "Epoch 814 | Loss: 0.0059\n",
      "Epoch 815 | Loss: 0.0053\n",
      "Epoch 816 | Loss: 0.0052\n",
      "Epoch 817 | Loss: 0.0058\n",
      "Epoch 818 | Loss: 0.0099\n",
      "Epoch 819 | Loss: 0.0099\n",
      "Epoch 820 | Loss: 0.0095\n",
      "Epoch 821 | Loss: 0.0073\n",
      "Epoch 822 | Loss: 0.0064\n",
      "Epoch 823 | Loss: 0.0060\n",
      "Epoch 824 | Loss: 0.0052\n",
      "Epoch 825 | Loss: 0.0054\n",
      "Epoch 826 | Loss: 0.0064\n",
      "Epoch 827 | Loss: 0.0048\n",
      "Epoch 828 | Loss: 0.0060\n",
      "Epoch 829 | Loss: 0.0054\n",
      "Epoch 830 | Loss: 0.0058\n",
      "Epoch 831 | Loss: 0.0055\n",
      "Epoch 832 | Loss: 0.0040\n",
      "Epoch 833 | Loss: 0.0044\n",
      "Epoch 834 | Loss: 0.0040\n",
      "Epoch 835 | Loss: 0.0040\n",
      "Epoch 836 | Loss: 0.0038\n",
      "Epoch 837 | Loss: 0.0031\n",
      "Epoch 838 | Loss: 0.0041\n",
      "Epoch 839 | Loss: 0.0038\n",
      "Epoch 840 | Loss: 0.0037\n",
      "Epoch 841 | Loss: 0.0030\n",
      "Epoch 842 | Loss: 0.0028\n",
      "Epoch 843 | Loss: 0.0028\n",
      "Epoch 844 | Loss: 0.0027\n",
      "Epoch 845 | Loss: 0.0028\n",
      "Epoch 846 | Loss: 0.0025\n",
      "Epoch 847 | Loss: 0.0025\n",
      "Epoch 848 | Loss: 0.0024\n",
      "Epoch 849 | Loss: 0.0023\n",
      "Epoch 850 | Loss: 0.0024\n",
      "Epoch 851 | Loss: 0.0028\n",
      "Epoch 852 | Loss: 0.0023\n",
      "Epoch 853 | Loss: 0.0021\n",
      "Epoch 854 | Loss: 0.0021\n",
      "Epoch 855 | Loss: 0.0074\n",
      "Epoch 856 | Loss: 0.0088\n",
      "Epoch 857 | Loss: 0.0088\n",
      "Epoch 858 | Loss: 0.0098\n",
      "Epoch 859 | Loss: 0.0080\n",
      "Epoch 860 | Loss: 0.0083\n",
      "Epoch 861 | Loss: 0.0077\n",
      "Epoch 862 | Loss: 0.0192\n",
      "Epoch 863 | Loss: 0.0117\n",
      "Epoch 864 | Loss: 0.0282\n",
      "Epoch 865 | Loss: 0.0229\n",
      "Epoch 866 | Loss: 0.0328\n",
      "Epoch 867 | Loss: 0.0258\n",
      "Epoch 868 | Loss: 0.0274\n",
      "Epoch 869 | Loss: 0.0173\n",
      "Epoch 870 | Loss: 0.0176\n",
      "Epoch 871 | Loss: 0.0126\n",
      "Epoch 872 | Loss: 0.0097\n",
      "Epoch 873 | Loss: 0.0104\n",
      "Epoch 874 | Loss: 0.0073\n",
      "Epoch 875 | Loss: 0.0068\n",
      "Epoch 876 | Loss: 0.0059\n",
      "Epoch 877 | Loss: 0.0050\n",
      "Epoch 878 | Loss: 0.0051\n",
      "Epoch 879 | Loss: 0.0050\n",
      "Epoch 880 | Loss: 0.0049\n",
      "Epoch 881 | Loss: 0.0042\n",
      "Epoch 882 | Loss: 0.0039\n",
      "Epoch 883 | Loss: 0.0036\n",
      "Epoch 884 | Loss: 0.0048\n",
      "Epoch 885 | Loss: 0.0041\n",
      "Epoch 886 | Loss: 0.0039\n",
      "Epoch 887 | Loss: 0.0039\n",
      "Epoch 888 | Loss: 0.0039\n",
      "Epoch 889 | Loss: 0.0055\n",
      "Epoch 890 | Loss: 0.0030\n",
      "Epoch 891 | Loss: 0.0036\n",
      "Epoch 892 | Loss: 0.0032\n",
      "Epoch 893 | Loss: 0.0030\n",
      "Epoch 894 | Loss: 0.0033\n",
      "Epoch 895 | Loss: 0.0028\n",
      "Epoch 896 | Loss: 0.0028\n",
      "Epoch 897 | Loss: 0.0046\n",
      "Epoch 898 | Loss: 0.0145\n",
      "Epoch 899 | Loss: 0.0111\n",
      "Epoch 900 | Loss: 0.0128\n",
      "Epoch 901 | Loss: 0.0113\n",
      "Epoch 902 | Loss: 0.0239\n",
      "Epoch 903 | Loss: 0.0532\n",
      "Epoch 904 | Loss: 0.0302\n",
      "Epoch 905 | Loss: 0.0272\n",
      "Epoch 906 | Loss: 0.0221\n",
      "Epoch 907 | Loss: 0.0243\n",
      "Epoch 908 | Loss: 0.0138\n",
      "Epoch 909 | Loss: 0.0147\n",
      "Epoch 910 | Loss: 0.0133\n",
      "Epoch 911 | Loss: 0.0132\n",
      "Epoch 912 | Loss: 0.0195\n",
      "Epoch 913 | Loss: 0.0266\n",
      "Epoch 914 | Loss: 0.0233\n",
      "Epoch 915 | Loss: 0.0346\n",
      "Epoch 916 | Loss: 0.0205\n",
      "Epoch 917 | Loss: 0.0241\n",
      "Epoch 918 | Loss: 0.0117\n",
      "Epoch 919 | Loss: 0.0121\n",
      "Epoch 920 | Loss: 0.0095\n",
      "Epoch 921 | Loss: 0.0083\n",
      "Epoch 922 | Loss: 0.0076\n",
      "Epoch 923 | Loss: 0.0067\n",
      "Epoch 924 | Loss: 0.0058\n",
      "Epoch 925 | Loss: 0.0050\n",
      "Epoch 926 | Loss: 0.0047\n",
      "Epoch 927 | Loss: 0.0044\n",
      "Epoch 928 | Loss: 0.0032\n",
      "Epoch 929 | Loss: 0.0033\n",
      "Epoch 930 | Loss: 0.0028\n",
      "Epoch 931 | Loss: 0.0028\n",
      "Epoch 932 | Loss: 0.0026\n",
      "Epoch 933 | Loss: 0.0029\n",
      "Epoch 934 | Loss: 0.0034\n",
      "Epoch 935 | Loss: 0.0041\n",
      "Epoch 936 | Loss: 0.0036\n",
      "Epoch 937 | Loss: 0.0038\n",
      "Epoch 938 | Loss: 0.0050\n",
      "Epoch 939 | Loss: 0.0061\n",
      "Epoch 940 | Loss: 0.0047\n",
      "Epoch 941 | Loss: 0.0028\n",
      "Epoch 942 | Loss: 0.0033\n",
      "Epoch 943 | Loss: 0.0027\n",
      "Epoch 944 | Loss: 0.0026\n",
      "Epoch 945 | Loss: 0.0024\n",
      "Epoch 946 | Loss: 0.0026\n",
      "Epoch 947 | Loss: 0.0023\n",
      "Epoch 948 | Loss: 0.0023\n",
      "Epoch 949 | Loss: 0.0029\n",
      "Epoch 950 | Loss: 0.0025\n",
      "Epoch 951 | Loss: 0.0020\n",
      "Epoch 952 | Loss: 0.0021\n",
      "Epoch 953 | Loss: 0.0019\n",
      "Epoch 954 | Loss: 0.0019\n",
      "Epoch 955 | Loss: 0.0018\n",
      "Epoch 956 | Loss: 0.0018\n",
      "Epoch 957 | Loss: 0.0019\n",
      "Epoch 958 | Loss: 0.0020\n",
      "Epoch 959 | Loss: 0.0018\n",
      "Epoch 960 | Loss: 0.0017\n",
      "Epoch 961 | Loss: 0.0022\n",
      "Epoch 962 | Loss: 0.0031\n",
      "Epoch 963 | Loss: 0.0040\n",
      "Epoch 964 | Loss: 0.0042\n",
      "Epoch 965 | Loss: 0.0040\n",
      "Epoch 966 | Loss: 0.0039\n",
      "Epoch 967 | Loss: 0.0032\n",
      "Epoch 968 | Loss: 0.0040\n",
      "Epoch 969 | Loss: 0.0043\n",
      "Epoch 970 | Loss: 0.0022\n",
      "Epoch 971 | Loss: 0.0025\n",
      "Epoch 972 | Loss: 0.0023\n",
      "Epoch 973 | Loss: 0.0019\n",
      "Epoch 974 | Loss: 0.0021\n",
      "Epoch 975 | Loss: 0.0018\n",
      "Epoch 976 | Loss: 0.0018\n",
      "Epoch 977 | Loss: 0.0017\n",
      "Epoch 978 | Loss: 0.0018\n",
      "Epoch 979 | Loss: 0.0016\n",
      "Epoch 980 | Loss: 0.0017\n",
      "Epoch 981 | Loss: 0.0016\n",
      "Epoch 982 | Loss: 0.0015\n",
      "Epoch 983 | Loss: 0.0016\n",
      "Epoch 984 | Loss: 0.0016\n",
      "Epoch 985 | Loss: 0.0016\n",
      "Epoch 986 | Loss: 0.0015\n",
      "Epoch 987 | Loss: 0.0015\n",
      "Epoch 988 | Loss: 0.0015\n",
      "Epoch 989 | Loss: 0.0014\n",
      "Epoch 990 | Loss: 0.0014\n",
      "Epoch 991 | Loss: 0.0015\n",
      "Epoch 992 | Loss: 0.0014\n",
      "Epoch 993 | Loss: 0.0015\n",
      "Epoch 994 | Loss: 0.0015\n",
      "Epoch 995 | Loss: 0.0014\n",
      "Epoch 996 | Loss: 0.0015\n",
      "Epoch 997 | Loss: 0.0015\n",
      "Epoch 998 | Loss: 0.0014\n",
      "Epoch 999 | Loss: 0.0014\n",
      "Epoch 1000 | Loss: 0.0016\n",
      "Epoch 1001 | Loss: 0.0016\n",
      "Epoch 1002 | Loss: 0.0016\n",
      "Epoch 1003 | Loss: 0.0014\n",
      "Epoch 1004 | Loss: 0.0015\n",
      "Epoch 1005 | Loss: 0.0016\n",
      "Epoch 1006 | Loss: 0.0025\n",
      "Epoch 1007 | Loss: 0.0036\n",
      "Epoch 1008 | Loss: 0.0050\n",
      "Epoch 1009 | Loss: 0.0028\n",
      "Epoch 1010 | Loss: 0.0046\n",
      "Epoch 1011 | Loss: 0.0092\n",
      "Epoch 1012 | Loss: 0.0059\n",
      "Epoch 1013 | Loss: 0.0071\n",
      "Epoch 1014 | Loss: 0.0045\n",
      "Epoch 1015 | Loss: 0.0039\n",
      "Epoch 1016 | Loss: 0.0023\n",
      "Epoch 1017 | Loss: 0.0023\n",
      "Epoch 1018 | Loss: 0.0023\n",
      "Epoch 1019 | Loss: 0.0019\n",
      "Epoch 1020 | Loss: 0.0023\n",
      "Epoch 1021 | Loss: 0.0028\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[218], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (dig, input_seq, output_seq) \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# stroke_seq: [batch, seq_len, 4]\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     input_seq \u001b[38;5;241m=\u001b[39m input_seq\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m     target_seq \u001b[38;5;241m=\u001b[39m output_seq\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:412\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_BaseDataLoaderIter\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_SingleProcessDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:735\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, loader):\n\u001b[1;32m--> 735\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:667\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collate_fn \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mcollate_fn\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_sampler)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_seed \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 667\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint64\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;241m.\u001b[39mrandom_(generator\u001b[38;5;241m=\u001b[39mloader\u001b[38;5;241m.\u001b[39mgenerator)\n\u001b[0;32m    669\u001b[0m     \u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    670\u001b[0m )\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent_workers \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mpersistent_workers\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 5000\n",
    "\n",
    "best_val_loss = 9999\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (dig, input_seq, output_seq) in loader:\n",
    "        # stroke_seq: [batch, seq_len, 4]\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = output_seq.to(device)\n",
    "        dig = dig.to(device)\n",
    "\n",
    "        pred_seq, hidden = model(input_seq, onehot_digit = dig)  # [batch, seq_len-1, 4]\n",
    "\n",
    "        # Separate predictions\n",
    "        pred_dxdy = pred_seq[..., :2]         # [batch, seq_len-1, 2]\n",
    "        pred_eos_eod = pred_seq[..., 2:]      # [batch, seq_len-1, 2]\n",
    "\n",
    "        # Separate targets\n",
    "        target_dxdy = target_seq[..., :2]\n",
    "        target_eos_eod = target_seq[..., 2:]\n",
    "\n",
    "        # Compute losses\n",
    "        loss_dxdy = dx_dy_loss_fn(pred_dxdy, target_dxdy)\n",
    "        loss_eos_eod = eos_eod_loss_fn(pred_eos_eod, target_eos_eod)\n",
    "\n",
    "        loss = loss_dxdy + loss_eos_eod\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # if total_loss < best_val_loss:\n",
    "    #     best_val_loss = total_loss\n",
    "    #     counter = 0\n",
    "    # else:\n",
    "    #     counter += 1\n",
    "    #     if counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f0d776b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(initial_input, onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(max_length-1):\n",
    "        output, hidden = model(output, hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "8e548538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c1ac7a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save model if good\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_weights.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save model if good\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e579b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitToStrokeLSTM(\n",
       "  (embedding): Linear(in_features=10, out_features=256, bias=True)\n",
       "  (lstm): LSTM(4, 256, num_layers=2, batch_first=True)\n",
       "  (output_head): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DigitToStrokeLSTM().to(device) # create a new instance\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()  # set to evaluation mode if you're doing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ec9780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shit: 0, 5, 6, 8, 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
