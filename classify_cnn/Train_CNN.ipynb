{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162670",
   "metadata": {},
   "source": [
    "# 1. Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e6bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(root='.', download=True, transform=torchvision.transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(mnist, np.arange(10000)), \n",
    "                                           batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(torch.utils.data.Subset(mnist, np.arange(10000, 15000)), \n",
    "                                          batch_size=1000, shuffle=True)\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2c63c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # First Convolution Block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Second Convolution Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Fully Connected (Dense) layers\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64 * 7 * 7, 512),  # Input size depends on the output of convolutional layers\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 10)  # Output layer (10 classes)\n",
    "        )\n",
    "            \n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "927f2300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.3190 | Accuracy: 90.30%\n",
      "Epoch [2/10] Loss: 0.1625 | Accuracy: 94.96%\n",
      "Epoch [3/10] Loss: 0.1239 | Accuracy: 96.27%\n",
      "Epoch [4/10] Loss: 0.1270 | Accuracy: 96.24%\n",
      "Epoch [5/10] Loss: 0.0980 | Accuracy: 97.10%\n",
      "Epoch [6/10] Loss: 0.0951 | Accuracy: 97.26%\n",
      "Epoch [7/10] Loss: 0.1018 | Accuracy: 96.76%\n",
      "Epoch [8/10] Loss: 0.0760 | Accuracy: 97.46%\n",
      "Epoch [9/10] Loss: 0.0651 | Accuracy: 97.94%\n",
      "Epoch [10/10] Loss: 0.0699 | Accuracy: 97.85%\n"
     ]
    }
   ],
   "source": [
    "model = CNNModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] Loss: {running_loss/len(train_loader):.4f} | Accuracy: {100.*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "371a46a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.62%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100.*correct/total:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "09aaebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn_weights/cnn_weights5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "12b59481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGFCAYAAAASI+9IAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAGpElEQVR4nO3csW7UWhRAUXsykYiUAv7//xBUFNOkMOPX7QYBz1dgm8la/ZVPLEd7bnPmdV3XCQCmabocPQAA5yEKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg16MHgLOY5/noEU5hXdejR+BAbgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACAW4nF6Z15Ud7/fN58Z/XtGFtVdLn73sY0vBoCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAxEI8hpx5Sd00jS2PO7u93vnIuzv798D/56YAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQBiIR67LTNblmXo3NPT0x+ehF+x3O59c1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQBiSyrT/X7f5Tm2b8L5uSkAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBYiIdFdf+It7e3o0fgHXBTACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAsRAP/hEvLy+bz3z58uUvTMIjc1MAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgCZ13Vdjx4C3pNlWYbOPT8/bz7j35ut3BQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYBcjx4A3puRbafTNE2fP3/+w5PAj9wUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBA5nVd16OH4Fgjn8Dlsv33xCN+avM87/asR3x/nI+bAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAyPXoATjeXkvdRp/zaIvgvn//fvQI8FNuCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIBbiMWRkSd3oQryRc8uy7HJmxOXitxjn5esEIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgCxEI/djCzRm6Zp+vr16+Yz1+v2T/t2u20+A4/GTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAMi8jq6uhBOb5/noEX5qWZahc09PT394EviRmwIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIiFeDykkYV4375923zm06dPm8+M8q/KHtwUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBArkcPAL9zu912ec7Hjx83nxlZUjeyrG+apunt7W3zmQ8fPgw9i/fLTQGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAGReRzZ6wY5GFsiNLNF7fX3dfGbE6EK8Ef692cpNAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoAxEI8djP6qV0u23+7nPmz9h44MzcFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAg16MH4P243+9Hj3AKI9tOYS++TgAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAkHld1/XoIeBX5nk+eoRTGFko6N2xlZsCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFADI9egB4HdGdjYuy7L5zPPz8+Yzt9tt85nX19fNZ2AvbgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACDzOrJtDICH5KYAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQEQBgIgCABEFACIKAEQUAIgoABBRACCiAEBEAYCIAgARBQAiCgBEFACIKAAQUQAgogBARAGAiAIAEQUAIgoARBQAiCgAEFEAIKIAQP4DWiWeDaGkqTgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted Class: 8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Step 1: Load the image (example with a saved file)\n",
    "img_path = '../sketch_stroke_digit_rnn/plots/image.png'  # Path to the saved 28x28 image\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Step 2: Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Ensure the image is in grayscale\n",
    "    transforms.ToTensor(),  # Convert to PyTorch tensor (scales pixels to [0, 1])\n",
    "])\n",
    "\n",
    "img_tensor = transform(img).to(device)\n",
    "\n",
    "img_tensor[img_tensor<0.9] = 0\n",
    "img = img_tensor.squeeze(0).squeeze(0).cpu().numpy()  # Remove batch and channel dimensions\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Add batch dimension (PyTorch models expect a batch of images)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension (1, 1, 28, 28)\n",
    "\n",
    "# Step 4: Pass the tensor to the model\n",
    "model.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():  # Turn off gradients since we're in inference mode\n",
    "    print(img_tensor.size())\n",
    "    output = model(img_tensor)  # Pass the image tensor to the model for prediction\n",
    "\n",
    "# Step 5: Interpret the output\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "print(f'Predicted Class: {predicted_class.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f823d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
