{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Loading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence, save_path=None, show=True):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.plot(xs, ys, linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9133b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 13, 14, 19, 20, 21, 22, 23, 26, 27, 28, 29, 30, 31, 37, 38, 39, 40, 44, 46, 48, 49, 51, 52, 53, 54, 55, 56, 57, 58, 61, 65, 66, 68, 69, 70, 71, 74, 76, 79, 80, 81, 85, 86, 88, 89, 90, 91, 94, 98, 99, 100, 101, 102, 110, 111, 112, 113, 116, 120, 123, 125, 126, 127, 128, 129, 130, 131, 132, 134, 135, 136, 137, 138, 139, 140, 142, 143, 145, 146, 148, 154, 156, 157, 161, 162, 165, 168, 169, 170, 171, 172, 173, 175, 176, 178, 180, 182, 183, 186, 187, 188, 189, 191, 192, 196, 197, 201, 202, 203, 204, 205, 213, 215, 217, 219, 223, 224, 225, 228, 230, 231, 234, 239, 240, 246, 250, 251, 252, 254, 255, 259, 260, 261, 263, 264, 265, 267, 268, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, \n",
    "           281, 283, 284, 286, 289, 291, 292, 294, 295, 296, 298, 302, 306, 309, 311, 312, 313, 316, 319, 321, 323, 329, 330, 331, 332, 333, 334, 335, 336, 339, 342, 344, 345, 346, 347, 348, 351, 352, 353, 354, 356, 357, 361, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 383, 384, 388, 389, 392, 393, 397, 398, 401, 406, 407, 411, 413, 414, 416, 419, 422, 424, 426, 427, 430, 434, 440, 442, 444, 446, 455, 456, 459, 461, 463, 466, 469, 471, 472, 473, 478, 480, 481, 485, 487, 489, 491, 494, 496, 498, 501, 502, 505, 506, 507, 508, 509, 512, 517, 518, 521, 525, 528, 530, 531, 532, 534, 537, 540, 541, 545, 548,\n",
    "           548, 556, 558, 564, 568, 574, 575, 576, 577, 585, 586, 587, 588, 589, 594, 598, 599, 600, 604, 606, 607, 608, 609, 615, 616, 618, 619, 624, 626, 634, 645, 649, 651, 652, 656, 663, 665, 666, 667, 674, 677, 682, 683, 688, 705, 706, 710, 713, 715, 722, 724, 725, 727, 728, 729, 734, 739, 741, 745, 747, 749, 750, 751, 752, 754, 755, 757, 759, 760, 766, 767, 768, 769, 771, 772, 773, 774, 775, 778, 783, 788, 789, 790, 807, 809, 814, 820, 824, 826, 828, 833, 835, 837, 840, 843, 847, 849, 851, 853, 854, 857, 859, 860, 867, 869, 870, 871, 880, 884, 886, 887, 889, 890, 891, 897, 899, 906, 907, 911, 917, 920, 927, 931, 940, 941, 944, 945, 946, 948, 954, 955, 963, 970, 972, 973, 974, 976, 978, 979, 982, 984, 985, 986, 997]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08ded58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = [[] for _ in range(10)]\n",
    "\n",
    "for i in indexes:\n",
    "    try:\n",
    "        data = np.loadtxt(f'../sequences/testimg-{i}-targetdata.txt', delimiter=' ')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå File not found at path: {i}\")\n",
    "        continue\n",
    "    \n",
    "    inputOneshot = data[0, 0:10]\n",
    "    outputStrokes = data[:, 10:]\n",
    "    outputStrokes[:, 0] = outputStrokes[:, 0]/28\n",
    "    outputStrokes[:, 1] = outputStrokes[:, 1]/28\n",
    "    \n",
    "    datas[getNumFromOneHot(inputOneshot)].append(outputStrokes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a163eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amount = min([len(x) for x in datas])\n",
    "amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84ec919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_stroke_sequence(datas[8][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd22b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = []\n",
    "output_data = []\n",
    "\n",
    "for i in range(10):\n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[i] = 1\n",
    "    \n",
    "    smallest_10 = sorted(datas[i], key=len)[:amount]\n",
    "    for k in smallest_10:\n",
    "        input_data.append(temp_onehot)\n",
    "        output_data.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e96a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = []\n",
    "# output_data = []\n",
    "\n",
    "# for i in range(10):\n",
    "#     temp_onehot = np.zeros(10)\n",
    "#     temp_onehot[i] = 1\n",
    "    \n",
    "#     for k in range(amount):\n",
    "#         input_data.append(temp_onehot)\n",
    "#         output_data.append(datas[i][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04766e4",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd2c493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "# Finding the max length of a sequence\n",
    "max_length = 0\n",
    "j = 0\n",
    "for i in range(len(output_data)):\n",
    "    if len(output_data[i]) > max_length:\n",
    "        max_length = len(output_data[i])\n",
    "    j += 1\n",
    "\n",
    "print(max_length)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edeec1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(len(output_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642bf923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding the sequences so that they are all the same size (good for batching)\n",
    "padded_output_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_output_data[i, :len(output_data[i]), :] = output_data[i]\n",
    "    padded_output_data[i, len(output_data[i]):, :] = [0, 0, 0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37cfe765",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_input_data = np.zeros( (len(output_data), max_length, 4) )\n",
    "\n",
    "for i in range(len(output_data)):\n",
    "    padded_input_data[i, 0, :] = [0, 0, 0, 0]\n",
    "    padded_input_data[i, 1:, :] = padded_output_data[i, :max_length-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8523173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrokeDataset(Dataset):\n",
    "    def __init__(self, onehot, inputs, outputstroke):\n",
    "        self.digit = onehot                     # shape: [N]\n",
    "        self.inputstroke = inputs               # list of [seq_len, 4] arrays\n",
    "        self.outputstroke = outputstroke        # list of [seq_len, 4] arrays\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.digit)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        digit = self.digit[idx]\n",
    "        inputs = self.inputstroke[idx]\n",
    "        outputs = self.outputstroke[idx]\n",
    "        return torch.tensor(digit, dtype=torch.float32), torch.tensor(inputs, dtype=torch.float32), torch.tensor(outputs, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1171d853",
   "metadata": {},
   "outputs": [],
   "source": [
    "strokeDataset = StrokeDataset(input_data, padded_input_data, padded_output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d67af144",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(strokeDataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d8f994",
   "metadata": {},
   "source": [
    "Creating Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2082a2d",
   "metadata": {},
   "source": [
    "Notes\n",
    "\n",
    "\n",
    "RNN:\n",
    "input_size = output_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e58d8d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4+10,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAMAGE WEIGHTS\n",
    "\n",
    "def damage_smallest(model, p_smallest): # energy constraint\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_smallest == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            weight_magnitudes = tensor.abs().view(-1)\n",
    "            k = int(weight_magnitudes.numel() * p_smallest)\n",
    "\n",
    "            if k == 0:\n",
    "                continue\n",
    "            threshold = weight_magnitudes.kthvalue(k).values.item()\n",
    "\n",
    "            mask = tensor.abs() >= threshold\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "def damage_fas(model,  p_block, p_reflect, p_filter):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_block + p_reflect + p_filter == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            flat_weights = tensor.view(-1)\n",
    "            nonzero_indices = (flat_weights!=0).nonzero(as_tuple=True)[0]\n",
    "            num_nonzero_indices = nonzero_indices.numel()\n",
    "            if num_nonzero_indices == 0:\n",
    "                continue\n",
    "\n",
    "            # percentage of weights damaged will be taken from the number of nonzero weights\n",
    "            # simulated fas damage occurs after energy constraint blockage\n",
    "            num_block = int(num_nonzero_indices * p_block)\n",
    "            num_reflect = int(num_nonzero_indices * p_reflect)\n",
    "            num_filter = int(num_nonzero_indices * p_filter)\n",
    "\n",
    "            shuffled_indices = nonzero_indices[torch.randperm(num_nonzero_indices, device=flat_weights.device)]\n",
    "\n",
    "            indices_block = shuffled_indices[:num_block]\n",
    "            indices_reflect = shuffled_indices[num_block:num_block+num_reflect]\n",
    "            indices_filter = shuffled_indices[num_block+num_reflect:num_block+num_reflect+num_filter]\n",
    "\n",
    "            # do damage\n",
    "            # blockage: set weights to 0\n",
    "            if p_block != 0:\n",
    "                flat_weights[indices_block] = 0\n",
    "\n",
    "            # reflect: halve weights\n",
    "            if p_reflect != 0:\n",
    "                flat_weights[indices_reflect] *= 0.5\n",
    "\n",
    "            # filter: low pass filter (lusch et al)\n",
    "            if p_filter != 0:\n",
    "                weights_to_filter = flat_weights[indices_filter]            # get weights before transformation\n",
    "                signs = torch.sign(weights_to_filter)                       # get signs of weights\n",
    "                abs_weights_to_filter = weights_to_filter.abs()             # get high_weight, should be in the 95th percentile for all weights\n",
    "                high_weight = torch.quantile(flat_weights.abs(), 0.95)      # scale weights to mostly between -1 and 1\n",
    "                x = abs_weights_to_filter / high_weight\n",
    "                transformed_weights = -0.2744 * x**2 + 0.9094 * x - 0.0192\n",
    "                gaussian_noise = torch.randn_like(transformed_weights) * 0.05\n",
    "                transformed_weights += gaussian_noise\n",
    "                transformed_weights = transformed_weights * signs * high_weight # rescale\n",
    "                flat_weights[indices_filter] = transformed_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92069110",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "\n",
    "# # damage weights to simulate energy constraint and lesioning\n",
    "# damage_smallest(model, 0.0)\n",
    "# damage_fas(model, 0.0, 0.0, 0.0)\n",
    "\n",
    "dx_dy_loss_fn = nn.MSELoss()\n",
    "eos_eod_loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7354307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.4013\n",
      "Epoch 2 | Loss: 0.2734\n",
      "Epoch 3 | Loss: 0.2226\n",
      "Epoch 4 | Loss: 0.1976\n",
      "Epoch 5 | Loss: 0.1876\n",
      "Epoch 6 | Loss: 0.1832\n",
      "Epoch 7 | Loss: 0.1769\n",
      "Epoch 8 | Loss: 0.1742\n",
      "Epoch 9 | Loss: 0.1722\n",
      "Epoch 10 | Loss: 0.1667\n",
      "Epoch 11 | Loss: 0.1585\n",
      "Epoch 12 | Loss: 0.1534\n",
      "Epoch 13 | Loss: 0.1541\n",
      "Epoch 14 | Loss: 0.1552\n",
      "Epoch 15 | Loss: 0.1625\n",
      "Epoch 16 | Loss: 0.1543\n",
      "Epoch 17 | Loss: 0.1533\n",
      "Epoch 18 | Loss: 0.1617\n",
      "Epoch 19 | Loss: 0.1628\n",
      "Epoch 20 | Loss: 0.1516\n",
      "Epoch 21 | Loss: 0.1450\n",
      "Epoch 22 | Loss: 0.1475\n",
      "Epoch 23 | Loss: 0.1491\n",
      "Epoch 24 | Loss: 0.1425\n",
      "Epoch 25 | Loss: 0.1428\n",
      "Epoch 26 | Loss: 0.1400\n",
      "Epoch 27 | Loss: 0.1482\n",
      "Epoch 28 | Loss: 0.1398\n",
      "Epoch 29 | Loss: 0.1411\n",
      "Epoch 30 | Loss: 0.1482\n",
      "Epoch 31 | Loss: 0.1404\n",
      "Epoch 32 | Loss: 0.1361\n",
      "Epoch 33 | Loss: 0.1359\n",
      "Epoch 34 | Loss: 0.1373\n",
      "Epoch 35 | Loss: 0.1384\n",
      "Epoch 36 | Loss: 0.1312\n",
      "Epoch 37 | Loss: 0.1335\n",
      "Epoch 38 | Loss: 0.1299\n",
      "Epoch 39 | Loss: 0.1283\n",
      "Epoch 40 | Loss: 0.1281\n",
      "Epoch 41 | Loss: 0.1274\n",
      "Epoch 42 | Loss: 0.1293\n",
      "Epoch 43 | Loss: 0.1325\n",
      "Epoch 44 | Loss: 0.1238\n",
      "Epoch 45 | Loss: 0.1366\n",
      "Epoch 46 | Loss: 0.1288\n",
      "Epoch 47 | Loss: 0.1250\n",
      "Epoch 48 | Loss: 0.1252\n",
      "Epoch 49 | Loss: 0.1475\n",
      "Epoch 50 | Loss: 0.1370\n",
      "Epoch 51 | Loss: 0.1376\n",
      "Epoch 52 | Loss: 0.1281\n",
      "Epoch 53 | Loss: 0.1227\n",
      "Epoch 54 | Loss: 0.1205\n",
      "Epoch 55 | Loss: 0.1215\n",
      "Epoch 56 | Loss: 0.1292\n",
      "Epoch 57 | Loss: 0.1192\n",
      "Epoch 58 | Loss: 0.1186\n",
      "Epoch 59 | Loss: 0.1177\n",
      "Epoch 60 | Loss: 0.1173\n",
      "Epoch 61 | Loss: 0.1159\n",
      "Epoch 62 | Loss: 0.1207\n",
      "Epoch 63 | Loss: 0.1196\n",
      "Epoch 64 | Loss: 0.1114\n",
      "Epoch 65 | Loss: 0.1134\n",
      "Epoch 66 | Loss: 0.1076\n",
      "Epoch 67 | Loss: 0.1052\n",
      "Epoch 68 | Loss: 0.1007\n",
      "Epoch 69 | Loss: 0.0981\n",
      "Epoch 70 | Loss: 0.0960\n",
      "Epoch 71 | Loss: 0.0955\n",
      "Epoch 72 | Loss: 0.0962\n",
      "Epoch 73 | Loss: 0.0927\n",
      "Epoch 74 | Loss: 0.0876\n",
      "Epoch 75 | Loss: 0.0840\n",
      "Epoch 76 | Loss: 0.0964\n",
      "Epoch 77 | Loss: 0.0946\n",
      "Epoch 78 | Loss: 0.0883\n",
      "Epoch 79 | Loss: 0.0828\n",
      "Epoch 80 | Loss: 0.0756\n",
      "Epoch 81 | Loss: 0.0777\n",
      "Epoch 82 | Loss: 0.0832\n",
      "Epoch 83 | Loss: 0.0924\n",
      "Epoch 84 | Loss: 0.1010\n",
      "Epoch 85 | Loss: 0.0897\n",
      "Epoch 86 | Loss: 0.0872\n",
      "Epoch 87 | Loss: 0.0787\n",
      "Epoch 88 | Loss: 0.0768\n",
      "Epoch 89 | Loss: 0.0750\n",
      "Epoch 90 | Loss: 0.0648\n",
      "Epoch 91 | Loss: 0.0674\n",
      "Epoch 92 | Loss: 0.0572\n",
      "Epoch 93 | Loss: 0.0560\n",
      "Epoch 94 | Loss: 0.0631\n",
      "Epoch 95 | Loss: 0.0603\n",
      "Epoch 96 | Loss: 0.0536\n",
      "Epoch 97 | Loss: 0.0501\n",
      "Epoch 98 | Loss: 0.0571\n",
      "Epoch 99 | Loss: 0.0600\n",
      "Epoch 100 | Loss: 0.0614\n",
      "Epoch 101 | Loss: 0.0673\n",
      "Epoch 102 | Loss: 0.0821\n",
      "Epoch 103 | Loss: 0.0751\n",
      "Epoch 104 | Loss: 0.0694\n",
      "Epoch 105 | Loss: 0.0767\n",
      "Epoch 106 | Loss: 0.0689\n",
      "Epoch 107 | Loss: 0.0699\n",
      "Epoch 108 | Loss: 0.0631\n",
      "Epoch 109 | Loss: 0.0546\n",
      "Epoch 110 | Loss: 0.0523\n",
      "Epoch 111 | Loss: 0.0525\n",
      "Epoch 112 | Loss: 0.0494\n",
      "Epoch 113 | Loss: 0.0567\n",
      "Epoch 114 | Loss: 0.0536\n",
      "Epoch 115 | Loss: 0.0673\n",
      "Epoch 116 | Loss: 0.0603\n",
      "Epoch 117 | Loss: 0.0761\n",
      "Epoch 118 | Loss: 0.0521\n",
      "Epoch 119 | Loss: 0.0522\n",
      "Epoch 120 | Loss: 0.0407\n",
      "Epoch 121 | Loss: 0.0390\n",
      "Epoch 122 | Loss: 0.0382\n",
      "Epoch 123 | Loss: 0.0382\n",
      "Epoch 124 | Loss: 0.0393\n",
      "Epoch 125 | Loss: 0.0381\n",
      "Epoch 126 | Loss: 0.0419\n",
      "Epoch 127 | Loss: 0.0356\n",
      "Epoch 128 | Loss: 0.0347\n",
      "Epoch 129 | Loss: 0.0284\n",
      "Epoch 130 | Loss: 0.0263\n",
      "Epoch 131 | Loss: 0.0259\n",
      "Epoch 132 | Loss: 0.0318\n",
      "Epoch 133 | Loss: 0.0211\n",
      "Epoch 134 | Loss: 0.0203\n",
      "Epoch 135 | Loss: 0.0216\n",
      "Epoch 136 | Loss: 0.0227\n",
      "Epoch 137 | Loss: 0.0271\n",
      "Epoch 138 | Loss: 0.0575\n",
      "Epoch 139 | Loss: 0.0589\n",
      "Epoch 140 | Loss: 0.0795\n",
      "Epoch 141 | Loss: 0.0770\n",
      "Epoch 142 | Loss: 0.0993\n",
      "Epoch 143 | Loss: 0.0794\n",
      "Epoch 144 | Loss: 0.0616\n",
      "Epoch 145 | Loss: 0.0538\n",
      "Epoch 146 | Loss: 0.0436\n",
      "Epoch 147 | Loss: 0.0383\n",
      "Epoch 148 | Loss: 0.0336\n",
      "Epoch 149 | Loss: 0.0305\n",
      "Epoch 150 | Loss: 0.0261\n",
      "Epoch 151 | Loss: 0.0219\n",
      "Epoch 152 | Loss: 0.0202\n",
      "Epoch 153 | Loss: 0.0212\n",
      "Epoch 154 | Loss: 0.0199\n",
      "Epoch 155 | Loss: 0.0170\n",
      "Epoch 156 | Loss: 0.0200\n",
      "Epoch 157 | Loss: 0.0186\n",
      "Epoch 158 | Loss: 0.0169\n",
      "Epoch 159 | Loss: 0.0160\n",
      "Epoch 160 | Loss: 0.0146\n",
      "Epoch 161 | Loss: 0.0117\n",
      "Epoch 162 | Loss: 0.0133\n",
      "Epoch 163 | Loss: 0.0130\n",
      "Epoch 164 | Loss: 0.0120\n",
      "Epoch 165 | Loss: 0.0130\n",
      "Epoch 166 | Loss: 0.0132\n",
      "Epoch 167 | Loss: 0.0129\n",
      "Epoch 168 | Loss: 0.0200\n",
      "Epoch 169 | Loss: 0.0289\n",
      "Epoch 170 | Loss: 0.0269\n",
      "Epoch 171 | Loss: 0.0267\n",
      "Epoch 172 | Loss: 0.0248\n",
      "Epoch 173 | Loss: 0.0432\n",
      "Epoch 174 | Loss: 0.0254\n",
      "Epoch 175 | Loss: 0.0413\n",
      "Epoch 176 | Loss: 0.0367\n",
      "Epoch 177 | Loss: 0.0328\n",
      "Epoch 178 | Loss: 0.0307\n",
      "Epoch 179 | Loss: 0.0252\n",
      "Epoch 180 | Loss: 0.0268\n",
      "Epoch 181 | Loss: 0.0358\n",
      "Epoch 182 | Loss: 0.0252\n",
      "Epoch 183 | Loss: 0.0282\n",
      "Epoch 184 | Loss: 0.0176\n",
      "Epoch 185 | Loss: 0.0196\n",
      "Epoch 186 | Loss: 0.0134\n",
      "Epoch 187 | Loss: 0.0133\n",
      "Epoch 188 | Loss: 0.0133\n",
      "Epoch 189 | Loss: 0.0121\n",
      "Epoch 190 | Loss: 0.0111\n",
      "Epoch 191 | Loss: 0.0102\n",
      "Epoch 192 | Loss: 0.0113\n",
      "Epoch 193 | Loss: 0.0113\n",
      "Epoch 194 | Loss: 0.0089\n",
      "Epoch 195 | Loss: 0.0092\n",
      "Epoch 196 | Loss: 0.0098\n",
      "Epoch 197 | Loss: 0.0088\n",
      "Epoch 198 | Loss: 0.0083\n",
      "Epoch 199 | Loss: 0.0097\n",
      "Epoch 200 | Loss: 0.0082\n",
      "Epoch 201 | Loss: 0.0086\n",
      "Epoch 202 | Loss: 0.0093\n",
      "Epoch 203 | Loss: 0.0086\n",
      "Epoch 204 | Loss: 0.0096\n",
      "Epoch 205 | Loss: 0.0103\n",
      "Epoch 206 | Loss: 0.0126\n",
      "Epoch 207 | Loss: 0.0124\n",
      "Epoch 208 | Loss: 0.0107\n",
      "Epoch 209 | Loss: 0.0091\n",
      "Epoch 210 | Loss: 0.0080\n",
      "Epoch 211 | Loss: 0.0122\n",
      "Epoch 212 | Loss: 0.0092\n",
      "Epoch 213 | Loss: 0.0095\n",
      "Epoch 214 | Loss: 0.0086\n",
      "Epoch 215 | Loss: 0.0079\n",
      "Epoch 216 | Loss: 0.0085\n",
      "Epoch 217 | Loss: 0.0089\n",
      "Epoch 218 | Loss: 0.0078\n",
      "Epoch 219 | Loss: 0.0075\n",
      "Epoch 220 | Loss: 0.0071\n",
      "Epoch 221 | Loss: 0.0111\n",
      "Epoch 222 | Loss: 0.0087\n",
      "Epoch 223 | Loss: 0.0086\n",
      "Epoch 224 | Loss: 0.0068\n",
      "Epoch 225 | Loss: 0.0064\n",
      "Epoch 226 | Loss: 0.0068\n",
      "Epoch 227 | Loss: 0.0067\n",
      "Epoch 228 | Loss: 0.0068\n",
      "Epoch 229 | Loss: 0.0067\n",
      "Epoch 230 | Loss: 0.0072\n",
      "Epoch 231 | Loss: 0.0102\n",
      "Epoch 232 | Loss: 0.0107\n",
      "Epoch 233 | Loss: 0.0130\n",
      "Epoch 234 | Loss: 0.0172\n",
      "Epoch 235 | Loss: 0.0166\n",
      "Epoch 236 | Loss: 0.0114\n",
      "Epoch 237 | Loss: 0.0175\n",
      "Epoch 238 | Loss: 0.0219\n",
      "Epoch 239 | Loss: 0.0343\n",
      "Epoch 240 | Loss: 0.0255\n",
      "Epoch 241 | Loss: 0.0355\n",
      "Epoch 242 | Loss: 0.0588\n",
      "Epoch 243 | Loss: 0.0503\n",
      "Epoch 244 | Loss: 0.0570\n",
      "Epoch 245 | Loss: 0.0508\n",
      "Epoch 246 | Loss: 0.0479\n",
      "Epoch 247 | Loss: 0.0377\n",
      "Epoch 248 | Loss: 0.0393\n",
      "Epoch 249 | Loss: 0.0260\n",
      "Epoch 250 | Loss: 0.0281\n",
      "Epoch 251 | Loss: 0.0202\n",
      "Epoch 252 | Loss: 0.0157\n",
      "Epoch 253 | Loss: 0.0147\n",
      "Epoch 254 | Loss: 0.0117\n",
      "Epoch 255 | Loss: 0.0102\n",
      "Epoch 256 | Loss: 0.0112\n",
      "Epoch 257 | Loss: 0.0100\n",
      "Epoch 258 | Loss: 0.0082\n",
      "Epoch 259 | Loss: 0.0087\n",
      "Epoch 260 | Loss: 0.0073\n",
      "Epoch 261 | Loss: 0.0078\n",
      "Epoch 262 | Loss: 0.0073\n",
      "Epoch 263 | Loss: 0.0079\n",
      "Epoch 264 | Loss: 0.0062\n",
      "Epoch 265 | Loss: 0.0089\n",
      "Epoch 266 | Loss: 0.0070\n",
      "Epoch 267 | Loss: 0.0070\n",
      "Epoch 268 | Loss: 0.0065\n",
      "Epoch 269 | Loss: 0.0076\n",
      "Epoch 270 | Loss: 0.0065\n",
      "Epoch 271 | Loss: 0.0063\n",
      "Epoch 272 | Loss: 0.0081\n",
      "Epoch 273 | Loss: 0.0062\n",
      "Epoch 274 | Loss: 0.0060\n",
      "Epoch 275 | Loss: 0.0064\n",
      "Epoch 276 | Loss: 0.0062\n",
      "Epoch 277 | Loss: 0.0091\n",
      "Epoch 278 | Loss: 0.0062\n",
      "Epoch 279 | Loss: 0.0071\n",
      "Epoch 280 | Loss: 0.0061\n",
      "Epoch 281 | Loss: 0.0086\n",
      "Epoch 282 | Loss: 0.0064\n",
      "Epoch 283 | Loss: 0.0082\n",
      "Epoch 284 | Loss: 0.0079\n",
      "Epoch 285 | Loss: 0.0056\n",
      "Epoch 286 | Loss: 0.0066\n",
      "Epoch 287 | Loss: 0.0069\n",
      "Epoch 288 | Loss: 0.0070\n",
      "Epoch 289 | Loss: 0.0052\n",
      "Epoch 290 | Loss: 0.0055\n",
      "Epoch 291 | Loss: 0.0059\n",
      "Epoch 292 | Loss: 0.0050\n",
      "Epoch 293 | Loss: 0.0058\n",
      "Epoch 294 | Loss: 0.0061\n",
      "Epoch 295 | Loss: 0.0058\n",
      "Epoch 296 | Loss: 0.0052\n",
      "Epoch 297 | Loss: 0.0067\n",
      "Epoch 298 | Loss: 0.0068\n",
      "Epoch 299 | Loss: 0.0051\n",
      "Epoch 300 | Loss: 0.0063\n",
      "Epoch 301 | Loss: 0.0061\n",
      "Epoch 302 | Loss: 0.0058\n",
      "Epoch 303 | Loss: 0.0058\n",
      "Epoch 304 | Loss: 0.0080\n",
      "Epoch 305 | Loss: 0.0073\n",
      "Epoch 306 | Loss: 0.0114\n",
      "Epoch 307 | Loss: 0.0099\n",
      "Epoch 308 | Loss: 0.0164\n",
      "Epoch 309 | Loss: 0.0208\n",
      "Epoch 310 | Loss: 0.0515\n",
      "Epoch 311 | Loss: 0.0412\n",
      "Epoch 312 | Loss: 0.0374\n",
      "Epoch 313 | Loss: 0.0345\n",
      "Epoch 314 | Loss: 0.0287\n",
      "Epoch 315 | Loss: 0.0297\n",
      "Epoch 316 | Loss: 0.0194\n",
      "Epoch 317 | Loss: 0.0165\n",
      "Epoch 318 | Loss: 0.0152\n",
      "Epoch 319 | Loss: 0.0121\n",
      "Epoch 320 | Loss: 0.0101\n",
      "Epoch 321 | Loss: 0.0090\n",
      "Epoch 322 | Loss: 0.0090\n",
      "Epoch 323 | Loss: 0.0076\n",
      "Epoch 324 | Loss: 0.0065\n",
      "Epoch 325 | Loss: 0.0058\n",
      "Epoch 326 | Loss: 0.0067\n",
      "Epoch 327 | Loss: 0.0056\n",
      "Epoch 328 | Loss: 0.0054\n",
      "Epoch 329 | Loss: 0.0057\n",
      "Epoch 330 | Loss: 0.0053\n",
      "Epoch 331 | Loss: 0.0066\n",
      "Epoch 332 | Loss: 0.0063\n",
      "Epoch 333 | Loss: 0.0061\n",
      "Epoch 334 | Loss: 0.0056\n",
      "Epoch 335 | Loss: 0.0057\n",
      "Epoch 336 | Loss: 0.0058\n",
      "Epoch 337 | Loss: 0.0062\n",
      "Epoch 338 | Loss: 0.0053\n",
      "Epoch 339 | Loss: 0.0059\n",
      "Epoch 340 | Loss: 0.0053\n",
      "Epoch 341 | Loss: 0.0051\n",
      "Epoch 342 | Loss: 0.0045\n",
      "Epoch 343 | Loss: 0.0052\n",
      "Epoch 344 | Loss: 0.0045\n",
      "Epoch 345 | Loss: 0.0043\n",
      "Epoch 346 | Loss: 0.0050\n",
      "Epoch 347 | Loss: 0.0046\n",
      "Epoch 348 | Loss: 0.0047\n",
      "Epoch 349 | Loss: 0.0054\n",
      "Epoch 350 | Loss: 0.0048\n",
      "Epoch 351 | Loss: 0.0041\n",
      "Epoch 352 | Loss: 0.0044\n",
      "Epoch 353 | Loss: 0.0051\n",
      "Epoch 354 | Loss: 0.0040\n",
      "Epoch 355 | Loss: 0.0044\n",
      "Epoch 356 | Loss: 0.0038\n",
      "Epoch 357 | Loss: 0.0052\n",
      "Epoch 358 | Loss: 0.0048\n",
      "Epoch 359 | Loss: 0.0042\n",
      "Epoch 360 | Loss: 0.0046\n",
      "Epoch 361 | Loss: 0.0052\n",
      "Epoch 362 | Loss: 0.0051\n",
      "Epoch 363 | Loss: 0.0056\n",
      "Epoch 364 | Loss: 0.0047\n",
      "Epoch 365 | Loss: 0.0073\n",
      "Epoch 366 | Loss: 0.0054\n",
      "Epoch 367 | Loss: 0.0048\n",
      "Epoch 368 | Loss: 0.0054\n",
      "Epoch 369 | Loss: 0.0048\n",
      "Epoch 370 | Loss: 0.0042\n",
      "Epoch 371 | Loss: 0.0049\n",
      "Epoch 372 | Loss: 0.0034\n",
      "Epoch 373 | Loss: 0.0047\n",
      "Epoch 374 | Loss: 0.0042\n",
      "Epoch 375 | Loss: 0.0055\n",
      "Epoch 376 | Loss: 0.0062\n",
      "Epoch 377 | Loss: 0.0047\n",
      "Epoch 378 | Loss: 0.0051\n",
      "Epoch 379 | Loss: 0.0046\n",
      "Epoch 380 | Loss: 0.0047\n",
      "Epoch 381 | Loss: 0.0053\n",
      "Epoch 382 | Loss: 0.0052\n",
      "Epoch 383 | Loss: 0.0047\n",
      "Epoch 384 | Loss: 0.0050\n",
      "Epoch 385 | Loss: 0.0051\n",
      "Epoch 386 | Loss: 0.0058\n",
      "Epoch 387 | Loss: 0.0042\n",
      "Epoch 388 | Loss: 0.0038\n",
      "Epoch 389 | Loss: 0.0047\n",
      "Epoch 390 | Loss: 0.0046\n",
      "Epoch 391 | Loss: 0.0039\n",
      "Epoch 392 | Loss: 0.0051\n",
      "Epoch 393 | Loss: 0.0040\n",
      "Epoch 394 | Loss: 0.0043\n",
      "Epoch 395 | Loss: 0.0055\n",
      "Epoch 396 | Loss: 0.0043\n",
      "Epoch 397 | Loss: 0.0049\n",
      "Epoch 398 | Loss: 0.0044\n",
      "Epoch 399 | Loss: 0.0052\n",
      "Epoch 400 | Loss: 0.0043\n",
      "Epoch 401 | Loss: 0.0047\n",
      "Epoch 402 | Loss: 0.0037\n",
      "Epoch 403 | Loss: 0.0045\n",
      "Epoch 404 | Loss: 0.0043\n",
      "Epoch 405 | Loss: 0.0040\n",
      "Epoch 406 | Loss: 0.0045\n",
      "Epoch 407 | Loss: 0.0037\n",
      "Epoch 408 | Loss: 0.0044\n",
      "Epoch 409 | Loss: 0.0048\n",
      "Epoch 410 | Loss: 0.0073\n",
      "Epoch 411 | Loss: 0.0040\n",
      "Epoch 412 | Loss: 0.0045\n",
      "Epoch 413 | Loss: 0.0038\n",
      "Epoch 414 | Loss: 0.0048\n",
      "Epoch 415 | Loss: 0.0036\n",
      "Epoch 416 | Loss: 0.0048\n",
      "Epoch 417 | Loss: 0.0046\n",
      "Epoch 418 | Loss: 0.0038\n",
      "Epoch 419 | Loss: 0.0038\n",
      "Epoch 420 | Loss: 0.0039\n",
      "Epoch 421 | Loss: 0.0045\n",
      "Epoch 422 | Loss: 0.0041\n",
      "Epoch 423 | Loss: 0.0035\n",
      "Epoch 424 | Loss: 0.0035\n",
      "Epoch 425 | Loss: 0.0044\n",
      "Epoch 426 | Loss: 0.0040\n",
      "Epoch 427 | Loss: 0.0043\n",
      "Epoch 428 | Loss: 0.0052\n",
      "Epoch 429 | Loss: 0.0047\n",
      "Epoch 430 | Loss: 0.0054\n",
      "Epoch 431 | Loss: 0.0051\n",
      "Epoch 432 | Loss: 0.0074\n",
      "Epoch 433 | Loss: 0.0037\n",
      "Epoch 434 | Loss: 0.0062\n",
      "Epoch 435 | Loss: 0.0054\n",
      "Epoch 436 | Loss: 0.0045\n",
      "Epoch 437 | Loss: 0.0048\n",
      "Epoch 438 | Loss: 0.0046\n",
      "Epoch 439 | Loss: 0.0038\n",
      "Epoch 440 | Loss: 0.0045\n",
      "Epoch 441 | Loss: 0.0054\n",
      "Epoch 442 | Loss: 0.0049\n",
      "Epoch 443 | Loss: 0.0048\n",
      "Epoch 444 | Loss: 0.0041\n",
      "Epoch 445 | Loss: 0.0044\n",
      "Epoch 446 | Loss: 0.0038\n",
      "Epoch 447 | Loss: 0.0051\n",
      "Epoch 448 | Loss: 0.0042\n",
      "Epoch 449 | Loss: 0.0043\n",
      "Epoch 450 | Loss: 0.0037\n",
      "Epoch 451 | Loss: 0.0036\n",
      "Epoch 452 | Loss: 0.0038\n",
      "Epoch 453 | Loss: 0.0039\n",
      "Epoch 454 | Loss: 0.0037\n",
      "Epoch 455 | Loss: 0.0035\n",
      "Epoch 456 | Loss: 0.0037\n",
      "Epoch 457 | Loss: 0.0037\n",
      "Epoch 458 | Loss: 0.0038\n",
      "Epoch 459 | Loss: 0.0048\n",
      "Epoch 460 | Loss: 0.0037\n",
      "Epoch 461 | Loss: 0.0040\n",
      "Epoch 462 | Loss: 0.0044\n",
      "Epoch 463 | Loss: 0.0045\n",
      "Epoch 464 | Loss: 0.0048\n",
      "Epoch 465 | Loss: 0.0040\n",
      "Epoch 466 | Loss: 0.0041\n",
      "Epoch 467 | Loss: 0.0040\n",
      "Epoch 468 | Loss: 0.0052\n",
      "Epoch 469 | Loss: 0.0056\n",
      "Epoch 470 | Loss: 0.0061\n",
      "Epoch 471 | Loss: 0.0218\n",
      "Epoch 472 | Loss: 0.0312\n",
      "Epoch 473 | Loss: 0.0343\n",
      "Epoch 474 | Loss: 0.0570\n",
      "Epoch 475 | Loss: 0.0695\n",
      "Epoch 476 | Loss: 0.1488\n",
      "Epoch 477 | Loss: 0.1276\n",
      "Epoch 478 | Loss: 0.1041\n",
      "Epoch 479 | Loss: 0.0750\n",
      "Epoch 480 | Loss: 0.0613\n",
      "Epoch 481 | Loss: 0.0419\n",
      "Epoch 482 | Loss: 0.0288\n",
      "Epoch 483 | Loss: 0.0225\n",
      "Epoch 484 | Loss: 0.0171\n",
      "Epoch 485 | Loss: 0.0155\n",
      "Epoch 486 | Loss: 0.0128\n",
      "Epoch 487 | Loss: 0.0118\n",
      "Epoch 488 | Loss: 0.0113\n",
      "Epoch 489 | Loss: 0.0104\n",
      "Epoch 490 | Loss: 0.0110\n",
      "Epoch 491 | Loss: 0.0077\n",
      "Epoch 492 | Loss: 0.0088\n",
      "Epoch 493 | Loss: 0.0075\n",
      "Epoch 494 | Loss: 0.0071\n",
      "Epoch 495 | Loss: 0.0062\n",
      "Epoch 496 | Loss: 0.0062\n",
      "Epoch 497 | Loss: 0.0064\n",
      "Epoch 498 | Loss: 0.0059\n",
      "Epoch 499 | Loss: 0.0049\n",
      "Epoch 500 | Loss: 0.0050\n",
      "Epoch 501 | Loss: 0.0054\n",
      "Epoch 502 | Loss: 0.0050\n",
      "Epoch 503 | Loss: 0.0043\n",
      "Epoch 504 | Loss: 0.0043\n",
      "Epoch 505 | Loss: 0.0046\n",
      "Epoch 506 | Loss: 0.0048\n",
      "Epoch 507 | Loss: 0.0051\n",
      "Epoch 508 | Loss: 0.0042\n",
      "Epoch 509 | Loss: 0.0045\n",
      "Epoch 510 | Loss: 0.0035\n",
      "Epoch 511 | Loss: 0.0042\n",
      "Epoch 512 | Loss: 0.0041\n",
      "Epoch 513 | Loss: 0.0052\n",
      "Epoch 514 | Loss: 0.0041\n",
      "Epoch 515 | Loss: 0.0044\n",
      "Epoch 516 | Loss: 0.0039\n",
      "Epoch 517 | Loss: 0.0042\n",
      "Epoch 518 | Loss: 0.0042\n",
      "Epoch 519 | Loss: 0.0038\n",
      "Epoch 520 | Loss: 0.0042\n",
      "Epoch 521 | Loss: 0.0040\n",
      "Epoch 522 | Loss: 0.0053\n",
      "Epoch 523 | Loss: 0.0042\n",
      "Epoch 524 | Loss: 0.0042\n",
      "Epoch 525 | Loss: 0.0038\n",
      "Epoch 526 | Loss: 0.0043\n",
      "Epoch 527 | Loss: 0.0043\n",
      "Epoch 528 | Loss: 0.0046\n",
      "Epoch 529 | Loss: 0.0048\n",
      "Epoch 530 | Loss: 0.0051\n",
      "Epoch 531 | Loss: 0.0055\n",
      "Epoch 532 | Loss: 0.0044\n",
      "Epoch 533 | Loss: 0.0041\n",
      "Epoch 534 | Loss: 0.0041\n",
      "Epoch 535 | Loss: 0.0061\n",
      "Epoch 536 | Loss: 0.0043\n",
      "Epoch 537 | Loss: 0.0048\n",
      "Epoch 538 | Loss: 0.0041\n",
      "Epoch 539 | Loss: 0.0056\n",
      "Epoch 540 | Loss: 0.0042\n",
      "Epoch 541 | Loss: 0.0051\n",
      "Epoch 542 | Loss: 0.0061\n",
      "Epoch 543 | Loss: 0.0042\n",
      "Epoch 544 | Loss: 0.0049\n",
      "Epoch 545 | Loss: 0.0081\n",
      "Epoch 546 | Loss: 0.0055\n",
      "Epoch 547 | Loss: 0.0098\n",
      "Epoch 548 | Loss: 0.0153\n",
      "Epoch 549 | Loss: 0.0650\n",
      "Epoch 550 | Loss: 0.0517\n",
      "Epoch 551 | Loss: 0.0435\n",
      "Epoch 552 | Loss: 0.0347\n",
      "Epoch 553 | Loss: 0.0272\n",
      "Epoch 554 | Loss: 0.0256\n",
      "Epoch 555 | Loss: 0.0202\n",
      "Epoch 556 | Loss: 0.0149\n",
      "Epoch 557 | Loss: 0.0110\n",
      "Epoch 558 | Loss: 0.0119\n",
      "Epoch 559 | Loss: 0.0089\n",
      "Epoch 560 | Loss: 0.0137\n",
      "Epoch 561 | Loss: 0.0112\n",
      "Epoch 562 | Loss: 0.0121\n",
      "Epoch 563 | Loss: 0.0086\n",
      "Epoch 564 | Loss: 0.0131\n",
      "Epoch 565 | Loss: 0.0085\n",
      "Epoch 566 | Loss: 0.0084\n",
      "Epoch 567 | Loss: 0.0099\n",
      "Epoch 568 | Loss: 0.0065\n",
      "Epoch 569 | Loss: 0.0069\n",
      "Epoch 570 | Loss: 0.0092\n",
      "Epoch 571 | Loss: 0.0087\n",
      "Epoch 572 | Loss: 0.0068\n",
      "Epoch 573 | Loss: 0.0060\n",
      "Epoch 574 | Loss: 0.0067\n",
      "Epoch 575 | Loss: 0.0053\n",
      "Epoch 576 | Loss: 0.0046\n",
      "Epoch 577 | Loss: 0.0053\n",
      "Epoch 578 | Loss: 0.0044\n",
      "Epoch 579 | Loss: 0.0049\n",
      "Epoch 580 | Loss: 0.0043\n",
      "Epoch 581 | Loss: 0.0042\n",
      "Epoch 582 | Loss: 0.0048\n",
      "Epoch 583 | Loss: 0.0044\n",
      "Epoch 584 | Loss: 0.0057\n",
      "Epoch 585 | Loss: 0.0076\n",
      "Epoch 586 | Loss: 0.0050\n",
      "Epoch 587 | Loss: 0.0047\n",
      "Epoch 588 | Loss: 0.0079\n",
      "Epoch 589 | Loss: 0.0044\n",
      "Epoch 590 | Loss: 0.0047\n",
      "Epoch 591 | Loss: 0.0039\n",
      "Epoch 592 | Loss: 0.0043\n",
      "Epoch 593 | Loss: 0.0051\n",
      "Epoch 594 | Loss: 0.0037\n",
      "Epoch 595 | Loss: 0.0045\n",
      "Epoch 596 | Loss: 0.0038\n",
      "Epoch 597 | Loss: 0.0039\n",
      "Epoch 598 | Loss: 0.0042\n",
      "Epoch 599 | Loss: 0.0049\n",
      "Epoch 600 | Loss: 0.0034\n",
      "Epoch 601 | Loss: 0.0051\n",
      "Epoch 602 | Loss: 0.0036\n",
      "Epoch 603 | Loss: 0.0070\n",
      "Epoch 604 | Loss: 0.0054\n",
      "Epoch 605 | Loss: 0.0039\n",
      "Epoch 606 | Loss: 0.0046\n",
      "Epoch 607 | Loss: 0.0051\n",
      "Epoch 608 | Loss: 0.0039\n",
      "Epoch 609 | Loss: 0.0042\n",
      "Epoch 610 | Loss: 0.0030\n",
      "Epoch 611 | Loss: 0.0042\n",
      "Epoch 612 | Loss: 0.0044\n",
      "Epoch 613 | Loss: 0.0046\n",
      "Epoch 614 | Loss: 0.0041\n",
      "Epoch 615 | Loss: 0.0031\n",
      "Epoch 616 | Loss: 0.0037\n",
      "Epoch 617 | Loss: 0.0037\n",
      "Epoch 618 | Loss: 0.0030\n",
      "Epoch 619 | Loss: 0.0041\n",
      "Epoch 620 | Loss: 0.0043\n",
      "Epoch 621 | Loss: 0.0030\n",
      "Epoch 622 | Loss: 0.0040\n",
      "Epoch 623 | Loss: 0.0037\n",
      "Epoch 624 | Loss: 0.0034\n",
      "Epoch 625 | Loss: 0.0035\n",
      "Epoch 626 | Loss: 0.0035\n",
      "Epoch 627 | Loss: 0.0036\n",
      "Epoch 628 | Loss: 0.0034\n",
      "Epoch 629 | Loss: 0.0046\n",
      "Epoch 630 | Loss: 0.0031\n",
      "Epoch 631 | Loss: 0.0036\n",
      "Epoch 632 | Loss: 0.0039\n",
      "Epoch 633 | Loss: 0.0033\n",
      "Epoch 634 | Loss: 0.0035\n",
      "Epoch 635 | Loss: 0.0032\n",
      "Epoch 636 | Loss: 0.0032\n",
      "Epoch 637 | Loss: 0.0038\n",
      "Epoch 638 | Loss: 0.0036\n",
      "Epoch 639 | Loss: 0.0034\n",
      "Epoch 640 | Loss: 0.0039\n",
      "Epoch 641 | Loss: 0.0037\n",
      "Epoch 642 | Loss: 0.0041\n",
      "Epoch 643 | Loss: 0.0034\n",
      "Epoch 644 | Loss: 0.0032\n",
      "Epoch 645 | Loss: 0.0030\n",
      "Epoch 646 | Loss: 0.0040\n",
      "Epoch 647 | Loss: 0.0044\n",
      "Epoch 648 | Loss: 0.0056\n",
      "Epoch 649 | Loss: 0.0032\n",
      "Epoch 650 | Loss: 0.0035\n",
      "Epoch 651 | Loss: 0.0039\n",
      "Epoch 652 | Loss: 0.0038\n",
      "Epoch 653 | Loss: 0.0035\n",
      "Epoch 654 | Loss: 0.0037\n",
      "Epoch 655 | Loss: 0.0032\n",
      "Epoch 656 | Loss: 0.0042\n",
      "Epoch 657 | Loss: 0.0050\n",
      "Epoch 658 | Loss: 0.0035\n",
      "Epoch 659 | Loss: 0.0049\n",
      "Epoch 660 | Loss: 0.0048\n",
      "Epoch 661 | Loss: 0.0038\n",
      "Epoch 662 | Loss: 0.0037\n",
      "Epoch 663 | Loss: 0.0037\n",
      "Epoch 664 | Loss: 0.0047\n",
      "Epoch 665 | Loss: 0.0041\n",
      "Epoch 666 | Loss: 0.0037\n",
      "Epoch 667 | Loss: 0.0037\n",
      "Epoch 668 | Loss: 0.0033\n",
      "Epoch 669 | Loss: 0.0034\n",
      "Epoch 670 | Loss: 0.0036\n",
      "Epoch 671 | Loss: 0.0027\n",
      "Epoch 672 | Loss: 0.0037\n",
      "Epoch 673 | Loss: 0.0035\n",
      "Epoch 674 | Loss: 0.0030\n",
      "Epoch 675 | Loss: 0.0039\n",
      "Epoch 676 | Loss: 0.0033\n",
      "Epoch 677 | Loss: 0.0035\n",
      "Epoch 678 | Loss: 0.0026\n",
      "Epoch 679 | Loss: 0.0040\n",
      "Epoch 680 | Loss: 0.0034\n",
      "Epoch 681 | Loss: 0.0051\n",
      "Epoch 682 | Loss: 0.0056\n",
      "Epoch 683 | Loss: 0.0042\n",
      "Epoch 684 | Loss: 0.0057\n",
      "Epoch 685 | Loss: 0.0057\n",
      "Epoch 686 | Loss: 0.0054\n",
      "Epoch 687 | Loss: 0.0059\n",
      "Epoch 688 | Loss: 0.0083\n",
      "Epoch 689 | Loss: 0.0268\n",
      "Epoch 690 | Loss: 0.0445\n",
      "Epoch 691 | Loss: 0.1168\n",
      "Epoch 692 | Loss: 0.1004\n",
      "Epoch 693 | Loss: 0.1012\n",
      "Epoch 694 | Loss: 0.0676\n",
      "Epoch 695 | Loss: 0.0507\n",
      "Epoch 696 | Loss: 0.0361\n",
      "Epoch 697 | Loss: 0.0285\n",
      "Epoch 698 | Loss: 0.0208\n",
      "Epoch 699 | Loss: 0.0130\n",
      "Epoch 700 | Loss: 0.0112\n",
      "Epoch 701 | Loss: 0.0097\n",
      "Epoch 702 | Loss: 0.0100\n",
      "Epoch 703 | Loss: 0.0088\n",
      "Epoch 704 | Loss: 0.0075\n",
      "Epoch 705 | Loss: 0.0062\n",
      "Epoch 706 | Loss: 0.0070\n",
      "Epoch 707 | Loss: 0.0066\n",
      "Epoch 708 | Loss: 0.0067\n",
      "Epoch 709 | Loss: 0.0082\n",
      "Epoch 710 | Loss: 0.0066\n",
      "Epoch 711 | Loss: 0.0059\n",
      "Epoch 712 | Loss: 0.0064\n",
      "Epoch 713 | Loss: 0.0075\n",
      "Epoch 714 | Loss: 0.0104\n",
      "Epoch 715 | Loss: 0.0179\n",
      "Epoch 716 | Loss: 0.0246\n",
      "Epoch 717 | Loss: 0.0614\n",
      "Epoch 718 | Loss: 0.0496\n",
      "Epoch 719 | Loss: 0.0440\n",
      "Epoch 720 | Loss: 0.0276\n",
      "Epoch 721 | Loss: 0.0237\n",
      "Epoch 722 | Loss: 0.0254\n",
      "Epoch 723 | Loss: 0.0184\n",
      "Epoch 724 | Loss: 0.0160\n",
      "Epoch 725 | Loss: 0.0146\n",
      "Epoch 726 | Loss: 0.0127\n",
      "Epoch 727 | Loss: 0.0142\n",
      "Epoch 728 | Loss: 0.0083\n",
      "Epoch 729 | Loss: 0.0091\n",
      "Epoch 730 | Loss: 0.0080\n",
      "Epoch 731 | Loss: 0.0063\n",
      "Epoch 732 | Loss: 0.0059\n",
      "Epoch 733 | Loss: 0.0061\n",
      "Epoch 734 | Loss: 0.0050\n",
      "Epoch 735 | Loss: 0.0049\n",
      "Epoch 736 | Loss: 0.0048\n",
      "Epoch 737 | Loss: 0.0054\n",
      "Epoch 738 | Loss: 0.0048\n",
      "Epoch 739 | Loss: 0.0043\n",
      "Epoch 740 | Loss: 0.0044\n",
      "Epoch 741 | Loss: 0.0044\n",
      "Epoch 742 | Loss: 0.0041\n",
      "Epoch 743 | Loss: 0.0047\n",
      "Epoch 744 | Loss: 0.0040\n",
      "Epoch 745 | Loss: 0.0036\n",
      "Epoch 746 | Loss: 0.0043\n",
      "Epoch 747 | Loss: 0.0037\n",
      "Epoch 748 | Loss: 0.0032\n",
      "Epoch 749 | Loss: 0.0036\n",
      "Epoch 750 | Loss: 0.0039\n",
      "Epoch 751 | Loss: 0.0038\n",
      "Epoch 752 | Loss: 0.0032\n",
      "Epoch 753 | Loss: 0.0040\n",
      "Epoch 754 | Loss: 0.0032\n",
      "Epoch 755 | Loss: 0.0037\n",
      "Epoch 756 | Loss: 0.0044\n",
      "Epoch 757 | Loss: 0.0035\n",
      "Epoch 758 | Loss: 0.0031\n",
      "Epoch 759 | Loss: 0.0034\n",
      "Epoch 760 | Loss: 0.0050\n",
      "Epoch 761 | Loss: 0.0039\n",
      "Epoch 762 | Loss: 0.0043\n",
      "Epoch 763 | Loss: 0.0040\n",
      "Epoch 764 | Loss: 0.0037\n",
      "Epoch 765 | Loss: 0.0036\n",
      "Epoch 766 | Loss: 0.0040\n",
      "Epoch 767 | Loss: 0.0036\n",
      "Epoch 768 | Loss: 0.0047\n",
      "Epoch 769 | Loss: 0.0040\n",
      "Epoch 770 | Loss: 0.0041\n",
      "Epoch 771 | Loss: 0.0037\n",
      "Epoch 772 | Loss: 0.0036\n",
      "Epoch 773 | Loss: 0.0039\n",
      "Epoch 774 | Loss: 0.0042\n",
      "Epoch 775 | Loss: 0.0034\n",
      "Epoch 776 | Loss: 0.0042\n",
      "Epoch 777 | Loss: 0.0037\n",
      "Epoch 778 | Loss: 0.0036\n",
      "Epoch 779 | Loss: 0.0036\n",
      "Epoch 780 | Loss: 0.0040\n",
      "Epoch 781 | Loss: 0.0040\n",
      "Epoch 782 | Loss: 0.0038\n",
      "Epoch 783 | Loss: 0.0036\n",
      "Epoch 784 | Loss: 0.0033\n",
      "Epoch 785 | Loss: 0.0044\n",
      "Epoch 786 | Loss: 0.0034\n",
      "Epoch 787 | Loss: 0.0044\n",
      "Epoch 788 | Loss: 0.0043\n",
      "Epoch 789 | Loss: 0.0032\n",
      "Epoch 790 | Loss: 0.0038\n",
      "Epoch 791 | Loss: 0.0044\n",
      "Epoch 792 | Loss: 0.0046\n",
      "Epoch 793 | Loss: 0.0039\n",
      "Epoch 794 | Loss: 0.0059\n",
      "Epoch 795 | Loss: 0.0059\n",
      "Epoch 796 | Loss: 0.0073\n",
      "Epoch 797 | Loss: 0.0087\n",
      "Epoch 798 | Loss: 0.0070\n",
      "Epoch 799 | Loss: 0.0100\n",
      "Epoch 800 | Loss: 0.0092\n",
      "Epoch 801 | Loss: 0.0050\n",
      "Epoch 802 | Loss: 0.0045\n",
      "Epoch 803 | Loss: 0.0051\n",
      "Epoch 804 | Loss: 0.0043\n",
      "Epoch 805 | Loss: 0.0069\n",
      "Epoch 806 | Loss: 0.0071\n",
      "Epoch 807 | Loss: 0.0127\n",
      "Epoch 808 | Loss: 0.0245\n",
      "Epoch 809 | Loss: 0.0453\n",
      "Epoch 810 | Loss: 0.0630\n",
      "Epoch 811 | Loss: 0.0780\n",
      "Epoch 812 | Loss: 0.1001\n",
      "Epoch 813 | Loss: 0.0765\n",
      "Epoch 814 | Loss: 0.0621\n",
      "Epoch 815 | Loss: 0.0392\n",
      "Epoch 816 | Loss: 0.0318\n",
      "Epoch 817 | Loss: 0.0208\n",
      "Epoch 818 | Loss: 0.0192\n",
      "Epoch 819 | Loss: 0.0162\n",
      "Epoch 820 | Loss: 0.0166\n",
      "Epoch 821 | Loss: 0.0157\n",
      "Epoch 822 | Loss: 0.0121\n",
      "Epoch 823 | Loss: 0.0105\n",
      "Epoch 824 | Loss: 0.0104\n",
      "Epoch 825 | Loss: 0.0110\n",
      "Epoch 826 | Loss: 0.0094\n",
      "Epoch 827 | Loss: 0.0089\n",
      "Epoch 828 | Loss: 0.0061\n",
      "Epoch 829 | Loss: 0.0086\n",
      "Epoch 830 | Loss: 0.0055\n",
      "Epoch 831 | Loss: 0.0055\n",
      "Epoch 832 | Loss: 0.0049\n",
      "Epoch 833 | Loss: 0.0052\n",
      "Epoch 834 | Loss: 0.0044\n",
      "Epoch 835 | Loss: 0.0045\n",
      "Epoch 836 | Loss: 0.0041\n",
      "Epoch 837 | Loss: 0.0044\n",
      "Epoch 838 | Loss: 0.0038\n",
      "Epoch 839 | Loss: 0.0043\n",
      "Epoch 840 | Loss: 0.0036\n",
      "Epoch 841 | Loss: 0.0038\n",
      "Epoch 842 | Loss: 0.0039\n",
      "Epoch 843 | Loss: 0.0040\n",
      "Epoch 844 | Loss: 0.0036\n",
      "Epoch 845 | Loss: 0.0041\n",
      "Epoch 846 | Loss: 0.0038\n",
      "Epoch 847 | Loss: 0.0037\n",
      "Epoch 848 | Loss: 0.0039\n",
      "Epoch 849 | Loss: 0.0049\n",
      "Epoch 850 | Loss: 0.0036\n",
      "Epoch 851 | Loss: 0.0052\n",
      "Epoch 852 | Loss: 0.0037\n",
      "Epoch 853 | Loss: 0.0040\n",
      "Epoch 854 | Loss: 0.0034\n",
      "Epoch 855 | Loss: 0.0036\n",
      "Epoch 856 | Loss: 0.0035\n",
      "Epoch 857 | Loss: 0.0028\n",
      "Epoch 858 | Loss: 0.0036\n",
      "Epoch 859 | Loss: 0.0037\n",
      "Epoch 860 | Loss: 0.0037\n",
      "Epoch 861 | Loss: 0.0033\n",
      "Epoch 862 | Loss: 0.0036\n",
      "Epoch 863 | Loss: 0.0040\n",
      "Epoch 864 | Loss: 0.0035\n",
      "Epoch 865 | Loss: 0.0044\n",
      "Epoch 866 | Loss: 0.0044\n",
      "Epoch 867 | Loss: 0.0033\n",
      "Epoch 868 | Loss: 0.0035\n",
      "Epoch 869 | Loss: 0.0037\n",
      "Epoch 870 | Loss: 0.0030\n",
      "Epoch 871 | Loss: 0.0034\n",
      "Epoch 872 | Loss: 0.0036\n",
      "Epoch 873 | Loss: 0.0037\n",
      "Epoch 874 | Loss: 0.0034\n",
      "Epoch 875 | Loss: 0.0038\n",
      "Epoch 876 | Loss: 0.0041\n",
      "Epoch 877 | Loss: 0.0039\n",
      "Epoch 878 | Loss: 0.0031\n",
      "Epoch 879 | Loss: 0.0051\n",
      "Epoch 880 | Loss: 0.0040\n",
      "Epoch 881 | Loss: 0.0069\n",
      "Epoch 882 | Loss: 0.0045\n",
      "Epoch 883 | Loss: 0.0040\n",
      "Epoch 884 | Loss: 0.0036\n",
      "Epoch 885 | Loss: 0.0056\n",
      "Epoch 886 | Loss: 0.0038\n",
      "Epoch 887 | Loss: 0.0039\n",
      "Epoch 888 | Loss: 0.0038\n",
      "Epoch 889 | Loss: 0.0040\n",
      "Epoch 890 | Loss: 0.0032\n",
      "Epoch 891 | Loss: 0.0038\n",
      "Epoch 892 | Loss: 0.0045\n",
      "Epoch 893 | Loss: 0.0038\n",
      "Epoch 894 | Loss: 0.0035\n",
      "Epoch 895 | Loss: 0.0035\n",
      "Epoch 896 | Loss: 0.0037\n",
      "Epoch 897 | Loss: 0.0040\n",
      "Epoch 898 | Loss: 0.0031\n",
      "Epoch 899 | Loss: 0.0029\n",
      "Epoch 900 | Loss: 0.0026\n",
      "Epoch 901 | Loss: 0.0034\n",
      "Epoch 902 | Loss: 0.0034\n",
      "Epoch 903 | Loss: 0.0037\n",
      "Epoch 904 | Loss: 0.0036\n",
      "Epoch 905 | Loss: 0.0041\n",
      "Epoch 906 | Loss: 0.0043\n",
      "Epoch 907 | Loss: 0.0030\n",
      "Epoch 908 | Loss: 0.0034\n",
      "Epoch 909 | Loss: 0.0047\n",
      "Epoch 910 | Loss: 0.0092\n",
      "Epoch 911 | Loss: 0.0040\n",
      "Epoch 912 | Loss: 0.0040\n",
      "Epoch 913 | Loss: 0.0033\n",
      "Epoch 914 | Loss: 0.0035\n",
      "Epoch 915 | Loss: 0.0034\n",
      "Epoch 916 | Loss: 0.0032\n",
      "Epoch 917 | Loss: 0.0032\n",
      "Epoch 918 | Loss: 0.0033\n",
      "Epoch 919 | Loss: 0.0037\n",
      "Epoch 920 | Loss: 0.0036\n",
      "Epoch 921 | Loss: 0.0035\n",
      "Epoch 922 | Loss: 0.0032\n",
      "Epoch 923 | Loss: 0.0031\n",
      "Epoch 924 | Loss: 0.0033\n",
      "Epoch 925 | Loss: 0.0032\n",
      "Epoch 926 | Loss: 0.0033\n",
      "Epoch 927 | Loss: 0.0038\n",
      "Epoch 928 | Loss: 0.0036\n",
      "Epoch 929 | Loss: 0.0028\n",
      "Epoch 930 | Loss: 0.0032\n",
      "Epoch 931 | Loss: 0.0035\n",
      "Epoch 932 | Loss: 0.0043\n",
      "Epoch 933 | Loss: 0.0030\n",
      "Epoch 934 | Loss: 0.0029\n",
      "Epoch 935 | Loss: 0.0039\n",
      "Epoch 936 | Loss: 0.0036\n",
      "Epoch 937 | Loss: 0.0034\n",
      "Epoch 938 | Loss: 0.0047\n",
      "Epoch 939 | Loss: 0.0035\n",
      "Epoch 940 | Loss: 0.0040\n",
      "Epoch 941 | Loss: 0.0034\n",
      "Epoch 942 | Loss: 0.0042\n",
      "Epoch 943 | Loss: 0.0046\n",
      "Epoch 944 | Loss: 0.0044\n",
      "Epoch 945 | Loss: 0.0037\n",
      "Epoch 946 | Loss: 0.0033\n",
      "Epoch 947 | Loss: 0.0027\n",
      "Epoch 948 | Loss: 0.0034\n",
      "Epoch 949 | Loss: 0.0032\n",
      "Epoch 950 | Loss: 0.0055\n",
      "Epoch 951 | Loss: 0.0038\n",
      "Epoch 952 | Loss: 0.0040\n",
      "Epoch 953 | Loss: 0.0031\n",
      "Epoch 954 | Loss: 0.0039\n",
      "Epoch 955 | Loss: 0.0030\n",
      "Epoch 956 | Loss: 0.0030\n",
      "Epoch 957 | Loss: 0.0030\n",
      "Epoch 958 | Loss: 0.0028\n",
      "Epoch 959 | Loss: 0.0025\n",
      "Epoch 960 | Loss: 0.0033\n",
      "Epoch 961 | Loss: 0.0034\n",
      "Epoch 962 | Loss: 0.0031\n",
      "Epoch 963 | Loss: 0.0031\n",
      "Epoch 964 | Loss: 0.0035\n",
      "Epoch 965 | Loss: 0.0029\n",
      "Epoch 966 | Loss: 0.0032\n",
      "Epoch 967 | Loss: 0.0036\n",
      "Epoch 968 | Loss: 0.0031\n",
      "Epoch 969 | Loss: 0.0036\n",
      "Epoch 970 | Loss: 0.0032\n",
      "Epoch 971 | Loss: 0.0039\n",
      "Epoch 972 | Loss: 0.0034\n",
      "Epoch 973 | Loss: 0.0037\n",
      "Epoch 974 | Loss: 0.0036\n",
      "Epoch 975 | Loss: 0.0033\n",
      "Epoch 976 | Loss: 0.0030\n",
      "Epoch 977 | Loss: 0.0036\n",
      "Epoch 978 | Loss: 0.0045\n",
      "Epoch 979 | Loss: 0.0029\n",
      "Epoch 980 | Loss: 0.0041\n",
      "Epoch 981 | Loss: 0.0030\n",
      "Epoch 982 | Loss: 0.0041\n",
      "Epoch 983 | Loss: 0.0034\n",
      "Epoch 984 | Loss: 0.0036\n",
      "Epoch 985 | Loss: 0.0040\n",
      "Epoch 986 | Loss: 0.0028\n",
      "Epoch 987 | Loss: 0.0039\n",
      "Epoch 988 | Loss: 0.0037\n",
      "Epoch 989 | Loss: 0.0031\n",
      "Epoch 990 | Loss: 0.0032\n",
      "Epoch 991 | Loss: 0.0036\n",
      "Epoch 992 | Loss: 0.0035\n",
      "Epoch 993 | Loss: 0.0034\n",
      "Epoch 994 | Loss: 0.0035\n",
      "Epoch 995 | Loss: 0.0032\n",
      "Epoch 996 | Loss: 0.0037\n",
      "Epoch 997 | Loss: 0.0033\n",
      "Epoch 998 | Loss: 0.0036\n",
      "Epoch 999 | Loss: 0.0038\n",
      "Epoch 1000 | Loss: 0.0027\n",
      "Epoch 1001 | Loss: 0.0034\n",
      "Epoch 1002 | Loss: 0.0028\n",
      "Epoch 1003 | Loss: 0.0034\n",
      "Epoch 1004 | Loss: 0.0029\n",
      "Epoch 1005 | Loss: 0.0026\n",
      "Epoch 1006 | Loss: 0.0043\n",
      "Epoch 1007 | Loss: 0.0024\n",
      "Epoch 1008 | Loss: 0.0039\n",
      "Epoch 1009 | Loss: 0.0036\n",
      "Epoch 1010 | Loss: 0.0032\n",
      "Epoch 1011 | Loss: 0.0031\n",
      "Epoch 1012 | Loss: 0.0028\n",
      "Epoch 1013 | Loss: 0.0032\n",
      "Epoch 1014 | Loss: 0.0029\n",
      "Epoch 1015 | Loss: 0.0030\n",
      "Epoch 1016 | Loss: 0.0035\n",
      "Epoch 1017 | Loss: 0.0036\n",
      "Epoch 1018 | Loss: 0.0035\n",
      "Epoch 1019 | Loss: 0.0030\n",
      "Epoch 1020 | Loss: 0.0037\n",
      "Epoch 1021 | Loss: 0.0033\n",
      "Epoch 1022 | Loss: 0.0030\n",
      "Epoch 1023 | Loss: 0.0031\n",
      "Epoch 1024 | Loss: 0.0036\n",
      "Epoch 1025 | Loss: 0.0032\n",
      "Epoch 1026 | Loss: 0.0028\n",
      "Epoch 1027 | Loss: 0.0030\n",
      "Epoch 1028 | Loss: 0.0028\n",
      "Epoch 1029 | Loss: 0.0033\n",
      "Epoch 1030 | Loss: 0.0029\n",
      "Epoch 1031 | Loss: 0.0031\n",
      "Epoch 1032 | Loss: 0.0034\n",
      "Epoch 1033 | Loss: 0.0030\n",
      "Epoch 1034 | Loss: 0.0039\n",
      "Epoch 1035 | Loss: 0.0027\n",
      "Epoch 1036 | Loss: 0.0040\n",
      "Epoch 1037 | Loss: 0.0030\n",
      "Epoch 1038 | Loss: 0.0027\n",
      "Epoch 1039 | Loss: 0.0029\n",
      "Epoch 1040 | Loss: 0.0031\n",
      "Epoch 1041 | Loss: 0.0027\n",
      "Epoch 1042 | Loss: 0.0030\n",
      "Epoch 1043 | Loss: 0.0033\n",
      "Epoch 1044 | Loss: 0.0033\n",
      "Epoch 1045 | Loss: 0.0034\n",
      "Epoch 1046 | Loss: 0.0046\n",
      "Epoch 1047 | Loss: 0.0041\n",
      "Epoch 1048 | Loss: 0.0029\n",
      "Epoch 1049 | Loss: 0.0032\n",
      "Epoch 1050 | Loss: 0.0034\n",
      "Epoch 1051 | Loss: 0.0033\n",
      "Epoch 1052 | Loss: 0.0034\n",
      "Epoch 1053 | Loss: 0.0037\n",
      "Epoch 1054 | Loss: 0.0034\n",
      "Epoch 1055 | Loss: 0.0042\n",
      "Epoch 1056 | Loss: 0.0040\n",
      "Epoch 1057 | Loss: 0.0033\n",
      "Epoch 1058 | Loss: 0.0027\n",
      "Epoch 1059 | Loss: 0.0033\n",
      "Epoch 1060 | Loss: 0.0029\n",
      "Epoch 1061 | Loss: 0.0028\n",
      "Epoch 1062 | Loss: 0.0035\n",
      "Epoch 1063 | Loss: 0.0033\n",
      "Epoch 1064 | Loss: 0.0037\n",
      "Epoch 1065 | Loss: 0.0031\n",
      "Epoch 1066 | Loss: 0.0027\n",
      "Epoch 1067 | Loss: 0.0032\n",
      "Epoch 1068 | Loss: 0.0033\n",
      "Epoch 1069 | Loss: 0.0031\n",
      "Epoch 1070 | Loss: 0.0032\n",
      "Epoch 1071 | Loss: 0.0032\n",
      "Epoch 1072 | Loss: 0.0029\n",
      "Epoch 1073 | Loss: 0.0031\n",
      "Epoch 1074 | Loss: 0.0032\n",
      "Epoch 1075 | Loss: 0.0029\n",
      "Epoch 1076 | Loss: 0.0035\n",
      "Epoch 1077 | Loss: 0.0030\n",
      "Epoch 1078 | Loss: 0.0032\n",
      "Epoch 1079 | Loss: 0.0035\n",
      "Epoch 1080 | Loss: 0.0028\n",
      "Epoch 1081 | Loss: 0.0032\n",
      "Epoch 1082 | Loss: 0.0035\n",
      "Epoch 1083 | Loss: 0.0033\n",
      "Epoch 1084 | Loss: 0.0029\n",
      "Epoch 1085 | Loss: 0.0042\n",
      "Epoch 1086 | Loss: 0.0030\n",
      "Epoch 1087 | Loss: 0.0042\n",
      "Epoch 1088 | Loss: 0.0042\n",
      "Epoch 1089 | Loss: 0.0036\n",
      "Epoch 1090 | Loss: 0.0031\n",
      "Epoch 1091 | Loss: 0.0036\n",
      "Epoch 1092 | Loss: 0.0033\n",
      "Epoch 1093 | Loss: 0.0031\n",
      "Epoch 1094 | Loss: 0.0039\n",
      "Epoch 1095 | Loss: 0.0038\n",
      "Epoch 1096 | Loss: 0.0032\n",
      "Epoch 1097 | Loss: 0.0053\n",
      "Epoch 1098 | Loss: 0.0039\n",
      "Epoch 1099 | Loss: 0.0091\n",
      "Epoch 1100 | Loss: 0.0069\n",
      "Epoch 1101 | Loss: 0.0095\n",
      "Epoch 1102 | Loss: 0.0240\n",
      "Epoch 1103 | Loss: 0.0545\n",
      "Epoch 1104 | Loss: 0.0714\n",
      "Epoch 1105 | Loss: 0.1036\n",
      "Epoch 1106 | Loss: 0.1143\n",
      "Epoch 1107 | Loss: 0.1177\n",
      "Epoch 1108 | Loss: 0.0939\n",
      "Epoch 1109 | Loss: 0.0760\n",
      "Epoch 1110 | Loss: 0.0728\n",
      "Epoch 1111 | Loss: 0.0536\n",
      "Epoch 1112 | Loss: 0.0470\n",
      "Epoch 1113 | Loss: 0.0325\n",
      "Epoch 1114 | Loss: 0.0320\n",
      "Epoch 1115 | Loss: 0.0343\n",
      "Epoch 1116 | Loss: 0.0239\n",
      "Epoch 1117 | Loss: 0.0185\n",
      "Epoch 1118 | Loss: 0.0142\n",
      "Epoch 1119 | Loss: 0.0123\n",
      "Epoch 1120 | Loss: 0.0093\n",
      "Epoch 1121 | Loss: 0.0077\n",
      "Epoch 1122 | Loss: 0.0084\n",
      "Epoch 1123 | Loss: 0.0062\n",
      "Epoch 1124 | Loss: 0.0055\n",
      "Epoch 1125 | Loss: 0.0057\n",
      "Epoch 1126 | Loss: 0.0053\n",
      "Epoch 1127 | Loss: 0.0060\n",
      "Epoch 1128 | Loss: 0.0045\n",
      "Epoch 1129 | Loss: 0.0047\n",
      "Epoch 1130 | Loss: 0.0046\n",
      "Epoch 1131 | Loss: 0.0054\n",
      "Epoch 1132 | Loss: 0.0046\n",
      "Epoch 1133 | Loss: 0.0040\n",
      "Epoch 1134 | Loss: 0.0044\n",
      "Epoch 1135 | Loss: 0.0044\n",
      "Epoch 1136 | Loss: 0.0041\n",
      "Epoch 1137 | Loss: 0.0042\n",
      "Epoch 1138 | Loss: 0.0037\n",
      "Epoch 1139 | Loss: 0.0037\n",
      "Epoch 1140 | Loss: 0.0038\n",
      "Epoch 1141 | Loss: 0.0039\n",
      "Epoch 1142 | Loss: 0.0034\n",
      "Epoch 1143 | Loss: 0.0040\n",
      "Epoch 1144 | Loss: 0.0041\n",
      "Epoch 1145 | Loss: 0.0038\n",
      "Epoch 1146 | Loss: 0.0036\n",
      "Epoch 1147 | Loss: 0.0034\n",
      "Epoch 1148 | Loss: 0.0036\n",
      "Epoch 1149 | Loss: 0.0036\n",
      "Epoch 1150 | Loss: 0.0037\n",
      "Epoch 1151 | Loss: 0.0037\n",
      "Epoch 1152 | Loss: 0.0036\n",
      "Epoch 1153 | Loss: 0.0051\n",
      "Epoch 1154 | Loss: 0.0047\n",
      "Epoch 1155 | Loss: 0.0041\n",
      "Epoch 1156 | Loss: 0.0037\n",
      "Epoch 1157 | Loss: 0.0050\n",
      "Epoch 1158 | Loss: 0.0060\n",
      "Epoch 1159 | Loss: 0.0042\n",
      "Epoch 1160 | Loss: 0.0032\n",
      "Epoch 1161 | Loss: 0.0041\n",
      "Epoch 1162 | Loss: 0.0040\n",
      "Epoch 1163 | Loss: 0.0035\n",
      "Epoch 1164 | Loss: 0.0036\n",
      "Epoch 1165 | Loss: 0.0034\n",
      "Epoch 1166 | Loss: 0.0031\n",
      "Epoch 1167 | Loss: 0.0033\n",
      "Epoch 1168 | Loss: 0.0029\n",
      "Epoch 1169 | Loss: 0.0037\n",
      "Epoch 1170 | Loss: 0.0035\n",
      "Epoch 1171 | Loss: 0.0038\n",
      "Epoch 1172 | Loss: 0.0027\n",
      "Epoch 1173 | Loss: 0.0039\n",
      "Epoch 1174 | Loss: 0.0040\n",
      "Epoch 1175 | Loss: 0.0033\n",
      "Epoch 1176 | Loss: 0.0039\n",
      "Epoch 1177 | Loss: 0.0029\n",
      "Epoch 1178 | Loss: 0.0035\n",
      "Epoch 1179 | Loss: 0.0029\n",
      "Epoch 1180 | Loss: 0.0034\n",
      "Epoch 1181 | Loss: 0.0033\n",
      "Epoch 1182 | Loss: 0.0037\n",
      "Epoch 1183 | Loss: 0.0035\n",
      "Epoch 1184 | Loss: 0.0036\n",
      "Epoch 1185 | Loss: 0.0037\n",
      "Epoch 1186 | Loss: 0.0032\n",
      "Epoch 1187 | Loss: 0.0039\n",
      "Epoch 1188 | Loss: 0.0031\n",
      "Epoch 1189 | Loss: 0.0035\n",
      "Epoch 1190 | Loss: 0.0033\n",
      "Epoch 1191 | Loss: 0.0031\n",
      "Epoch 1192 | Loss: 0.0046\n",
      "Epoch 1193 | Loss: 0.0034\n",
      "Epoch 1194 | Loss: 0.0039\n",
      "Epoch 1195 | Loss: 0.0032\n",
      "Epoch 1196 | Loss: 0.0036\n",
      "Epoch 1197 | Loss: 0.0033\n",
      "Epoch 1198 | Loss: 0.0031\n",
      "Epoch 1199 | Loss: 0.0032\n",
      "Epoch 1200 | Loss: 0.0032\n",
      "Epoch 1201 | Loss: 0.0029\n",
      "Epoch 1202 | Loss: 0.0035\n",
      "Epoch 1203 | Loss: 0.0034\n",
      "Epoch 1204 | Loss: 0.0033\n",
      "Epoch 1205 | Loss: 0.0030\n",
      "Epoch 1206 | Loss: 0.0029\n",
      "Epoch 1207 | Loss: 0.0035\n",
      "Epoch 1208 | Loss: 0.0039\n",
      "Epoch 1209 | Loss: 0.0038\n",
      "Epoch 1210 | Loss: 0.0041\n",
      "Epoch 1211 | Loss: 0.0039\n",
      "Epoch 1212 | Loss: 0.0033\n",
      "Epoch 1213 | Loss: 0.0053\n",
      "Epoch 1214 | Loss: 0.0036\n",
      "Epoch 1215 | Loss: 0.0036\n",
      "Epoch 1216 | Loss: 0.0038\n",
      "Epoch 1217 | Loss: 0.0032\n",
      "Epoch 1218 | Loss: 0.0034\n",
      "Epoch 1219 | Loss: 0.0042\n",
      "Epoch 1220 | Loss: 0.0036\n",
      "Epoch 1221 | Loss: 0.0031\n",
      "Epoch 1222 | Loss: 0.0042\n",
      "Epoch 1223 | Loss: 0.0031\n",
      "Epoch 1224 | Loss: 0.0038\n",
      "Epoch 1225 | Loss: 0.0027\n",
      "Epoch 1226 | Loss: 0.0039\n",
      "Epoch 1227 | Loss: 0.0038\n",
      "Epoch 1228 | Loss: 0.0032\n",
      "Epoch 1229 | Loss: 0.0033\n",
      "Epoch 1230 | Loss: 0.0036\n",
      "Epoch 1231 | Loss: 0.0031\n",
      "Epoch 1232 | Loss: 0.0035\n",
      "Epoch 1233 | Loss: 0.0033\n",
      "Epoch 1234 | Loss: 0.0030\n",
      "Epoch 1235 | Loss: 0.0036\n",
      "Epoch 1236 | Loss: 0.0032\n",
      "Epoch 1237 | Loss: 0.0031\n",
      "Epoch 1238 | Loss: 0.0038\n",
      "Epoch 1239 | Loss: 0.0031\n",
      "Epoch 1240 | Loss: 0.0031\n",
      "Epoch 1241 | Loss: 0.0029\n",
      "Epoch 1242 | Loss: 0.0040\n",
      "Epoch 1243 | Loss: 0.0036\n",
      "Epoch 1244 | Loss: 0.0032\n",
      "Epoch 1245 | Loss: 0.0035\n",
      "Epoch 1246 | Loss: 0.0035\n",
      "Epoch 1247 | Loss: 0.0034\n",
      "Epoch 1248 | Loss: 0.0046\n",
      "Epoch 1249 | Loss: 0.0057\n",
      "Epoch 1250 | Loss: 0.0045\n",
      "Epoch 1251 | Loss: 0.0041\n",
      "Epoch 1252 | Loss: 0.0038\n",
      "Epoch 1253 | Loss: 0.0034\n",
      "Epoch 1254 | Loss: 0.0033\n",
      "Epoch 1255 | Loss: 0.0031\n",
      "Epoch 1256 | Loss: 0.0038\n",
      "Epoch 1257 | Loss: 0.0030\n",
      "Epoch 1258 | Loss: 0.0035\n",
      "Epoch 1259 | Loss: 0.0039\n",
      "Epoch 1260 | Loss: 0.0030\n",
      "Epoch 1261 | Loss: 0.0032\n",
      "Epoch 1262 | Loss: 0.0048\n",
      "Epoch 1263 | Loss: 0.0129\n",
      "Epoch 1264 | Loss: 0.0110\n",
      "Epoch 1265 | Loss: 0.0154\n",
      "Epoch 1266 | Loss: 0.0369\n",
      "Epoch 1267 | Loss: 0.1171\n",
      "Epoch 1268 | Loss: 0.1101\n",
      "Epoch 1269 | Loss: 0.0877\n",
      "Epoch 1270 | Loss: 0.0762\n",
      "Epoch 1271 | Loss: 0.0775\n",
      "Epoch 1272 | Loss: 0.0429\n",
      "Epoch 1273 | Loss: 0.0372\n",
      "Epoch 1274 | Loss: 0.0217\n",
      "Epoch 1275 | Loss: 0.0186\n",
      "Epoch 1276 | Loss: 0.0149\n",
      "Epoch 1277 | Loss: 0.0115\n",
      "Epoch 1278 | Loss: 0.0077\n",
      "Epoch 1279 | Loss: 0.0103\n",
      "Epoch 1280 | Loss: 0.0096\n",
      "Epoch 1281 | Loss: 0.0072\n",
      "Epoch 1282 | Loss: 0.0058\n",
      "Epoch 1283 | Loss: 0.0097\n",
      "Epoch 1284 | Loss: 0.0054\n",
      "Epoch 1285 | Loss: 0.0053\n",
      "Epoch 1286 | Loss: 0.0047\n",
      "Epoch 1287 | Loss: 0.0049\n",
      "Epoch 1288 | Loss: 0.0043\n",
      "Epoch 1289 | Loss: 0.0044\n",
      "Epoch 1290 | Loss: 0.0045\n",
      "Epoch 1291 | Loss: 0.0049\n",
      "Epoch 1292 | Loss: 0.0047\n",
      "Epoch 1293 | Loss: 0.0039\n",
      "Epoch 1294 | Loss: 0.0045\n",
      "Epoch 1295 | Loss: 0.0045\n",
      "Epoch 1296 | Loss: 0.0042\n",
      "Epoch 1297 | Loss: 0.0039\n",
      "Epoch 1298 | Loss: 0.0044\n",
      "Epoch 1299 | Loss: 0.0032\n",
      "Epoch 1300 | Loss: 0.0031\n",
      "Epoch 1301 | Loss: 0.0036\n",
      "Epoch 1302 | Loss: 0.0033\n",
      "Epoch 1303 | Loss: 0.0040\n",
      "Epoch 1304 | Loss: 0.0040\n",
      "Epoch 1305 | Loss: 0.0033\n",
      "Epoch 1306 | Loss: 0.0037\n",
      "Epoch 1307 | Loss: 0.0040\n",
      "Epoch 1308 | Loss: 0.0037\n",
      "Epoch 1309 | Loss: 0.0032\n",
      "Epoch 1310 | Loss: 0.0031\n",
      "Epoch 1311 | Loss: 0.0039\n",
      "Epoch 1312 | Loss: 0.0031\n",
      "Epoch 1313 | Loss: 0.0038\n",
      "Epoch 1314 | Loss: 0.0039\n",
      "Epoch 1315 | Loss: 0.0041\n",
      "Epoch 1316 | Loss: 0.0035\n",
      "Epoch 1317 | Loss: 0.0042\n",
      "Epoch 1318 | Loss: 0.0030\n",
      "Epoch 1319 | Loss: 0.0038\n",
      "Epoch 1320 | Loss: 0.0037\n",
      "Epoch 1321 | Loss: 0.0044\n",
      "Epoch 1322 | Loss: 0.0038\n",
      "Epoch 1323 | Loss: 0.0039\n",
      "Epoch 1324 | Loss: 0.0032\n",
      "Epoch 1325 | Loss: 0.0037\n",
      "Epoch 1326 | Loss: 0.0035\n",
      "Epoch 1327 | Loss: 0.0033\n",
      "Epoch 1328 | Loss: 0.0036\n",
      "Epoch 1329 | Loss: 0.0038\n",
      "Epoch 1330 | Loss: 0.0031\n",
      "Epoch 1331 | Loss: 0.0033\n",
      "Epoch 1332 | Loss: 0.0035\n",
      "Epoch 1333 | Loss: 0.0031\n",
      "Epoch 1334 | Loss: 0.0032\n",
      "Epoch 1335 | Loss: 0.0038\n",
      "Epoch 1336 | Loss: 0.0035\n",
      "Epoch 1337 | Loss: 0.0032\n",
      "Epoch 1338 | Loss: 0.0029\n",
      "Epoch 1339 | Loss: 0.0033\n",
      "Epoch 1340 | Loss: 0.0032\n",
      "Epoch 1341 | Loss: 0.0034\n",
      "Epoch 1342 | Loss: 0.0033\n",
      "Epoch 1343 | Loss: 0.0031\n",
      "Epoch 1344 | Loss: 0.0033\n",
      "Epoch 1345 | Loss: 0.0034\n",
      "Epoch 1346 | Loss: 0.0032\n",
      "Epoch 1347 | Loss: 0.0031\n",
      "Epoch 1348 | Loss: 0.0031\n",
      "Epoch 1349 | Loss: 0.0031\n",
      "Epoch 1350 | Loss: 0.0029\n",
      "Epoch 1351 | Loss: 0.0037\n",
      "Epoch 1352 | Loss: 0.0038\n",
      "Epoch 1353 | Loss: 0.0028\n",
      "Epoch 1354 | Loss: 0.0031\n",
      "Epoch 1355 | Loss: 0.0030\n",
      "Epoch 1356 | Loss: 0.0030\n",
      "Epoch 1357 | Loss: 0.0030\n",
      "Epoch 1358 | Loss: 0.0033\n",
      "Epoch 1359 | Loss: 0.0038\n",
      "Epoch 1360 | Loss: 0.0037\n",
      "Epoch 1361 | Loss: 0.0032\n",
      "Epoch 1362 | Loss: 0.0032\n",
      "Epoch 1363 | Loss: 0.0032\n",
      "Epoch 1364 | Loss: 0.0031\n",
      "Epoch 1365 | Loss: 0.0034\n",
      "Epoch 1366 | Loss: 0.0028\n",
      "Epoch 1367 | Loss: 0.0030\n",
      "Epoch 1368 | Loss: 0.0031\n",
      "Epoch 1369 | Loss: 0.0030\n",
      "Epoch 1370 | Loss: 0.0029\n",
      "Epoch 1371 | Loss: 0.0035\n",
      "Epoch 1372 | Loss: 0.0032\n",
      "Epoch 1373 | Loss: 0.0034\n",
      "Epoch 1374 | Loss: 0.0033\n",
      "Epoch 1375 | Loss: 0.0029\n",
      "Epoch 1376 | Loss: 0.0032\n",
      "Epoch 1377 | Loss: 0.0037\n",
      "Epoch 1378 | Loss: 0.0039\n",
      "Epoch 1379 | Loss: 0.0029\n",
      "Epoch 1380 | Loss: 0.0033\n",
      "Epoch 1381 | Loss: 0.0034\n",
      "Epoch 1382 | Loss: 0.0035\n",
      "Epoch 1383 | Loss: 0.0033\n",
      "Epoch 1384 | Loss: 0.0049\n",
      "Epoch 1385 | Loss: 0.0028\n",
      "Epoch 1386 | Loss: 0.0058\n",
      "Epoch 1387 | Loss: 0.0046\n",
      "Epoch 1388 | Loss: 0.0048\n",
      "Epoch 1389 | Loss: 0.0078\n",
      "Epoch 1390 | Loss: 0.0084\n",
      "Epoch 1391 | Loss: 0.0096\n",
      "Epoch 1392 | Loss: 0.0100\n",
      "Epoch 1393 | Loss: 0.0271\n",
      "Epoch 1394 | Loss: 0.0277\n",
      "Epoch 1395 | Loss: 0.0388\n",
      "Epoch 1396 | Loss: 0.0281\n",
      "Epoch 1397 | Loss: 0.0526\n",
      "Epoch 1398 | Loss: 0.0377\n",
      "Epoch 1399 | Loss: 0.0353\n",
      "Epoch 1400 | Loss: 0.0260\n",
      "Epoch 1401 | Loss: 0.0286\n",
      "Epoch 1402 | Loss: 0.0381\n",
      "Epoch 1403 | Loss: 0.0312\n",
      "Epoch 1404 | Loss: 0.0254\n",
      "Epoch 1405 | Loss: 0.0277\n",
      "Epoch 1406 | Loss: 0.0151\n",
      "Epoch 1407 | Loss: 0.0099\n",
      "Epoch 1408 | Loss: 0.0105\n",
      "Epoch 1409 | Loss: 0.0075\n",
      "Epoch 1410 | Loss: 0.0085\n",
      "Epoch 1411 | Loss: 0.0087\n",
      "Epoch 1412 | Loss: 0.0059\n",
      "Epoch 1413 | Loss: 0.0065\n",
      "Epoch 1414 | Loss: 0.0050\n",
      "Epoch 1415 | Loss: 0.0039\n",
      "Epoch 1416 | Loss: 0.0041\n",
      "Epoch 1417 | Loss: 0.0041\n",
      "Epoch 1418 | Loss: 0.0039\n",
      "Epoch 1419 | Loss: 0.0045\n",
      "Epoch 1420 | Loss: 0.0038\n",
      "Epoch 1421 | Loss: 0.0031\n",
      "Epoch 1422 | Loss: 0.0036\n",
      "Epoch 1423 | Loss: 0.0045\n",
      "Epoch 1424 | Loss: 0.0037\n",
      "Epoch 1425 | Loss: 0.0033\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m dig_expanded \u001b[38;5;241m=\u001b[39m dig\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape (32, 80, 10)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((input_seq, dig_expanded), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m pred_seq, hidden \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monehot_digit\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdig\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch, seq_len-1, 4]\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Separate predictions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m pred_dxdy \u001b[38;5;241m=\u001b[39m pred_seq[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m2\u001b[39m]         \u001b[38;5;66;03m# [batch, seq_len-1, 2]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36mDigitToStrokeLSTM.forward\u001b[1;34m(self, x, hidden, onehot_digit)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hidden \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m onehot_digit \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size),\n\u001b[0;32m     43\u001b[0m               torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m---> 45\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)\n\u001b[0;32m     48\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_head(out)\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:1123\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m   1120\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1130\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1132\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1137\u001b[0m         batch_sizes,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m   1145\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 2500\n",
    "\n",
    "best_val_loss = 9999\n",
    "patience = 10\n",
    "counter = 0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (dig, input_seq, output_seq) in loader:\n",
    "        # stroke_seq: [batch, seq_len, 4]\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = output_seq.to(device)\n",
    "        dig = dig.to(device)\n",
    "\n",
    "        dig_expanded = dig.unsqueeze(1).expand(-1, 80, -1)  # shape (32, 80, 10)\n",
    "        inputs = torch.cat((input_seq, dig_expanded), dim=2)\n",
    "\n",
    "        pred_seq, hidden = model(inputs, onehot_digit = dig)  # [batch, seq_len-1, 4]\n",
    "\n",
    "        # Separate predictions\n",
    "        pred_dxdy = pred_seq[..., :2]         # [batch, seq_len-1, 2]\n",
    "        pred_eos_eod = pred_seq[..., 2:]      # [batch, seq_len-1, 2]\n",
    "\n",
    "        # Separate targets\n",
    "        target_dxdy = target_seq[..., :2]\n",
    "        target_eos_eod = target_seq[..., 2:]\n",
    "\n",
    "        # Compute losses\n",
    "        loss_dxdy = dx_dy_loss_fn(pred_dxdy, target_dxdy)\n",
    "        loss_eos_eod = eos_eod_loss_fn(pred_eos_eod, target_eos_eod)\n",
    "\n",
    "        loss = loss_dxdy + loss_eos_eod\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    # if total_loss < best_val_loss:\n",
    "    #     best_val_loss = total_loss\n",
    "    #     counter = 0\n",
    "    # else:\n",
    "    #     counter += 1\n",
    "    #     if counter >= patience:\n",
    "    #         print(\"Early stopping triggered\")\n",
    "    #         break\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0d776b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    \n",
    "    temp_onehot_concat = temp_onehot.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(torch.cat((initial_input, temp_onehot_concat), dim=-1), onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(max_length-1):\n",
    "        output, hidden = model(torch.cat((output, temp_onehot_concat), dim=-1), hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e548538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1ac7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model if good\n",
    "torch.save(model.state_dict(), 'model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e579b40",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DigitToStrokeLSTM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([512, 10]) from checkpoint, the shape in current model is torch.Size([256, 10]).\n\tsize mismatch for embedding.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 4]) from checkpoint, the shape in current model is torch.Size([1024, 4]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([4, 512]) from checkpoint, the shape in current model is torch.Size([4, 256]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m DigitToStrokeLSTM()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# create a new instance\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_weights.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# set to evaluation mode if you're doing inference\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kapj_\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DigitToStrokeLSTM:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([512, 10]) from checkpoint, the shape in current model is torch.Size([256, 10]).\n\tsize mismatch for embedding.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([256]).\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([2048, 4]) from checkpoint, the shape in current model is torch.Size([1024, 4]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.weight_ih_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.weight_hh_l1: copying a param with shape torch.Size([2048, 512]) from checkpoint, the shape in current model is torch.Size([1024, 256]).\n\tsize mismatch for lstm.bias_ih_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for lstm.bias_hh_l1: copying a param with shape torch.Size([2048]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for output_head.weight: copying a param with shape torch.Size([4, 512]) from checkpoint, the shape in current model is torch.Size([4, 256])."
     ]
    }
   ],
   "source": [
    "model = DigitToStrokeLSTM().to(device) # create a new instance\n",
    "model.load_state_dict(torch.load('model_weights.pth', weights_only=True))\n",
    "model.eval()  # set to evaluation mode if you're doing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39ec9780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_text(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca8420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shit: 0, 5, 6, 8, 9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
