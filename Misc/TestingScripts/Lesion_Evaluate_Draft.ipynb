{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87b20e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63447d8c",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "965b3076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumFromOneHot(inp):\n",
    "    for i in range(10):\n",
    "        if inp[i] == 1:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c69400a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_stroke_sequence(sequence):\n",
    "    \"\"\"\n",
    "    sequence: numpy array or list of shape (T, 4) where each row is [dx, dy, eos, eod]\n",
    "    save_path: optional path to save the plot as an image\n",
    "    show: whether to display the plot\n",
    "    \"\"\"\n",
    "    x, y = 0, 0\n",
    "    xs, ys = [], []\n",
    "\n",
    "    for dx, dy, eos, eod in sequence:\n",
    "        x += dx*28\n",
    "        y += dy*28\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        if eos > 0.5:  # end of stroke\n",
    "            xs.append(None)\n",
    "            ys.append(None)\n",
    "\n",
    "        if eod > 0.5:\n",
    "            break\n",
    "\n",
    "    # Load onto variable img_array\n",
    "    plt.figure(figsize=(1, 1), dpi=28)  # 1 inch * 28 dpi = 28 pixels\n",
    "    plt.plot(xs, ys, color = \"black\", linewidth=2)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.axis('off')\n",
    "    plt.axis('equal')\n",
    "    # Use a BytesIO buffer to save the plot into memory\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format='png', transparent=False, facecolor='white')\n",
    "    plt.close()\n",
    "    buf.seek(0)  # Rewind the buffer to the beginning\n",
    "    img = Image.open(buf)  # Open the image from the buffer\n",
    "    \n",
    "    img_array = np.array(img.convert('L'))  # Convert to grayscale (1 channel) as a numpy array\n",
    "    \n",
    "    buf.close()  # Close the buffer\n",
    "    \n",
    "    return img_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f162670",
   "metadata": {},
   "source": [
    "# 1. Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bb9a8",
   "metadata": {},
   "source": [
    "### 1.1 Define the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28e6bbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitToStrokeLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=256, num_layers=2, batch_size=32):\n",
    "        super(DigitToStrokeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedding = nn.Linear(10, hidden_size)  # From one-hot to hidden dim\n",
    "        \n",
    "        # LSTM\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        # Inital hidden state is the one-hot of number\n",
    "        # Initial input is [0, 0, 0, 0, 0]\n",
    "        # Input at t > 0 is output from t-1\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=4,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "\n",
    "        # Output layer: predicts [dx, dy, eos, eod]\n",
    "        self.output_head = nn.Linear(hidden_size, 4)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.sigmoid = nn.Sigmoid()  # For eos/eod\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "\n",
    "    def forward(self, x, hidden=None, onehot_digit=None):\n",
    "        \n",
    "        if onehot_digit != None and hidden == None:\n",
    "            # Embed the digit\n",
    "            h0 = self.embedding(onehot_digit)\n",
    "            h0 = h0.unsqueeze(0).repeat(self.num_layers, 1, 1)\n",
    "            c0 = torch.zeros_like(h0)\n",
    "            hidden = (h0, c0)\n",
    "\n",
    "        elif hidden == None and onehot_digit == None:\n",
    "            hidden = (torch.zeros(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, self.batch_size, self.hidden_size))\n",
    "            \n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.output_head(out)\n",
    "        \n",
    "        out[:, :, 0:2] = self.tanh(out[:, :, 0:2])\n",
    "        # out[:, :, 2:] = self.sigmoid(out[:, :, 2:])\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401329e5",
   "metadata": {},
   "source": [
    "### 1.2 Load the model from pre-saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c2c63c14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DigitToStrokeLSTM(\n",
       "  (embedding): Linear(in_features=10, out_features=512, bias=True)\n",
       "  (lstm): LSTM(4, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (output_head): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (tanh): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "model.load_state_dict(torch.load('model_weights/sketch_model_weights1.pth', weights_only=True))\n",
    "model.eval()  # set to evaluation mode if you're doing inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b820f24a",
   "metadata": {},
   "source": [
    "### 1.3 Function to generate digit from the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40ad9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw the number using the RNN\n",
    "\n",
    "def generate_text(model, number):\n",
    "    model.eval()\n",
    "    \n",
    "    temp_onehot = np.zeros(10)\n",
    "    temp_onehot[number] = 1\n",
    "    temp_onehot = torch.tensor(temp_onehot, dtype=torch.float32).to(device)\n",
    "    \n",
    "    initial_input = torch.tensor([0, 0, 0, 0], dtype=torch.float32).to(device).unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    outputs = []\n",
    "    \n",
    "    output, hidden = model(initial_input, onehot_digit=temp_onehot)\n",
    "    output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "\n",
    "    outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "\n",
    "    for i in range(62-1):\n",
    "        output, hidden = model(output, hidden=hidden)\n",
    "        output[..., 2:] = (torch.sigmoid(output[..., -1, 2:]) > 0.5).float()\n",
    "        outputs.append(output[:, -1, :].detach().cpu().numpy()[0])\n",
    "        \n",
    "        # print(outputs[-1])\n",
    "        if output[:, -1, 3] == 1:\n",
    "            # print(\"HI\")\n",
    "            break\n",
    "    \n",
    "    return draw_stroke_sequence(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30178f42",
   "metadata": {},
   "source": [
    "### 1.4 Test the drawing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a163eb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 320x320 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test if the loaded model works\n",
    "meow = generate_text(model, 0) # <----- Change number and see it print it :)\n",
    "plt.figure(figsize=(4, 4), dpi=80)\n",
    "plt.imshow(meow, cmap='gray')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "91ce2023",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # First Convolution Block\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),  # 32 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(32),\n",
    "            \n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Second Convolution Block\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),  # 64 filters, 3x3 kernel, 'same' padding\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.MaxPool2d(2, 2),  # Max pooling (2x2) with stride 2\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            # Fully Connected (Dense) layers\n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(64 * 7 * 7, 512),  # Input size depends on the output of convolutional layers\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.25),\n",
    "            \n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(0.5),\n",
    "            \n",
    "            nn.Linear(1024, 10)  # Output layer (10 classes)\n",
    "        )\n",
    "            \n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2b249db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNModel(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Dropout(p=0.25, inplace=False)\n",
       "    (6): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (11): Dropout(p=0.25, inplace=False)\n",
       "    (12): Flatten(start_dim=1, end_dim=-1)\n",
       "    (13): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (14): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (15): Dropout(p=0.25, inplace=False)\n",
       "    (16): Linear(in_features=512, out_features=1024, bias=True)\n",
       "    (17): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): Dropout(p=0.5, inplace=False)\n",
       "    (19): Linear(in_features=1024, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnnEvaluator = CNNModel().to(device)\n",
    "cnnEvaluator.load_state_dict(torch.load('../classify_cnn/cnn_weights/cnn_weights1.pth', weights_only=True))\n",
    "cnnEvaluator.eval()  # set to evaluation mode if you're doing inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2cd97208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "Predicted Class: 2\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Step 2: Preprocess the image\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()  # Convert to PyTorch tensor (scales pixels to [0, 1])\n",
    "])\n",
    "\n",
    "img_tensor = transform(meow).to(device)\n",
    "\n",
    "img_tensor[img_tensor<0.6] = 0\n",
    "img_tensor[img_tensor>=0.6] = 1\n",
    "\n",
    "img_tensor = 1-img_tensor\n",
    "\n",
    "img = img_tensor.squeeze(0).squeeze(0).cpu().numpy()  # Remove batch and channel dimensions\n",
    "\n",
    "# Display the image using matplotlib\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Step 3: Add batch dimension (PyTorch models expect a batch of images)\n",
    "img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension (1, 1, 28, 28)\n",
    "\n",
    "# Step 4: Pass the tensor to the model\n",
    "cnnEvaluator.eval()  # Ensure the model is in evaluation mode\n",
    "with torch.no_grad():  # Turn off gradients since we're in inference mode\n",
    "    print(img_tensor.size())\n",
    "    output = cnnEvaluator(img_tensor)  # Pass the image tensor to the model for prediction\n",
    "\n",
    "# Step 5: Interpret the output\n",
    "_, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "print(f'Predicted Class: {predicted_class.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87b2677",
   "metadata": {},
   "source": [
    "### Check evaluate hand-writing with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4c72ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: 1 Desired Class: 0 Predicted Class: 7\n",
      "Model: 1 Desired Class: 6 Predicted Class: 5\n",
      "Model: 1 Desired Class: 9 Predicted Class: 4\n",
      "Model: 2 Desired Class: 8 Predicted Class: 7\n",
      "Model: 3 Desired Class: 6 Predicted Class: 5\n",
      "Model: 3 Desired Class: 9 Predicted Class: 4\n",
      "Model: 4 Desired Class: 9 Predicted Class: 7\n",
      "Model: 5 Desired Class: 6 Predicted Class: 5\n",
      "Model: 5 Desired Class: 9 Predicted Class: 7\n",
      "Accuracy 0.82\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "sum = 0\n",
    "\n",
    "for i in range(1, 6):\n",
    "    model = DigitToStrokeLSTM(hidden_size = 512, num_layers=2).to(device)\n",
    "    model.load_state_dict(torch.load(f'model_weights/sketch_model_weights{i}.pth', weights_only=True))\n",
    "    model.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "    cnnEvaluator = CNNModel().to(device)\n",
    "    cnnEvaluator.load_state_dict(torch.load(f'../classify_cnn/cnn_weights/cnn_weights{i}.pth', weights_only=True))\n",
    "    cnnEvaluator.eval()  # set to evaluation mode if you're doing inference\n",
    "\n",
    "    for j in range(10):\n",
    "        # Test if the loaded model works\n",
    "        img = generate_text(model, j) # <----- Change number and see it print it :)\n",
    "        \n",
    "        \n",
    "        # Step 2: Preprocess the image\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor()  # Convert to PyTorch tensor (scales pixels to [0, 1])\n",
    "        ])\n",
    "\n",
    "        img_tensor = transform(img).to(device)\n",
    "\n",
    "        img_tensor[img_tensor<0.6] = 0\n",
    "        img_tensor[img_tensor>=0.6] = 1\n",
    "\n",
    "        img_tensor = 1-img_tensor\n",
    "\n",
    "        # Step 3: Add batch dimension (PyTorch models expect a batch of images)\n",
    "        img_tensor = img_tensor.unsqueeze(0)  # Add batch dimension (1, 1, 28, 28)\n",
    "\n",
    "        # Step 4: Pass the tensor to the model\n",
    "        cnnEvaluator.eval()  # Ensure the model is in evaluation mode\n",
    "        with torch.no_grad():  # Turn off gradients since we're in inference mode\n",
    "            output = cnnEvaluator(img_tensor)  # Pass the image tensor to the model for prediction\n",
    "\n",
    "        # Step 5: Interpret the output\n",
    "        _, predicted_class = torch.max(output, 1)  # Get the predicted class index\n",
    "        # print(f'Predicted Class: {predicted_class.item() == j}')\n",
    "        if predicted_class.item() == j:\n",
    "            sum += 1\n",
    "        else:\n",
    "            print(f'Model: {i} Desired Class: {j} Predicted Class: {predicted_class.item()}')\n",
    "        total+= 1\n",
    "        \n",
    "print(f\"Accuracy {sum/total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5455727",
   "metadata": {},
   "source": [
    "# 2. Lesion the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce103f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAMAGE WEIGHTS\n",
    "\n",
    "def damage_smallest(model, p_smallest): # energy constraint\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_smallest == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            weight_magnitudes = tensor.abs().view(-1)\n",
    "            k = int(weight_magnitudes.numel() * p_smallest)\n",
    "\n",
    "            if k == 0:\n",
    "                continue\n",
    "            threshold = weight_magnitudes.kthvalue(k).values.item()\n",
    "\n",
    "            mask = tensor.abs() >= threshold\n",
    "            param.data.mul_(mask)\n",
    "\n",
    "def damage_fas(model, p_block, p_reflect, p_filter):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad and param.ndim >= 2:\n",
    "            if p_block + p_reflect + p_filter == 0:\n",
    "                continue\n",
    "\n",
    "            tensor = param.data\n",
    "            flat_weights = tensor.view(-1)\n",
    "            nonzero_indices = (flat_weights!=0).nonzero(as_tuple=True)[0]\n",
    "            num_nonzero_indices = nonzero_indices.numel()\n",
    "            if num_nonzero_indices == 0:\n",
    "                continue\n",
    "\n",
    "            # percentage of weights damaged will be taken from the number of nonzero weights\n",
    "            # simulated fas damage occurs after energy constraint blockage\n",
    "            num_block = int(num_nonzero_indices * p_block)\n",
    "            num_reflect = int(num_nonzero_indices * p_reflect)\n",
    "            num_filter = int(num_nonzero_indices * p_filter)\n",
    "\n",
    "            shuffled_indices = nonzero_indices[torch.randperm(num_nonzero_indices, device=flat_weights.device)]\n",
    "\n",
    "            indices_block = shuffled_indices[:num_block]\n",
    "            indices_reflect = shuffled_indices[num_block:num_block+num_reflect]\n",
    "            indices_filter = shuffled_indices[num_block+num_reflect:num_block+num_reflect+num_filter]\n",
    "\n",
    "            # do damage\n",
    "            # blockage: set weights to 0\n",
    "            if p_block != 0:\n",
    "                flat_weights[indices_block] = 0\n",
    "\n",
    "            # reflect: halve weights\n",
    "            if p_reflect != 0:\n",
    "                flat_weights[indices_reflect] *= 0.5\n",
    "\n",
    "            # filter: low pass filter (lusch et al)\n",
    "            if p_filter != 0:\n",
    "                weights_to_filter = flat_weights[indices_filter]            # get weights before transformation\n",
    "                signs = torch.sign(weights_to_filter)                       # get signs of weights\n",
    "                abs_weights_to_filter = weights_to_filter.abs()             # get high_weight, should be in the 95th percentile for all weights\n",
    "                high_weight = torch.quantile(flat_weights.abs(), 0.95)      # scale weights to mostly between -1 and 1\n",
    "                x = abs_weights_to_filter / high_weight\n",
    "                transformed_weights = -0.2744 * x**2 + 0.9094 * x - 0.0192\n",
    "                gaussian_noise = torch.randn_like(transformed_weights) * 0.05\n",
    "                transformed_weights += gaussian_noise\n",
    "                transformed_weights = transformed_weights * signs * high_weight # rescale\n",
    "                flat_weights[indices_filter] = transformed_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d389cf1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 200x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "copy_model = copy.deepcopy(model)\n",
    "damage_smallest(copy_model, 0.1)\n",
    "damage_fas(copy_model, 0, 0, 0)\n",
    "generate_text(copy_model, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6236228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
